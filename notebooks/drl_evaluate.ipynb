{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch # Ensure torch is imported if policy_kwargs might be used or for consistency\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming utils are in parent directory or PYTHONPATH is set\n",
    "from utils.portfolio_env import PortfolioEnv \n",
    "from utils.drl_agent_jules import DRLAgent # Import the modified agent\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# !!! USER: Specify which window's best agent to load and analyze !!!\n",
    "WINDOW_TO_ANALYZE = 1 # Example: Load best agent from Window 1 (valid: 1 to 10)\n",
    "\n",
    "BASE_START_YEAR = 2006 # Must match the training notebook's BASE_START_YEAR\n",
    "\n",
    "# Data paths (must match the training notebook)\n",
    "PRICE_DATA_PATH = \"../data/prices.parquet\"\n",
    "RETURNS_DATA_PATH = \"../data/returns.parquet\"\n",
    "VOLA_DATA_PATH = \"../data/vola.parquet\"\n",
    "\n",
    "# Directory where models from the training notebook are saved\n",
    "folder_name = \"xxx\"\n",
    "MODEL_SAVE_DIR = f\"../models/{folder_name}\" \n",
    "\n",
    "# --- PortfolioEnv Parameters (must match training env config for consistency) ---\n",
    "ENV_WINDOW_SIZE = 60 \n",
    "TRANSACTION_COST = 0.0 \n",
    "INITIAL_BALANCE = 100_000\n",
    "REWARD_SCALING = 1.0 # Usually 1.0 for evaluation if not affecting state/rewards directly\n",
    "ETA_DSR = 1 / 252 \n",
    "\n",
    "# --- DRL Agent Parameters (needed for DRLAgent instantiation before loading) ---\n",
    "# These are less critical if the model is fully self-contained, but good for consistency\n",
    "POLICY_KWARGS = dict(\n",
    "    activation_fn=torch.nn.Tanh,\n",
    "    net_arch=[64, 64], \n",
    "    log_std_init=-1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full datasets...\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading full datasets...\")\n",
    "try:\n",
    "    prices_df_full = pd.read_parquet(PRICE_DATA_PATH)\n",
    "    returns_df_full = pd.read_parquet(RETURNS_DATA_PATH)\n",
    "    vola_df_full = pd.read_parquet(VOLA_DATA_PATH)\n",
    "    \n",
    "    for df in [prices_df_full, returns_df_full, vola_df_full]:\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            \n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Data file not found. {e}\")\n",
    "    print(\"Please ensure data is generated and paths are correct in Cell 2.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Window 1\n",
      "  Original training start year for this window's agent: 2006\n",
      "  Backtest Period for this agent: 2012-01-01 to 2012-12-31\n",
      "  Backtest data slice length: 250 days.\n"
     ]
    }
   ],
   "source": [
    "# Calculate the start year for the *training* of the specified window\n",
    "# Window numbering is 1-based for user input, index is 0-based\n",
    "window_index = WINDOW_TO_ANALYZE - 1 \n",
    "if not (0 <= window_index < 10): # N_WINDOWS is 10\n",
    "    raise ValueError(f\"WINDOW_TO_ANALYZE must be between 1 and 10. Got: {WINDOW_TO_ANALYZE}\")\n",
    "\n",
    "train_start_year_for_window = BASE_START_YEAR + window_index\n",
    "\n",
    "# The backtest period for this window is:\n",
    "# Training: train_start_year_for_window to train_start_year_for_window + 4 (5 years)\n",
    "# Validation: train_start_year_for_window + 5 (1 year)\n",
    "# Backtest: train_start_year_for_window + 6 (1 year)\n",
    "backtest_period_start_year = train_start_year_for_window + 5 + 1\n",
    "\n",
    "backtest_start_date = pd.to_datetime(f\"{backtest_period_start_year}-01-01\")\n",
    "backtest_end_date = pd.to_datetime(f\"{backtest_period_start_year}-12-31\")\n",
    "\n",
    "print(f\"Analyzing Window {WINDOW_TO_ANALYZE}\")\n",
    "print(f\"  Original training start year for this window's agent: {train_start_year_for_window}\")\n",
    "print(f\"  Backtest Period for this agent: {backtest_start_date.date()} to {backtest_end_date.date()}\")\n",
    "\n",
    "# Slice data for the backtest period\n",
    "backtest_prices = prices_df_full[backtest_start_date:backtest_end_date]\n",
    "backtest_returns = returns_df_full[backtest_start_date:backtest_end_date]\n",
    "backtest_vola = vola_df_full[backtest_start_date:backtest_end_date]\n",
    "\n",
    "if backtest_prices.empty:\n",
    "    print(f\"ERROR: No data found for the backtest period of Window {WINDOW_TO_ANALYZE}.\")\n",
    "    print(\"Check data availability and date calculations.\")\n",
    "    raise ValueError(\"Empty backtest data slice.\")\n",
    "else:\n",
    "    print(f\"  Backtest data slice length: {len(backtest_prices)} days.\")\n",
    "\n",
    "# PortfolioEnv requires at least `window_size` days of data.\n",
    "min_data_len = ENV_WINDOW_SIZE + 1 \n",
    "if len(backtest_prices) < min_data_len:\n",
    "    print(f\"WARNING: Backtest data length ({len(backtest_prices)}) is less than required minimum ({min_data_len}).\")\n",
    "    # This might prevent the environment from starting or running a full episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best agent path from summary CSV: ../models/sliding_window_jules/backtest_results_summary_20250531_124739.csv\n",
      "Will load agent model from: ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "Loading model into DRLAgent from ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip...\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Attempt to find the best agent model file for the specified window.\n",
    "# The training script saves them as \"best_agent_window_{i}.zip\" or similar.\n",
    "# This part might need adjustment based on the exact naming convention used in the training script's output.\n",
    "# For now, let's assume a pattern or that the user might need to specify the exact file.\n",
    "\n",
    "# A more robust way: the training script should output a manifest or consistent names.\n",
    "# Let's assume the training script saved a summary CSV that includes the path to the best agent.\n",
    "# For this example, we'll try to infer the path or require user input if it's complex.\n",
    "\n",
    "# Try to find a summary CSV from the training notebook\n",
    "summary_csv_path = None\n",
    "potential_summaries = sorted([\n",
    "    os.path.join(MODEL_SAVE_DIR, f) for f in os.listdir(MODEL_SAVE_DIR) if f.startswith(\"backtest_results_summary_\") and f.endswith(\".csv\")\n",
    "], reverse=True) # Get the latest summary\n",
    "\n",
    "if potential_summaries:\n",
    "    summary_csv_path = potential_summaries[0]\n",
    "    print(f\"Loading best agent path from summary CSV: {summary_csv_path}\")\n",
    "    summary_df = pd.read_csv(summary_csv_path)\n",
    "    # Assuming 'window' column is 1-based and 'best_agent_path' column exists\n",
    "    agent_path_series = summary_df.loc[summary_df['window'] == WINDOW_TO_ANALYZE, 'best_agent_path']\n",
    "    if not agent_path_series.empty:\n",
    "        MODEL_PATH_TO_LOAD = agent_path_series.iloc[0]\n",
    "        if pd.isna(MODEL_PATH_TO_LOAD): # Handle case where path might be None/NaN if window was skipped\n",
    "             MODEL_PATH_TO_LOAD = None \n",
    "    else:\n",
    "        MODEL_PATH_TO_LOAD = None\n",
    "        print(f\"Could not find best_agent_path for Window {WINDOW_TO_ANALYZE} in summary CSV.\")\n",
    "else:\n",
    "    MODEL_PATH_TO_LOAD = None\n",
    "    print(\"No summary CSV found. Model path cannot be automatically determined.\")\n",
    "    print(f\"Please manually set MODEL_PATH_TO_LOAD if you know the direct path to the .zip file for Window {WINDOW_TO_ANALYZE}'s best agent.\")\n",
    "\n",
    "# Fallback or manual override:\n",
    "# MODEL_PATH_TO_LOAD = \"../models/sliding_window_jules/MANUAL_PATH_TO_BEST_AGENT_FOR_WINDOW_X.zip\" \n",
    "\n",
    "if not MODEL_PATH_TO_LOAD or not os.path.exists(MODEL_PATH_TO_LOAD):\n",
    "    print(f\"ERROR: Model path for Window {WINDOW_TO_ANALYZE} not found or is invalid: {MODEL_PATH_TO_LOAD}\")\n",
    "    print(\"Please ensure the training script ran successfully and saved the models, or specify the path manually.\")\n",
    "    raise FileNotFoundError(f\"Best agent model for Window {WINDOW_TO_ANALYZE} not found at {MODEL_PATH_TO_LOAD}\")\n",
    "else:\n",
    "    print(f\"Will load agent model from: {MODEL_PATH_TO_LOAD}\")\n",
    "\n",
    "# Create the Backtesting Environment\n",
    "env_backtest_config = {\n",
    "    'returns_df': backtest_returns, \n",
    "    'prices_df': backtest_prices, \n",
    "    'vol_df': backtest_vola,\n",
    "    'window_size': ENV_WINDOW_SIZE, \n",
    "    'transaction_cost': TRANSACTION_COST,\n",
    "    'initial_balance': INITIAL_BALANCE, \n",
    "    'reward_scaling': REWARD_SCALING, \n",
    "    'eta': ETA_DSR\n",
    "}\n",
    "env_backtest = PortfolioEnv(**env_backtest_config)\n",
    "\n",
    "# Create a temporary env instance for DRLAgent initialization before loading the model.\n",
    "# This uses minimal data but maintains the structure (obs/action space).\n",
    "temp_env_for_load_init = PortfolioEnv(\n",
    "    returns_df=backtest_returns.iloc[:ENV_WINDOW_SIZE+5], # Minimal data for structure\n",
    "    prices_df=backtest_prices.iloc[:ENV_WINDOW_SIZE+5],\n",
    "    vol_df=backtest_vola.iloc[:ENV_WINDOW_SIZE+5],\n",
    "    window_size=ENV_WINDOW_SIZE, \n",
    "    initial_balance=INITIAL_BALANCE\n",
    "    # eta is not strictly needed for just init if DRLAgent doesn't require it for __init__ before load\n",
    ")\n",
    "\n",
    "# Instantiate the DRLAgent (shell)\n",
    "# n_envs=1 because we are evaluating on a single backtest environment\n",
    "loaded_agent = DRLAgent(\n",
    "    env=temp_env_for_load_init, \n",
    "    n_envs=1, \n",
    "    policy_kwargs=POLICY_KWARGS # For consistency, though PPO.load uses saved model's kwargs\n",
    ")\n",
    "\n",
    "print(f\"Loading model into DRLAgent from {MODEL_PATH_TO_LOAD}...\")\n",
    "# Load the weights and associate with the actual backtest environment\n",
    "loaded_agent.load(path=MODEL_PATH_TO_LOAD, env=env_backtest)\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting backtest evaluation...\n",
      "\n",
      "--- Backtest Evaluation Metrics ---\n",
      "  n_eval_episodes: 1\n",
      "  final_portfolio_value_first_episode: 99187.4918\n",
      "  mean_reward: 2.9870\n",
      "  std_reward: 0.0000\n",
      "  Annual return: -0.0108\n",
      "  Cumulative returns: -0.0081\n",
      "  Annual volatility: 0.1169\n",
      "  Sharpe ratio: -0.2058\n",
      "  Calmar ratio: -0.1994\n",
      "  Stability: 0.8954\n",
      "  Max drawdown: -0.0542\n",
      "  Omega ratio: 0.9667\n",
      "  Sortino ratio: -0.3323\n",
      "  Skew: 0.1295\n",
      "  Kurtosis: 0.4449\n",
      "  Tail ratio: 1.2120\n",
      "  Daily value at risk (95%): -0.0110\n",
      "  Portfolio turnover: nan\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStarting backtest evaluation...\")\n",
    "\n",
    "# Evaluate the loaded agent on the backtest environment\n",
    "# n_eval_episodes=1 is typical for a single chronological backtest run\n",
    "backtest_metrics, backtest_portfolio = loaded_agent.evaluate(eval_env=env_backtest, n_eval_episodes=1, deterministic=True)\n",
    "\n",
    "print(\"\\n--- Backtest Evaluation Metrics ---\")\n",
    "if backtest_metrics:\n",
    "    for key, value in backtest_metrics.items():\n",
    "        if isinstance(value, float):\n",
    "            print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"  No metrics returned from evaluation.\")\n",
    "\n",
    "# Example: Access specific metrics\n",
    "# final_value = backtest_metrics.get('final_portfolio_value_first_episode', INITIAL_BALANCE)\n",
    "# sharpe = backtest_metrics.get('Sharpe ratio', np.nan)\n",
    "# print(f\"\\nFinal portfolio value: ${final_value:,.2f}\")\n",
    "# print(f\"Sharpe ratio: {sharpe:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(99187.49176919078)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_backtest.portfolio_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Skipping plot: Portfolio history not available in env_backtest or not implemented)\n",
      "\n",
      "Notebook execution complete.\n"
     ]
    }
   ],
   "source": [
    "# This cell can be used for more detailed analysis or plotting if desired.\n",
    "# For example, plotting portfolio value over time, asset allocations, etc.\n",
    "# The `env_backtest` object might store history if it's instrumented to do so,\n",
    "# or one could modify the `evaluate` loop to collect more detailed step-by-step information.\n",
    "\n",
    "# Example: If PortfolioEnv was modified to store history of portfolio values:\n",
    "if hasattr(env_backtest, 'history') and 'portfolio_value' in env_backtest.history:\n",
    "   import matplotlib.pyplot as plt\n",
    "   pd.Series(env_backtest.history['portfolio_value']).plot(title=f\"Portfolio Value - Window {WINDOW_TO_ANALYZE} Backtest\")\n",
    "   plt.show()\n",
    "else:\n",
    "   print(\"\\n(Skipping plot: Portfolio history not available in env_backtest or not implemented)\")\n",
    "\n",
    "print(\"\\nNotebook execution complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
