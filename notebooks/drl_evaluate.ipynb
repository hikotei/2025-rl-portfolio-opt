{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "from utils.config import DRLConfig # Assuming DRLConfig can be instantiated from a dict\n",
    "from utils.portfolio import Portfolio\n",
    "from utils.portfolio_env import PortfolioEnv # Make sure this is the correct environment class\n",
    "from utils.drl_train import slice_data # Or replicate logic if it's complex\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Inputs\n",
    "\n",
    "Please specify the following parameters for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIRED: Timestamp of the DRL model training run (e.g., \"20250611_234904\")\n",
    "# This corresponds to the folder name in `../models/`\n",
    "MODEL_TIMESTAMP = \"20250611_234904\"  # <--- CHANGE THIS\n",
    "\n",
    "# OPTIONAL: Specify evaluation period. \n",
    "# If None, the script will try to determine the test period from the DRL config.\n",
    "EVAL_START_DATE = \"2012-01-01\"  # Example: \"2012-01-01\" or None\n",
    "EVAL_END_DATE = \"2022-12-31\"    # Example: \"2022-12-31\" or None\n",
    "\n",
    "# OPTIONAL: Path to MVO results CSV for comparison\n",
    "# Example: \"../results/20250626_0038_mvo_backtest_[2012-01-01,2022-12-31]_daily/mvo_metrics.csv\"\n",
    "MVO_RESULTS_CSV_PATH = \"../results/20250626_0038_mvo_backtest_[2012-01-01,2022-12-31]_daily/mvo_metrics.csv\" # <--- CHANGE THIS or set to None\n",
    "\n",
    "# OPTIONAL: Specify which DRL agents to evaluate from the model timestamp folder\n",
    "# If None or empty list, all agents (agent_seed*_valrew*.zip) in the folder will be evaluated.\n",
    "# Example: [\"agent_seed0_valrew-8.70.zip\", \"agent_seed1_valrew-11.50.zip\"]\n",
    "SPECIFIC_AGENTS_TO_EVALUATE = [] # or None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f\"../models/{MODEL_TIMESTAMP}\"\n",
    "config_path = os.path.join(model_dir, f\"config_{MODEL_TIMESTAMP}.json\")\n",
    "\n",
    "if not os.path.exists(model_dir):\n",
    "    raise FileNotFoundError(f\"Model directory not found: {model_dir}\")\n",
    "if not os.path.exists(config_path):\n",
    "    raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "\n",
    "with open(config_path, 'r') as f:\n",
    "    config_dict = json.load(f)\n",
    "\n",
    "# Manually create DRLConfig object. If DRLConfig cannot take a dict directly,\n",
    "# you might need to instantiate it by passing each key-value pair from config_dict.\n",
    "# This assumes DRLConfig can be created by unpacking a dictionary.\n",
    "try:\n",
    "    drl_config = DRLConfig(**config_dict)\n",
    "except TypeError as e:\n",
    "    print(f\"Error instantiating DRLConfig: {e}\")\n",
    "    print(\"Please ensure DRLConfig can be instantiated from the dictionary keys in the JSON.\")\n",
    "    # Fallback: Create a simple namespace object if DRLConfig is problematic\n",
    "    from argparse import Namespace\n",
    "    drl_config = Namespace(**config_dict)\n",
    "\n",
    "print(f\"Successfully loaded configuration from {config_path}\")\n",
    "print(f\"DRL Config: {drl_config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths (consistent with drl_train.ipynb)\n",
    "# These paths might need adjustment if your data structure is different.\n",
    "DATA_DIR = \"../data/snp_new\" # Or use a path from drl_config if available\n",
    "RETURNS_PATH = os.path.join(DATA_DIR, \"returns_1d.parquet\")\n",
    "PRICES_PATH = os.path.join(DATA_DIR, \"prices_1d.parquet\")\n",
    "VOLA_PATH = os.path.join(DATA_DIR, \"vola_1d.parquet\")\n",
    "\n",
    "if not os.path.exists(RETURNS_PATH):\n",
    "    raise FileNotFoundError(f\"Returns data not found: {RETURNS_PATH}. Please check DATA_DIR.\")\n",
    "if not os.path.exists(PRICES_PATH):\n",
    "    raise FileNotFoundError(f\"Prices data not found: {PRICES_PATH}. Please check DATA_DIR.\")\n",
    "if not os.path.exists(VOLA_PATH):\n",
    "    raise FileNotFoundError(f\"Volatility data not found: {VOLA_PATH}. Please check DATA_DIR.\")\n",
    "\n",
    "df_ret = pd.read_parquet(RETURNS_PATH)\n",
    "df_prices = pd.read_parquet(PRICES_PATH)\n",
    "df_vol = pd.read_parquet(VOLA_PATH)\n",
    "\n",
    "# Ensure datetime index\n",
    "df_ret.index = pd.to_datetime(df_ret.index)\n",
    "df_prices.index = pd.to_datetime(df_prices.index)\n",
    "df_vol.index = pd.to_datetime(df_vol.index)\n",
    "\n",
    "print(\"Data loaded successfully:\")\n",
    "print(f\"Returns shape: {df_ret.shape}, from {df_ret.index.min()} to {df_ret.index.max()}\")\n",
    "print(f\"Prices shape: {df_prices.shape}, from {df_prices.index.min()} to {df_prices.index.max()}\")\n",
    "print(f\"Volatility shape: {df_vol.shape}, from {df_vol.index.min()} to {df_vol.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Determine Evaluation Period and Slice Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if EVAL_START_DATE and EVAL_END_DATE:\n",
    "    eval_start_date = pd.to_datetime(EVAL_START_DATE)\n",
    "    eval_end_date = pd.to_datetime(EVAL_END_DATE)\n",
    "    print(f\"Using user-defined evaluation period: {eval_start_date.date()} to {eval_end_date.date()}\")\n",
    "else:\n",
    "    print(\"Warning: EVAL_START_DATE or EVAL_END_DATE not fully specified by the user.\")\n",
    "    print(\"The DRL training process uses rolling windows, so a single 'test period' from the config isn't directly applicable for a continuous evaluation run.\")\n",
    "    print(f\"Defaulting to the last full year of available data based on df_prices: {df_prices.index.max() - pd.DateOffset(years=1) + pd.DateOffset(days=1)} to {df_prices.index.max()}\")\n",
    "    eval_end_date = df_prices.index.max()\n",
    "    eval_start_date = eval_end_date - pd.DateOffset(years=1) + pd.DateOffset(days=1)\n",
    "    EVAL_START_DATE = eval_start_date.strftime('%Y-%m-%d') # Update global var for reporting\n",
    "    EVAL_END_DATE = eval_end_date.strftime('%Y-%m-%d')     # Update global var for reporting\n",
    "    print(f\"Using default evaluation period: {EVAL_START_DATE} to {EVAL_END_DATE}\")\n",
    "\n",
    "# Slice data according to the evaluation period\n",
    "eval_df_prices = df_prices.loc[eval_start_date:eval_end_date].copy() \n",
    "eval_df_ret = df_ret.loc[eval_start_date:eval_end_date].copy()\n",
    "eval_df_vol = df_vol.loc[eval_start_date:eval_end_date].copy()\n",
    "\n",
    "# Check for sufficient data for the environment window\n",
    "if eval_df_prices.empty or len(eval_df_prices) < drl_config.env_window_size:\n",
    "    raise ValueError(\n",
    "        f\"Insufficient data for the evaluation period {eval_start_date.date()} to {eval_end_date.date()} \"\n",
    "        f\"after slicing. Need at least {drl_config.env_window_size} days for the environment observation window. \"\n",
    "        f\"Found {len(eval_df_prices)} days. Please check your dates or data source.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nEvaluation data shapes after slicing ({eval_start_date.date()} to {eval_end_date.date()}):\")\n",
    "print(f\"Prices: {eval_df_prices.shape}\")\n",
    "print(f\"Returns: {eval_df_ret.shape}\")\n",
    "print(f\"Volatility: {eval_df_vol.shape}\")\n",
    "\n",
    "# Ensure data alignment and select common tickers across all three evaluation dataframes\n",
    "common_tickers = eval_df_ret.columns.intersection(eval_df_prices.columns).intersection(eval_df_vol.columns)\n",
    "eval_df_ret = eval_df_ret[list(common_tickers)]\n",
    "eval_df_prices = eval_df_prices[list(common_tickers)]\n",
    "eval_df_vol = eval_df_vol[list(common_tickers)]\n",
    "\n",
    "if eval_df_ret.empty or eval_df_prices.empty or eval_df_vol.empty:\n",
    "    raise ValueError(\"Dataframes are empty after aligning common tickers. Check data consistency for the selected period.\")\n",
    "\n",
    "print(f\"Data aligned to {len(common_tickers)} common tickers. Example tickers: {list(common_tickers[:5])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load DRL Models and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPECIFIC_AGENTS_TO_EVALUATE and len(SPECIFIC_AGENTS_TO_EVALUATE) > 0:\n",
    "    agent_model_files = [os.path.join(model_dir, fname) for fname in SPECIFIC_AGENTS_TO_EVALUATE]\n",
    "    agent_model_files = [path for path in agent_model_files if os.path.exists(path)]\n",
    "    print(f\"Using user-specified agent models: {agent_model_files}\")\n",
    "else:\n",
    "    agent_model_files = glob.glob(os.path.join(model_dir, \"agent_seed*_valrew*.zip\"))\n",
    "    print(f\"Found {len(agent_model_files)} agent models in {model_dir}:\")\n",
    "    for f in agent_model_files:\n",
    "        print(f\"  - {os.path.basename(f)}\")\n",
    "\n",
    "if not agent_model_files:\n",
    "    raise FileNotFoundError(f\"No agent model files found in {model_dir} matching criteria.\")\n",
    "\n",
    "evaluated_portfolios = {} # To store Portfolio objectskeyed by agent name\n",
    "agent_errors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_path in agent_model_files:\n",
    "    agent_name = os.path.basename(agent_path)\n",
    "    print(f\"\\n--- Evaluating Agent: {agent_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Load the DRL model\n",
    "        model = PPO.load(agent_path, device='cpu') # Assuming CPU for evaluation\n",
    "        print(f\"Loaded model from {agent_path}\")\n",
    "\n",
    "        # 2. Create the Portfolio Environment for evaluation\n",
    "        # Ensure the environment uses the sliced evaluation data\n",
    "        eval_env = PortfolioEnv(\n",
    "            df_prices=eval_df_prices,\n",
    "            df_returns=eval_df_ret,\n",
    "            df_features=eval_df_vol, # Assuming vol is the feature, adjust if more features are used\n",
    "            tickers=list(common_tickers),\n",
    "            initial_balance=drl_config.initial_balance,\n",
    "            window_size=drl_config.env_window_size,\n",
    "            transaction_cost=drl_config.transaction_cost,\n",
    "            reward_scaling=drl_config.reward_scaling,\n",
    "            eta_dsr=drl_config.eta_dsr,\n",
    "            seed=None # No need for a specific seed during deterministic evaluation\n",
    "        )\n",
    "        print(f\"PortfolioEnv created for evaluation with window size {drl_config.env_window_size}\")\n",
    "\n",
    "        # 3. Initialize a Portfolio object to track performance\n",
    "        # The tickers for the portfolio should match those in the environment\n",
    "        portfolio = Portfolio(tickers=list(common_tickers), initial_balance=drl_config.initial_balance)\n",
    "        print(f\"Portfolio tracker initialized with balance: {portfolio.initial_balance}\")\n",
    "\n",
    "        # 4. Run the evaluation loop\n",
    "        obs, info = eval_env.reset()\n",
    "        done = False\n",
    "        episode_rewards = []\n",
    "        \n",
    "        # Record initial state\n",
    "        initial_date = eval_env.current_date()\n",
    "        if initial_date:\n",
    "             portfolio.update(current_prices=eval_df_prices.loc[initial_date], date=initial_date)\n",
    "        else:\n",
    "            print(\"Warning: Could not get initial date from environment for portfolio recording.\")\n",
    "\n",
    "        print(f\"Starting evaluation loop from {eval_env.current_date()}...\")\n",
    "        num_steps = 0\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_rewards.append(reward)\n",
    "            num_steps += 1\n",
    "\n",
    "            # Update the portfolio object with the new state from the environment\n",
    "            current_date = eval_env.current_date()\n",
    "            current_prices_series = eval_df_prices.loc[current_date]\n",
    "            \n",
    "            # The environment internally manages its portfolio state after an action.\n",
    "            # We need to get the weights *after* the action has been processed by the env.\n",
    "            # The Portfolio class's update_rebalance expects target weights if rebalancing,\n",
    "            # or just new prices if only updating values. \n",
    "            # Here, the DRL agent's action *is* the target weights (plus cash).\n",
    "            # The env.portfolio object reflects the state *after* the action.\n",
    "            \n",
    "            # Sync our external Portfolio tracker with the environment's internal one.\n",
    "            portfolio.current_balance = eval_env.portfolio.current_balance\n",
    "            portfolio.cash = eval_env.portfolio.cash\n",
    "            portfolio.positions = eval_env.portfolio.positions.copy() # copy to be safe\n",
    "            portfolio.weights = eval_env.portfolio.weights.copy()\n",
    "            portfolio.w_c = eval_env.portfolio.w_c\n",
    "            \n",
    "            # Record this state in our external portfolio's history\n",
    "            portfolio.history.append({\n",
    "                'date': current_date,\n",
    "                'portfolio_value': portfolio.current_balance,\n",
    "                'cash': portfolio.cash,\n",
    "                'w_c': portfolio.w_c,\n",
    "                **{f'w_{t}': portfolio.weights.get(t, 0.0) for t in common_tickers},\n",
    "                **{f's_{t}': portfolio.positions.get(t, 0.0) for t in common_tickers}\n",
    "            })\n",
    "            \n",
    "            if num_steps % 252 == 0: # Log progress every year (approx)\n",
    "                print(f\"  Step {num_steps}, Date: {current_date}, Portfolio Value: {portfolio.current_balance:.2f}\")\n",
    "\n",
    "        evaluated_portfolios[agent_name] = portfolio\n",
    "        mean_reward = np.mean(episode_rewards) if episode_rewards else 0\n",
    "        print(f\"Evaluation complete for {agent_name}. Steps: {num_steps}. Mean reward: {mean_reward:.4f}\")\n",
    "        print(f\"Final portfolio value: {portfolio.current_balance:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR evaluating agent {agent_name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        agent_errors[agent_name] = str(e)\n",
    "\n",
    "if not evaluated_portfolios:\n",
    "    print(\"\\nNo agents were successfully evaluated. Check errors above.\")\n",
    "else:\n",
    "    print(f\"\\nSuccessfully evaluated {len(evaluated_portfolios)} agents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate DRL Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drl_performance_metrics_list = []\n",
    "\n",
    "if not evaluated_portfolios:\n",
    "    print(\"No portfolios were evaluated, so no metrics to calculate.\")\n",
    "else:\n",
    "    for agent_name, portfolio_obj in evaluated_portfolios.items():\n",
    "        print(f\"Calculating metrics for {agent_name}...\")\n",
    "        try:\n",
    "            # Ensure the portfolio history is not empty before calculating metrics\n",
    "            if not portfolio_obj.history:\n",
    "                print(f\"  Skipping metrics for {agent_name}: Portfolio history is empty.\")\n",
    "                # Add a record with NaNs or default values if desired, or just skip\n",
    "                metrics = {'Agent': agent_name, 'Error': 'Empty history'}\n",
    "            else:\n",
    "                # The risk_free_rate can be a parameter, e.g., from drl_config or a fixed value\n",
    "                risk_free_rate = getattr(drl_config, 'risk_free_rate', 0.0) # Default to 0 if not in config\n",
    "                metrics = portfolio_obj.calc_metrics(risk_free_rate=risk_free_rate)\n",
    "                metrics['Agent'] = agent_name # Add agent name for identification\n",
    "            \n",
    "            drl_performance_metrics_list.append(metrics)\n",
    "            print(f\"  Metrics calculated for {agent_name}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error calculating metrics for {agent_name}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            drl_performance_metrics_list.append({'Agent': agent_name, 'Error': str(e)})\n",
    "\n",
    "if drl_performance_metrics_list:\n",
    "    df_drl_metrics = pd.DataFrame(drl_performance_metrics_list)\n",
    "    # Reorder columns to have 'Agent' first, then 'Error' if it exists, then others\n",
    "    cols = ['Agent']\n",
    "    if 'Error' in df_drl_metrics.columns: # Check if Error column was added for any agent\n",
    "        cols.append('Error')\n",
    "    cols.extend([col for col in df_drl_metrics.columns if col not in cols])\n",
    "    df_drl_metrics = df_drl_metrics[cols]\n",
    "    \n",
    "    print(\"\\nDRL Performance Metrics Summary:\")\n",
    "    print(df_drl_metrics.to_markdown(index=False))\n",
    "else:\n",
    "    print(\"\\nNo DRL performance metrics were generated.\")\n",
    "    df_drl_metrics = pd.DataFrame() # Ensure df_drl_metrics exists for later steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Results Directory and Save Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_dir_name = f\"drl_eval_{MODEL_TIMESTAMP}_{eval_timestamp}\"\n",
    "results_save_dir = os.path.join(\"../results\", results_dir_name)\n",
    "\n",
    "os.makedirs(results_save_dir, exist_ok=True)\n",
    "print(f\"Created results directory: {results_save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save info.txt\n",
    "info_content = f\"DRL Evaluation Run Information\\n\"\n",
    "info_content += f\"------------------------------------\\n\"\n",
    "info_content += f\"Evaluation Timestamp: {eval_timestamp}\\n\"\n",
    "info_content += f\"Source DRL Model Timestamp: {MODEL_TIMESTAMP}\\n\"\n",
    "info_content += f\"Evaluation Start Date: {EVAL_START_DATE}\\n\"\n",
    "info_content += f\"Evaluation End Date: {EVAL_END_DATE}\\n\"\n",
    "info_content += f\"\\nEvaluated DRL Agents from {model_dir}:\\n\"\n",
    "if evaluated_portfolios:\n",
    "    for agent_name in evaluated_portfolios.keys():\n",
    "        info_content += f\"  - {agent_name}\\n\"\n",
    "else:\n",
    "    info_content += \"  - No agents were successfully evaluated.\\n\"\n",
    "    \n",
    "if agent_errors:\n",
    "    info_content += f\"\\nErrors during evaluation for some agents:\\n\"\n",
    "    for agent_name, error_msg in agent_errors.items():\n",
    "        info_content += f\"  - {agent_name}: {error_msg}\\n\"\n",
    "\n",
    "info_content += f\"\\nMVO Comparison File: {MVO_RESULTS_CSV_PATH if MVO_RESULTS_CSV_PATH else 'Not specified'}\\n\"\n",
    "\n",
    "info_file_path = os.path.join(results_save_dir, \"info.txt\")\n",
    "with open(info_file_path, 'w') as f:\n",
    "    f.write(info_content)\n",
    "print(f\"Saved evaluation info to: {info_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DRL performance metrics CSV\n",
    "if not df_drl_metrics.empty:\n",
    "    drl_metrics_path = os.path.join(results_save_dir, \"drl_performance_metrics.csv\")\n",
    "    df_drl_metrics.to_csv(drl_metrics_path, index=False)\n",
    "    print(f\"Saved DRL performance metrics to: {drl_metrics_path}\")\n",
    "else:\n",
    "    print(\"DRL performance metrics DataFrame is empty. Not saving.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save portfolio history for each DRL agent\n",
    "if evaluated_portfolios:\n",
    "    for agent_name, portfolio_obj in evaluated_portfolios.items():\n",
    "        if portfolio_obj.history:\n",
    "            history_df = portfolio_obj.get_history()\n",
    "            # Sanitize agent_name for filename (e.g., remove .zip)\n",
    "            safe_agent_name = agent_name.replace(\".zip\", \"\").replace(\".\", \"_\")\n",
    "            history_filename = f\"drl_agent_{safe_agent_name}_portfolio_history.csv\"\n",
    "            history_path = os.path.join(results_save_dir, history_filename)\n",
    "            history_df.to_csv(history_path, index=True) # index=True because history has date index\n",
    "            print(f\"Saved portfolio history for {agent_name} to: {history_path}\")\n",
    "        else:\n",
    "            print(f\"Portfolio history for {agent_name} is empty. Not saving.\")\n",
    "else:\n",
    "    print(\"No evaluated portfolios. Not saving any portfolio histories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare with MVO Results (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MVO_RESULTS_CSV_PATH and os.path.exists(MVO_RESULTS_CSV_PATH):\n",
    "    print(f\"\\n--- MVO Comparison --- C\")\n",
    "    print(f\"Loading MVO results from: {MVO_RESULTS_CSV_PATH}\")\n",
    "    try:\n",
    "        df_mvo_metrics = pd.read_csv(MVO_RESULTS_CSV_PATH)\n",
    "        \n",
    "        # The MVO metrics CSV might have an unnamed index column if saved with index=True\n",
    "        if 'Unnamed: 0' in df_mvo_metrics.columns:\n",
    "            df_mvo_metrics = df_mvo_metrics.rename(columns={'Unnamed: 0': 'MVO_Strategy_ID'})\n",
    "            # Or df_mvo_metrics.set_index('MVO_Strategy_ID', inplace=True) if that's preferred\n",
    "        \n",
    "        print(\"\\nMVO Performance Metrics:\")\n",
    "        print(df_mvo_metrics.to_markdown(index=False))\n",
    "        \n",
    "        # Save MVO metrics to the results folder as well for completeness\n",
    "        mvo_metrics_copy_path = os.path.join(results_save_dir, \"mvo_comparison_metrics.csv\")\n",
    "        df_mvo_metrics.to_csv(mvo_metrics_copy_path, index=False)\n",
    "        print(f\"\\nCopied MVO metrics to: {mvo_metrics_copy_path}\")\n",
    "\n",
    "        if not df_drl_metrics.empty:\n",
    "            print(\"\\nDRL Performance Metrics (repeated for comparison):\")\n",
    "            print(df_drl_metrics.to_markdown(index=False))\n",
    "            \n",
    "            # Attempt a simple concatenation for side-by-side view if structures are somewhat similar\n",
    "            # This is a basic comparison; more sophisticated merging might be needed depending on exact formats\n",
    "            try:\n",
    "                # Add a 'Type' column to distinguish DRL from MVO\n",
    "                df_drl_metrics_typed = df_drl_metrics.copy()\n",
    "                df_drl_metrics_typed['Type'] = 'DRL'\n",
    "                # Standardize 'Agent' column name for MVO if possible, or use a generic ID\n",
    "                # For now, assuming MVO has a 'lookback' or 'MVO_Strategy_ID' that can act as an identifier\n",
    "                df_mvo_metrics_typed = df_mvo_metrics.copy()\n",
    "                df_mvo_metrics_typed['Type'] = 'MVO'\n",
    "                if 'MVO_Strategy_ID' in df_mvo_metrics_typed.columns:\n",
    "                    df_mvo_metrics_typed.rename(columns={'MVO_Strategy_ID': 'Agent'}, inplace=True)\n",
    "                elif 'lookback' in df_mvo_metrics_typed.columns:\n",
    "                     df_mvo_metrics_typed.rename(columns={'lookback': 'Agent'}, inplace=True)\n",
    "                else: # Add a placeholder agent column if no clear identifier\n",
    "                    df_mvo_metrics_typed['Agent'] = 'MVO_lookback_' + df_mvo_metrics_typed.index.astype(str)\n",
    "\n",
    "                # Ensure 'Agent' column is string type for both before concat if it exists\n",
    "                if 'Agent' in df_drl_metrics_typed.columns:\n",
    "                    df_drl_metrics_typed['Agent'] = df_drl_metrics_typed['Agent'].astype(str)\n",
    "                if 'Agent' in df_mvo_metrics_typed.columns:\n",
    "                    df_mvo_metrics_typed['Agent'] = df_mvo_metrics_typed['Agent'].astype(str)\n",
    "\n",
    "                df_combined_metrics = pd.concat([df_drl_metrics_typed, df_mvo_metrics_typed], ignore_index=True)\n",
    "                \n",
    "                # Reorder columns for better readability\n",
    "                cols_combined = ['Type', 'Agent']\n",
    "                if 'Error' in df_combined_metrics.columns:\n",
    "                    cols_combined.append('Error')\n",
    "                cols_combined.extend([col for col in df_combined_metrics.columns if col not in cols_combined])\n",
    "                df_combined_metrics = df_combined_metrics[cols_combined]\n",
    "                \n",
    "                print(\"\\nCombined DRL and MVO Metrics:\")\n",
    "                print(df_combined_metrics.to_markdown(index=False))\n",
    "                \n",
    "                combined_metrics_path = os.path.join(results_save_dir, \"drl_vs_mvo_metrics_comparison.csv\")\n",
    "                df_combined_metrics.to_csv(combined_metrics_path, index=False)\n",
    "                print(f\"Saved combined metrics comparison to: {combined_metrics_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Could not create a combined comparison table: {e}\")\n",
    "        else:\n",
    "            print(\"DRL metrics are empty, cannot show side-by-side comparison.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing MVO results: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "elif MVO_RESULTS_CSV_PATH:\n",
    "    print(f\"\\nSpecified MVO results file not found: {MVO_RESULTS_CSV_PATH}\")\n",
    "else:\n",
    "    print(\"\\nNo MVO results CSV path specified. Skipping MVO comparison.\")\n",
    "\n",
    "print(\"\\n--- Evaluation Notebook Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
