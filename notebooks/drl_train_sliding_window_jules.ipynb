{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming utils are in parent directory or PYTHONPATH is set\n",
    "from utils.portfolio_env import PortfolioEnv\n",
    "from utils.drl_agent_jules import DRLAgent  # Import the modified agent\n",
    "\n",
    "# For learning rate schedule\n",
    "from typing import Callable\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Logging\n",
    "\n",
    "This notebook logs training progress using TensorBoard. Logs for each agent and window will be saved in subdirectories within the `../tensorboard_logs/` directory (relative to this notebook's location).\n",
    "\n",
    "To view the logs:\n",
    "1. Open a terminal or command prompt.\n",
    "2. Navigate to the directory *containing* the `tensorboard_logs` directory (i.e., the root of this repository if you are running the notebook from the `notebooks` folder).\n",
    "3. Run the command: `tensorboard --logdir tensorboard_logs/`\n",
    "4. Open the URL provided by TensorBoard (usually http://localhost:6006/) in your web browser.\n",
    "\n",
    "You should see experiments named like `PPO_WindowX_AgentY_SeedZ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Configuration ---\n",
    "N_WINDOWS = 2  # 10 in paper\n",
    "AGENTS_PER_WINDOW = 2  # 5 in paper\n",
    "BASE_START_YEAR = 2006\n",
    "\n",
    "# Data paths\n",
    "PRICE_DATA_PATH = \"../data/prices.parquet\"\n",
    "RETURNS_DATA_PATH = \"../data/returns.parquet\"\n",
    "VOLA_DATA_PATH = \"../data/vola.parquet\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_SAVE_DIR = f\"../models/sliding_window_jules/{timestamp}/\"\n",
    "TENSORBOARD_LOG_DIR = f\"../tensorboard_logs/{timestamp}/\"\n",
    "\n",
    "# Ensure model save directory exists\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(TENSORBOARD_LOG_DIR, exist_ok=True)\n",
    "\n",
    "# --- DRL Agent Hyperparameters (from paper) ---\n",
    "N_ENVS = 10\n",
    "TOTAL_TIMESTEPS_PER_ROUND = 10**6 # 7_500_000  in paper\n",
    "N_STEPS_PER_ENV = 252 * 3  # n_steps = 252 * 3 * n_envs (this is per env for PPO buffer)\n",
    "# total buffer size before update = N_STEPS_PER_ENV * N_ENVS\n",
    "\n",
    "BATCH_SIZE = 1260\n",
    "N_EPOCHS = 16\n",
    "GAMMA = 0.9\n",
    "GAE_LAMBDA = 0.9\n",
    "CLIP_RANGE = 0.25\n",
    "LOG_STD_INIT = -1.0\n",
    "POLICY_KWARGS = dict(\n",
    "    activation_fn=torch.nn.Tanh,\n",
    "    net_arch=[64, 64],  # Shared layers for policy and value networks\n",
    "    log_std_init=LOG_STD_INIT,\n",
    ")\n",
    "\n",
    "# Learning rate schedule: linear decay from 3e-4 to 1e-5\n",
    "INITIAL_LR = 3e-4\n",
    "FINAL_LR = 1e-5\n",
    "\n",
    "\n",
    "def linear_schedule(\n",
    "    initial_value: float, final_value: float\n",
    ") -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :param final_value: Final learning rate.\n",
    "    :return: schedule that computes current learning rate depending on progress remaining (1.0 -> 0.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1.0 to 0.0\n",
    "        \"\"\"\n",
    "        return final_value + progress_remaining * (initial_value - final_value)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "LEARNING_RATE_SCHEDULE = linear_schedule(INITIAL_LR, FINAL_LR)\n",
    "\n",
    "# --- PortfolioEnv Parameters ---\n",
    "ENV_WINDOW_SIZE = 60  # Lookback window for features in PortfolioEnv\n",
    "TRANSACTION_COST = 0.0  # As per paper (or can be adjusted)\n",
    "INITIAL_BALANCE = 100_000\n",
    "REWARD_SCALING = 1.0\n",
    "ETA_DSR = 1 / 252  # For Differential Sharpe Ratio in PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the full datasets once\n",
    "try:\n",
    "    print(\"Loading data...\")\n",
    "    prices_df_full = pd.read_parquet(PRICE_DATA_PATH)\n",
    "    returns_df_full = pd.read_parquet(RETURNS_DATA_PATH)\n",
    "    vola_df_full = pd.read_parquet(VOLA_DATA_PATH)\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "    # Ensure DataFrames have DateTimeIndex\n",
    "    for df in [prices_df_full, returns_df_full, vola_df_full]:\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # print(\"\\nPrice Data Head:\")\n",
    "    # print(prices_df_full.head())\n",
    "    # print(\"\\nReturns Data Head:\")\n",
    "    # print(returns_df_full.head())\n",
    "    # print(\"\\nVolatility Data Head:\")\n",
    "    # print(vola_df_full.head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Data file not found. {e}\")\n",
    "    print(\"Please ensure data is generated and paths are correct in Cell 2.\")\n",
    "    # Stop execution or raise error if data is critical for notebook to run\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(\n",
    "    year_start,\n",
    "    num_train_years,\n",
    "    num_val_years,\n",
    "    num_test_years,\n",
    "    prices_df,\n",
    "    returns_df,\n",
    "    vol_df,\n",
    "):\n",
    "    \"\"\"Slices data for a given window configuration.\"\"\"\n",
    "\n",
    "    train_start_date = pd.to_datetime(f\"{year_start}-01-01\")\n",
    "    train_end_date = pd.to_datetime(f\"{year_start + num_train_years - 1}-12-31\")\n",
    "\n",
    "    val_start_date = pd.to_datetime(f\"{year_start + num_train_years}-01-01\")\n",
    "    val_end_date = pd.to_datetime(\n",
    "        f\"{year_start + num_train_years + num_val_years - 1}-12-31\"\n",
    "    )\n",
    "\n",
    "    test_start_date = pd.to_datetime(\n",
    "        f\"{year_start + num_train_years + num_val_years}-01-01\"\n",
    "    )\n",
    "    test_end_date = pd.to_datetime(\n",
    "        f\"{year_start + num_train_years + num_val_years + num_test_years - 1}-12-31\"\n",
    "    )\n",
    "\n",
    "    print(f\"  Train Period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "    print(f\"  Val Period  : {val_start_date.date()} to {val_end_date.date()}\")\n",
    "    print(f\"  Test Period : {test_start_date.date()} to {test_end_date.date()}\")\n",
    "\n",
    "    # Slicing (ensure index is datetime)\n",
    "    train_prices = prices_df[train_start_date:train_end_date]\n",
    "    train_returns = returns_df[train_start_date:train_end_date]\n",
    "    train_vola = vol_df[train_start_date:train_end_date]\n",
    "\n",
    "    val_prices = prices_df[val_start_date:val_end_date]\n",
    "    val_returns = returns_df[val_start_date:val_end_date]\n",
    "    val_vola = vol_df[val_start_date:val_end_date]\n",
    "\n",
    "    test_prices = prices_df[test_start_date:test_end_date]\n",
    "    test_returns = returns_df[test_start_date:test_end_date]\n",
    "    test_vola = vol_df[test_start_date:test_end_date]\n",
    "\n",
    "    # Basic check for empty slices which can halt env creation\n",
    "    if train_prices.empty or val_prices.empty or test_prices.empty:\n",
    "        print(\n",
    "            \"WARNING: One or more data slices are empty. Check date ranges and data availability.\"\n",
    "        )\n",
    "        # Potentially raise an error or handle as per requirements\n",
    "\n",
    "    return (\n",
    "        (train_prices, train_returns, train_vola),\n",
    "        (val_prices, val_returns, val_vola),\n",
    "        (test_prices, test_returns, test_vola),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Window 1/2 (Train Year Start: 2006) ---\n",
      "  Train Period: 2006-01-01 to 2010-12-31\n",
      "  Val Period  : 2011-01-01 to 2011-12-31\n",
      "  Test Period : 2012-01-01 to 2012-12-31\n",
      "  Training Agent 1/2 with seed 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c4a9cb4143437cbc97e53802f7062e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for 1000000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 1000000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Window1_Agent1_Seed0' saved in directory: ../tensorboard_logs/20250531_205413/\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 0.9366\n",
      "    Agent saved to: ../models/sliding_window_jules/20250531_205413/agent_win1_seed0_valrew0.94.zip\n",
      "    New best agent for this window with validation reward: 0.9366\n",
      "  Training Agent 2/2 with seed 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ff9ffdbc5b48fea7ad8b77fafce0a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for 1000000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 1000000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Window1_Agent2_Seed1' saved in directory: ../tensorboard_logs/20250531_205413/\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 0.7091\n",
      "    Agent saved to: ../models/sliding_window_jules/20250531_205413/agent_win1_seed1_valrew0.71.zip\n",
      "  Backtesting best agent for Window 1 (../models/sliding_window_jules/20250531_205413/agent_win1_seed0_valrew0.94.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/20250531_205413/agent_win1_seed0_valrew0.94.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 1:\n",
      "      Annual return: -0.0136138887051519\n",
      "      Cumulative returns: -0.010227891285999546\n",
      "      Annual volatility: 0.11287923825024124\n",
      "      Sharpe ratio: -0.24218885407465138\n",
      "      Calmar ratio: -0.2959433792822436\n",
      "      Stability: 0.8985700924498159\n",
      "      Max drawdown: -0.04600166673155483\n",
      "      Omega ratio: 0.961026191085054\n",
      "      Sortino ratio: -0.3871844855323953\n",
      "      Skew: 0.08620512658961246\n",
      "      Kurtosis: 0.5085734993078597\n",
      "      Tail ratio: 1.2229594977637874\n",
      "      Daily value at risk (95%): -0.010403084798213391\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: -0.14644622867117363\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 98977.21087140005\n",
      "--- Starting Window 2/2 (Train Year Start: 2007) ---\n",
      "  Train Period: 2007-01-01 to 2011-12-31\n",
      "  Val Period  : 2012-01-01 to 2012-12-31\n",
      "  Test Period : 2013-01-01 to 2013-12-31\n",
      "  Training Agent 1/2 with seed 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c3fb97d88524903af6b0a3062fc6bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/20250531_205413/agent_win1_seed0_valrew0.94.zip\n",
      "Model loaded from ../models/sliding_window_jules/20250531_205413/agent_win1_seed0_valrew0.94.zip\n",
      "    Starting training for 1000000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 1000000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Window2_Agent1_Seed2' saved in directory: ../tensorboard_logs/20250531_205413/\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -0.4063\n",
      "    Agent saved to: ../models/sliding_window_jules/20250531_205413/agent_win2_seed2_valrew-0.41.zip\n",
      "    New best agent for this window with validation reward: -0.4063\n",
      "  Training Agent 2/2 with seed 3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a260c8b2219c4580823b9ea3a45014ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/20250531_205413/agent_win1_seed0_valrew0.94.zip\n",
      "Model loaded from ../models/sliding_window_jules/20250531_205413/agent_win1_seed0_valrew0.94.zip\n",
      "    Starting training for 1000000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 1000000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Window2_Agent2_Seed3' saved in directory: ../tensorboard_logs/20250531_205413/\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 0.0972\n",
      "    Agent saved to: ../models/sliding_window_jules/20250531_205413/agent_win2_seed3_valrew0.10.zip\n",
      "    New best agent for this window with validation reward: 0.0972\n",
      "  Backtesting best agent for Window 2 (../models/sliding_window_jules/20250531_205413/agent_win2_seed3_valrew0.10.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/20250531_205413/agent_win2_seed3_valrew0.10.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 2:\n",
      "      Annual return: 0.0257492772058614\n",
      "      Cumulative returns: 0.019456134240499523\n",
      "      Annual volatility: 0.10605036983949467\n",
      "      Sharpe ratio: 0.10423545782037345\n",
      "      Calmar ratio: 0.46144547351441667\n",
      "      Stability: 0.9041179563505012\n",
      "      Max drawdown: -0.055801343135413656\n",
      "      Omega ratio: 1.0168103518826235\n",
      "      Sortino ratio: 0.16003076374470915\n",
      "      Skew: -0.26329324451356545\n",
      "      Kurtosis: 0.6404627057807439\n",
      "      Tail ratio: 0.9455245664454488\n",
      "      Daily value at risk (95%): -0.011435571565433188\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: 2.5734197213743237\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 101945.61342404995\n",
      "\n",
      "--- All Windows Processed ---\n",
      "Summary of Best Agent Paths:\n",
      "Window 1: ../models/sliding_window_jules/20250531_205413/agent_win1_seed0_valrew0.94.zip\n",
      "Window 2: ../models/sliding_window_jules/20250531_205413/agent_win2_seed3_valrew0.10.zip\n",
      "\n",
      "Summary of Backtest Results:\n",
      "Window 1 (completed):\n",
      "    Annual return: -0.0136\n",
      "    Cumulative returns: -0.0102\n",
      "    Annual volatility: 0.1129\n",
      "    Sharpe ratio: -0.2422\n",
      "    Calmar ratio: -0.2959\n",
      "    Stability: 0.8986\n",
      "    Max drawdown: -0.0460\n",
      "    Omega ratio: 0.9610\n",
      "    Sortino ratio: -0.3872\n",
      "    Skew: 0.0862\n",
      "    Kurtosis: 0.5086\n",
      "    Tail ratio: 1.2230\n",
      "    Daily value at risk (95%): -0.0104\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: -0.1464\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 98977.2109\n",
      "Window 2 (completed):\n",
      "    Annual return: 0.0257\n",
      "    Cumulative returns: 0.0195\n",
      "    Annual volatility: 0.1061\n",
      "    Sharpe ratio: 0.1042\n",
      "    Calmar ratio: 0.4614\n",
      "    Stability: 0.9041\n",
      "    Max drawdown: -0.0558\n",
      "    Omega ratio: 1.0168\n",
      "    Sortino ratio: 0.1600\n",
      "    Skew: -0.2633\n",
      "    Kurtosis: 0.6405\n",
      "    Tail ratio: 0.9455\n",
      "    Daily value at risk (95%): -0.0114\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: 2.5734\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 101945.6134\n"
     ]
    }
   ],
   "source": [
    "all_backtest_results = []\n",
    "best_agent_paths_per_window = [] # To store path of the best agent for each window\n",
    "\n",
    "# --- Main Loop for Sliding Windows ---\n",
    "for i_window in range(N_WINDOWS):\n",
    "    current_start_year = BASE_START_YEAR + i_window\n",
    "    print(f\"--- Starting Window {i_window+1}/{N_WINDOWS} (Train Year Start: {current_start_year}) ---\")\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    # 1. Slice Data for the current window\n",
    "    # 5 years train, 1 year validation, 1 year test\n",
    "    train_data, val_data, test_data = slice_data(\n",
    "        year_start=current_start_year,\n",
    "        num_train_years=5,\n",
    "        num_val_years=1,\n",
    "        num_test_years=1,\n",
    "        prices_df=prices_df_full,\n",
    "        returns_df=returns_df_full,\n",
    "        vol_df=vola_df_full\n",
    "    )\n",
    "    \n",
    "    # Unpack data\n",
    "    (train_prices, train_returns, train_vola) = train_data\n",
    "    (val_prices, val_returns, val_vola) = val_data\n",
    "    (test_prices, test_returns, test_vola) = test_data\n",
    "\n",
    "    # Check if any crucial dataframe is too short (e.g., shorter than ENV_WINDOW_SIZE)\n",
    "    # PortfolioEnv requires at least `window_size` days of data to start.\n",
    "    min_data_len = ENV_WINDOW_SIZE + 1 # Need at least window_size + 1 for one step\n",
    "    if len(train_prices) < min_data_len or len(val_prices) < min_data_len or len(test_prices) < min_data_len:\n",
    "        print(f\"SKIPPING Window {i_window+1} due to insufficient data length for one or more periods.\")\n",
    "        print(f\"  Train length: {len(train_prices)}, Val length: {len(val_prices)}, Test length: {len(test_prices)}\")\n",
    "        print(f\"  Required minimum: {min_data_len}\")\n",
    "        best_agent_paths_per_window.append(None) # Mark as skipped\n",
    "        all_backtest_results.append({\"window\": i_window+1, \"status\": \"skipped_insufficient_data\", \"metrics\": {}})\n",
    "        continue\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    # 2. Create Training and Validation Environments\n",
    "    # These envs are re-created for each agent to ensure fresh state and correct data.\n",
    "    # However, the data slice itself is per-window.\n",
    "\n",
    "    best_agent_for_window = None\n",
    "    best_val_reward = -np.inf\n",
    "    \n",
    "    # --- Inner Loop for Training AGENTS_PER_WINDOW Agents ---\n",
    "    for i_agent in range(AGENTS_PER_WINDOW):\n",
    "        agent_seed = (i_window * AGENTS_PER_WINDOW) + i_agent # Unique seed for each agent run\n",
    "        print(f\"  Training Agent {i_agent+1}/{AGENTS_PER_WINDOW} with seed {agent_seed}...\")\n",
    "\n",
    "        # Create environments for this specific agent\n",
    "        # Training Env\n",
    "        env_train_config = {\n",
    "            'returns_df': train_returns, 'prices_df': train_prices, 'vol_df': train_vola,\n",
    "            'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "            'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "        }\n",
    "        # The DRLAgent class will use this first env to understand structure for SubprocVecEnv\n",
    "        # This single_env_for_init is just for the DRLAgent constructor to get parameters.\n",
    "        # The actual training will use N_ENVS created by DRLAgent.\n",
    "        single_env_for_init_train = PortfolioEnv(**env_train_config)\n",
    "\n",
    "        # Validation Env (single, not vectorized for evaluation)\n",
    "        env_val_config = {\n",
    "            'returns_df': val_returns, 'prices_df': val_prices, 'vol_df': val_vola,\n",
    "            'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "            'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "        }\n",
    "        env_val = PortfolioEnv(**env_val_config)\n",
    "        \n",
    "        # Instantiate DRL Agent\n",
    "        agent = DRLAgent(\n",
    "            env=single_env_for_init_train, # Pass the sample env for DRLAgent to clone\n",
    "            n_envs=N_ENVS,\n",
    "            policy_kwargs=POLICY_KWARGS,\n",
    "            n_steps=N_STEPS_PER_ENV, # n_steps per environment for PPO\n",
    "            batch_size=BATCH_SIZE,\n",
    "            n_epochs=N_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE_SCHEDULE,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            clip_range=CLIP_RANGE,\n",
    "            seed=agent_seed,\n",
    "            tensorboard_log=TENSORBOARD_LOG_DIR\n",
    "        )\n",
    "\n",
    "        # Agent Seeding: Load previous window's best agent if not the first window\n",
    "        if i_window > 0 and best_agent_paths_per_window[i_window-1] is not None:\n",
    "            previous_best_agent_path = best_agent_paths_per_window[i_window-1]\n",
    "            print(f\"    Seeding agent from: {previous_best_agent_path}\")\n",
    "            # The env for load_from_file should match the new training env structure\n",
    "            # DRLAgent's load_from_file uses its internal self.env by default if env=None.\n",
    "            # This self.env is already configured with N_ENVS and the new train_data.\n",
    "            agent.load_from_file(path=previous_best_agent_path, env=None) \n",
    "            agent.model.set_random_seed(agent_seed) # Ensure the loaded model uses the new agent_seed\n",
    "                                   \n",
    "        # Train the agent\n",
    "        print(f\"    Starting training for {TOTAL_TIMESTEPS_PER_ROUND} timesteps...\")\n",
    "        # Note: Training can be very long. For testing, reduce TOTAL_TIMESTEPS_PER_ROUND.\n",
    "        # Example: agent.train(total_timesteps=10000, tb_log_name=f\"ppo_win{i_window}_agent{i_agent}\")\n",
    "        agent.train(\n",
    "            total_timesteps=TOTAL_TIMESTEPS_PER_ROUND, \n",
    "            tb_experiment_name=f\"PPO_Window{i_window+1}_Agent{i_agent+1}_Seed{agent_seed}\",\n",
    "        )\n",
    "        \n",
    "        # Evaluate the agent on the validation set\n",
    "        print(\"    Evaluating agent on validation set...\")\n",
    "        # The evaluate method in DRLAgentJules is designed for a single eval_env\n",
    "        val_metrics = agent.evaluate(eval_env=env_val, n_eval_episodes=1) # Use 1 episode for validation speed\n",
    "        current_val_reward = val_metrics.get(\"mean_reward\", -np.inf)\n",
    "        print(f\"    Validation Mean Reward: {current_val_reward:.4f}\")\n",
    "        \n",
    "        # Save this agent\n",
    "        current_agent_model_name = f\"agent_win{i_window+1}_seed{agent_seed}_valrew{current_val_reward:.2f}.zip\"\n",
    "        current_agent_save_path = os.path.join(MODEL_SAVE_DIR, current_agent_model_name)\n",
    "        agent.save(current_agent_save_path)\n",
    "        print(f\"    Agent saved to: {current_agent_save_path}\")\n",
    "\n",
    "        if current_val_reward > best_val_reward:\n",
    "            best_val_reward = current_val_reward\n",
    "            best_agent_for_window_path = current_agent_save_path \n",
    "            print(f\"    New best agent for this window with validation reward: {best_val_reward:.4f}\")\n",
    "\n",
    "        # Clean up to free memory if needed, though Python's GC should handle agent and envs\n",
    "        del agent\n",
    "        del single_env_for_init_train\n",
    "        del env_val\n",
    "        torch.cuda.empty_cache() # If using GPU\n",
    "\n",
    "    best_agent_paths_per_window.append(best_agent_for_window_path if 'best_agent_for_window_path' in locals() and best_agent_for_window_path is not None else None)\n",
    "    \n",
    "    if best_agent_paths_per_window[-1] is None:\n",
    "        print(f\"  No best agent found or saved for window {i_window+1}. Skipping backtest.\")\n",
    "        all_backtest_results.append({\"window\": i_window+1, \"status\": \"no_best_agent\", \"metrics\": {}})\n",
    "        continue\n",
    "    \n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    # 3. Backtest the best agent of the window\n",
    "    print(f\"  Backtesting best agent for Window {i_window+1} ({best_agent_paths_per_window[-1]})\" )\n",
    "    \n",
    "    # Create Backtesting Environment\n",
    "    env_test_config = {\n",
    "        'returns_df': test_returns, 'prices_df': test_prices, 'vol_df': test_vola,\n",
    "        'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "        'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "    }\n",
    "    env_test = PortfolioEnv(**env_test_config)\n",
    "    \n",
    "    # Load the best agent for this window\n",
    "    # For loading, we need a sample env. We can create a dummy one or use env_test.\n",
    "    # The DRLAgent needs an env instance for its constructor to derive parameters for make_env.\n",
    "    # So, we pass a temporary env instance here.\n",
    "    # The actual self.env for the loaded model will be set by PPO.load(env=env_test)\n",
    "    \n",
    "    # Create a temporary env instance for DRLAgent initialization before loading the model.\n",
    "    # This env should reflect the structure the agent was trained on (e.g. observation/action space from PortfolioEnv)\n",
    "    # but the actual data doesn't matter as much for just loading.\n",
    "    # However, to be safe, use a structure similar to what it was trained on.\n",
    "    # The DRLAgent.load method sets the environment for the loaded PPO model.\n",
    "    \n",
    "    # Simplified: Create a DRLAgent shell, then load into it.\n",
    "    # The DRLAgent constructor needs an 'env' to setup its internal SubprocVecEnv, even if we immediately load.\n",
    "    # We can pass the test_env for this, but DRLAgent will make it a VecEnv.\n",
    "    # For loading for evaluation, the internal self.env of DRLAgent is less critical\n",
    "    # if the PPO.load() correctly associates the model with the new eval_env.\n",
    "    \n",
    "    # Let's use the DRLAgent.load() method which takes an env.\n",
    "    # We need to initialize DRLAgent first with *some* env that has the right structure.\n",
    "    # The `single_env_for_init_train` used earlier has the correct structure.\n",
    "    # It is important that the observation and action spaces match.\n",
    "    \n",
    "    # Re-create a template env for agent initialization before loading\n",
    "    # This is just to satisfy DRLAgent's __init__ requirement for an env instance.\n",
    "    # The actual environment for the loaded model will be `env_test`.\n",
    "    temp_env_for_load_init = PortfolioEnv(\n",
    "        returns_df=train_returns.iloc[:ENV_WINDOW_SIZE+5], # minimal data for init\n",
    "        prices_df=train_prices.iloc[:ENV_WINDOW_SIZE+5],\n",
    "        vol_df=train_vola.iloc[:ENV_WINDOW_SIZE+5],\n",
    "        window_size=ENV_WINDOW_SIZE, \n",
    "        initial_balance=INITIAL_BALANCE\n",
    "    )\n",
    "\n",
    "    best_agent_loaded = DRLAgent(\n",
    "        env=temp_env_for_load_init, # Template env\n",
    "        n_envs=1, # For eval, n_envs=1 is fine for the DRLAgent wrapper\n",
    "        policy_kwargs=POLICY_KWARGS \n",
    "        # Other params don't matter as much as we are loading a pre-trained model\n",
    "    )\n",
    "    \n",
    "    print(f\"    Loading model from: {best_agent_paths_per_window[-1]}\")\n",
    "    # Pass the actual test_env to PPO.load via DRLAgent.load method\n",
    "    best_agent_loaded.load(path=best_agent_paths_per_window[-1], env=env_test) \n",
    "                                   \n",
    "    print(\"    Running backtest evaluation...\")\n",
    "    backtest_metrics = best_agent_loaded.evaluate(eval_env=env_test, n_eval_episodes=1)\n",
    "    \n",
    "    print(f\"    Backtest Metrics for Window {i_window+1}:\")\n",
    "    for key, value in backtest_metrics.items():\n",
    "        print(f\"      {key}: {value}\")\n",
    "    \n",
    "    all_backtest_results.append({\n",
    "        \"window\": i_window+1, \n",
    "        \"best_agent_path\": best_agent_paths_per_window[-1],\n",
    "        \"status\": \"completed\",\n",
    "        \"metrics\": backtest_metrics\n",
    "    })\n",
    "    \n",
    "    del best_agent_loaded\n",
    "    del temp_env_for_load_init\n",
    "    del env_test\n",
    "    torch.cuda.empty_cache() # If using GPU\n",
    "\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "# 4. Save results\n",
    "print(\"\\n--- All Windows Processed ---\")\n",
    "print(\"Summary of Best Agent Paths:\")\n",
    "for i, path in enumerate(best_agent_paths_per_window):\n",
    "    print(f\"Window {i+1}: {path}\")\n",
    "\n",
    "print(\"\\nSummary of Backtest Results:\")\n",
    "for result in all_backtest_results:\n",
    "    print(f\"Window {result['window']} ({result['status']}):\")\n",
    "    if result['status'] == 'completed':\n",
    "        # print(f\"  Agent: {result['best_agent_path']}\")\n",
    "        for k, v in result['metrics'].items():\n",
    "            if isinstance(v, float): print(f\"    {k}: {v:.4f}\")\n",
    "            else: print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backtest results summary saved to: ../models/sliding_window_jules/20250531_205413/backtest_results_summary_20250531_210357.csv\n",
      "\n",
      "Final Results DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>best_agent_path</th>\n",
       "      <th>status</th>\n",
       "      <th>Annual return</th>\n",
       "      <th>Cumulative returns</th>\n",
       "      <th>Annual volatility</th>\n",
       "      <th>Sharpe ratio</th>\n",
       "      <th>Calmar ratio</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Max drawdown</th>\n",
       "      <th>...</th>\n",
       "      <th>Sortino ratio</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Tail ratio</th>\n",
       "      <th>Daily value at risk (95%)</th>\n",
       "      <th>Portfolio turnover</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>n_eval_episodes</th>\n",
       "      <th>final_portfolio_value_first_episode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>../models/sliding_window_jules/20250531_205413...</td>\n",
       "      <td>completed</td>\n",
       "      <td>-0.013614</td>\n",
       "      <td>-0.010228</td>\n",
       "      <td>0.112879</td>\n",
       "      <td>-0.242189</td>\n",
       "      <td>-0.295943</td>\n",
       "      <td>0.898570</td>\n",
       "      <td>-0.046002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.387184</td>\n",
       "      <td>0.086205</td>\n",
       "      <td>0.508573</td>\n",
       "      <td>1.222959</td>\n",
       "      <td>-0.010403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.146446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98977.210871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>../models/sliding_window_jules/20250531_205413...</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.025749</td>\n",
       "      <td>0.019456</td>\n",
       "      <td>0.106050</td>\n",
       "      <td>0.104235</td>\n",
       "      <td>0.461445</td>\n",
       "      <td>0.904118</td>\n",
       "      <td>-0.055801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160031</td>\n",
       "      <td>-0.263293</td>\n",
       "      <td>0.640463</td>\n",
       "      <td>0.945525</td>\n",
       "      <td>-0.011436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.573420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101945.613424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   window                                    best_agent_path     status  \\\n",
       "0       1  ../models/sliding_window_jules/20250531_205413...  completed   \n",
       "1       2  ../models/sliding_window_jules/20250531_205413...  completed   \n",
       "\n",
       "   Annual return  Cumulative returns  Annual volatility  Sharpe ratio  \\\n",
       "0      -0.013614           -0.010228           0.112879     -0.242189   \n",
       "1       0.025749            0.019456           0.106050      0.104235   \n",
       "\n",
       "   Calmar ratio  Stability  Max drawdown  ...  Sortino ratio      Skew  \\\n",
       "0     -0.295943   0.898570     -0.046002  ...      -0.387184  0.086205   \n",
       "1      0.461445   0.904118     -0.055801  ...       0.160031 -0.263293   \n",
       "\n",
       "   Kurtosis  Tail ratio  Daily value at risk (95%)  Portfolio turnover  \\\n",
       "0  0.508573    1.222959                  -0.010403                 NaN   \n",
       "1  0.640463    0.945525                  -0.011436                 NaN   \n",
       "\n",
       "   mean_reward  std_reward  n_eval_episodes  \\\n",
       "0    -0.146446         0.0              1.0   \n",
       "1     2.573420         0.0              1.0   \n",
       "\n",
       "   final_portfolio_value_first_episode  \n",
       "0                         98977.210871  \n",
       "1                        101945.613424  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(all_backtest_results)\n",
    "\n",
    "# Expand the 'metrics' dictionary into separate columns\n",
    "metrics_df = results_df[\"metrics\"].apply(pd.Series)\n",
    "results_df = pd.concat([results_df.drop(\"metrics\", axis=1), metrics_df], axis=1)\n",
    "\n",
    "results_filename = (\n",
    "    f\"backtest_results_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    ")\n",
    "results_save_path = os.path.join(MODEL_SAVE_DIR, results_filename)\n",
    "results_df.to_csv(results_save_path, index=False)\n",
    "print(f\"\\nBacktest results summary saved to: {results_save_path}\")\n",
    "print(\"\\nFinal Results DataFrame:\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEMS\n",
    "\n",
    "# P1\n",
    "# after the first window, the best agent is chosen as starting point for all agents in the next window\n",
    "# but smth is wrong with the training process, randomization or smth\n",
    "# all of the 5 agents in the following window are the same, with same rewards and performance etc\n",
    "# therefore all of the following 9 windows after the first are superfluous\n",
    "# or at least training 5 agents in the following windows is a waste of time ... smth\n",
    "\n",
    "# P2\n",
    "# the performance is getting better, but the final portfolio value is still just 101919 ie +2k$ which is really not good\n",
    "\n",
    "# P3\n",
    "# training logs are not saved, or at least the print statement says its saved\n",
    "# but there is not directory and I cant find the log files anywhere\n",
    "\n",
    "# P4\n",
    "# training progress is not visible, only a tqdm bar\n",
    "# maybe smth like pytorch lightning with live monitoring would be great\n",
    "\n",
    "# P5\n",
    "# in the calculated metrics portfolio turnover is always nan for all agents and windows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
