{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch # Ensure torch is imported for policy_kwargs if needed by DRLAgentJules\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming utils are in parent directory or PYTHONPATH is set\n",
    "from utils.portfolio_env import PortfolioEnv \n",
    "from utils.drl_agent_jules import DRLAgent # Import the modified agent\n",
    "\n",
    "# For learning rate schedule\n",
    "from typing import Callable\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Configuration ---\n",
    "N_WINDOWS = 10\n",
    "AGENTS_PER_WINDOW = 5 \n",
    "BASE_START_YEAR = 2006\n",
    "\n",
    "# Data paths\n",
    "PRICE_DATA_PATH = \"../data/prices.parquet\"\n",
    "RETURNS_DATA_PATH = \"../data/returns.parquet\"\n",
    "VOLA_DATA_PATH = \"../data/vola.parquet\"\n",
    "MODEL_SAVE_DIR = \"../models/sliding_window_jules/\"\n",
    "\n",
    "# Ensure model save directory exists\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- DRL Agent Hyperparameters (from paper) ---\n",
    "N_ENVS = 10\n",
    "TOTAL_TIMESTEPS_PER_ROUND = 7_500_000 # 7.5M\n",
    "N_STEPS_PER_ENV = 252 * 3 # n_steps = 252 * 3 * n_envs (this is per env for PPO buffer)\n",
    "# total buffer size before update = N_STEPS_PER_ENV * N_ENVS\n",
    "\n",
    "BATCH_SIZE = 1260\n",
    "N_EPOCHS = 16\n",
    "GAMMA = 0.9\n",
    "GAE_LAMBDA = 0.9\n",
    "CLIP_RANGE = 0.25\n",
    "LOG_STD_INIT = -1.0\n",
    "POLICY_KWARGS = dict(\n",
    "    activation_fn=torch.nn.Tanh,\n",
    "    net_arch=[64, 64], # Shared layers for policy and value networks\n",
    "    log_std_init=LOG_STD_INIT\n",
    ")\n",
    "\n",
    "# Learning rate schedule: linear decay from 3e-4 to 1e-5\n",
    "INITIAL_LR = 3e-4\n",
    "FINAL_LR = 1e-5\n",
    "\n",
    "def linear_schedule(initial_value: float, final_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :param final_value: Final learning rate.\n",
    "    :return: schedule that computes current learning rate depending on progress remaining (1.0 -> 0.0)\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1.0 to 0.0\n",
    "        \"\"\"\n",
    "        return final_value + progress_remaining * (initial_value - final_value)\n",
    "    return func\n",
    "\n",
    "LEARNING_RATE_SCHEDULE = linear_schedule(INITIAL_LR, FINAL_LR)\n",
    "\n",
    "# --- PortfolioEnv Parameters ---\n",
    "ENV_WINDOW_SIZE = 60 # Lookback window for features in PortfolioEnv\n",
    "TRANSACTION_COST = 0.0 # As per paper (or can be adjusted)\n",
    "INITIAL_BALANCE = 100_000\n",
    "REWARD_SCALING = 1.0\n",
    "ETA_DSR = 1 / 252 # For Differential Sharpe Ratio in PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n",
      "\n",
      "Price Data Head:\n",
      "Ticker            XLF        XLK        XLV        XLY        XLP        XLE  \\\n",
      "Date                                                                           \n",
      "2006-01-03  17.763966  16.536201  23.101501  25.836199  14.134378  30.129393   \n",
      "2006-01-04  17.747419  16.730202  23.331730  25.859682  14.182621  30.215170   \n",
      "2006-01-05  17.808100  16.807791  23.274168  25.883167  14.110260  29.832109   \n",
      "2006-01-06  17.912922  17.071632  23.425253  26.094555  14.170568  30.563917   \n",
      "2006-01-09  17.962572  17.125944  23.533169  26.360754  14.236897  30.529608   \n",
      "\n",
      "Ticker            XLI        XLU        XLB  XLRE  XLC  \n",
      "Date                                                    \n",
      "2006-01-03  21.745033  16.358213  20.255630   NaN  NaN  \n",
      "2006-01-04  21.800028  16.383776  20.380095   NaN  NaN  \n",
      "2006-01-05  21.765644  16.276424  20.347343   NaN  NaN  \n",
      "2006-01-06  21.848143  16.450228  20.563517   NaN  NaN  \n",
      "2006-01-09  22.026896  16.347998  20.556973   NaN  NaN  \n",
      "\n",
      "Returns Data Head:\n",
      "Ticker           XLF       XLK       XLV       XLY       XLP       XLE  \\\n",
      "Date                                                                     \n",
      "2006-01-04 -0.000932  0.011664  0.009917  0.000909  0.003407  0.002843   \n",
      "2006-01-05  0.003413  0.004627 -0.002470  0.000908 -0.005115 -0.012759   \n",
      "2006-01-06  0.005869  0.015576  0.006471  0.008134  0.004265  0.024235   \n",
      "2006-01-09  0.002768  0.003176  0.004596  0.010150  0.004670 -0.001123   \n",
      "2006-01-10  0.000921 -0.000906 -0.003675  0.000297 -0.002544  0.010803   \n",
      "\n",
      "Ticker           XLI       XLU       XLB  XLRE  XLC  \n",
      "Date                                                 \n",
      "2006-01-04  0.002526  0.001561  0.006126   NaN  NaN  \n",
      "2006-01-05 -0.001578 -0.006574 -0.001608   NaN  NaN  \n",
      "2006-01-06  0.003783  0.010622  0.010568   NaN  NaN  \n",
      "2006-01-09  0.008148 -0.006234 -0.000318   NaN  NaN  \n",
      "2006-01-10 -0.002813  0.001562 -0.006394   NaN  NaN  \n",
      "\n",
      "Volatility Data Head:\n",
      "            vol20  vol_ratio       VIX\n",
      "Date                                  \n",
      "2006-01-04    NaN        NaN  0.707107\n",
      "2006-01-05    NaN        NaN  0.307341\n",
      "2006-01-06    NaN        NaN -1.221480\n",
      "2006-01-09    NaN        NaN -0.402241\n",
      "2006-01-10    NaN        NaN -1.450386\n"
     ]
    }
   ],
   "source": [
    "# Load the full datasets once\n",
    "try:\n",
    "    print(\"Loading data...\")\n",
    "    prices_df_full = pd.read_parquet(PRICE_DATA_PATH)\n",
    "    returns_df_full = pd.read_parquet(RETURNS_DATA_PATH)\n",
    "    vola_df_full = pd.read_parquet(VOLA_DATA_PATH)\n",
    "    print(\"Data loaded successfully.\")\n",
    "    \n",
    "    # Ensure DataFrames have DateTimeIndex\n",
    "    for df in [prices_df_full, returns_df_full, vola_df_full]:\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "            \n",
    "    # print(\"\\nPrice Data Head:\")\n",
    "    # print(prices_df_full.head())\n",
    "    # print(\"\\nReturns Data Head:\")\n",
    "    # print(returns_df_full.head())\n",
    "    # print(\"\\nVolatility Data Head:\")\n",
    "    # print(vola_df_full.head())\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Data file not found. {e}\")\n",
    "    print(\"Please ensure data is generated and paths are correct in Cell 2.\")\n",
    "    # Stop execution or raise error if data is critical for notebook to run\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(year_start, num_train_years, num_val_years, num_test_years, prices_df, returns_df, vol_df):\n",
    "    \"\"\"Slices data for a given window configuration.\"\"\"\n",
    "    \n",
    "    train_start_date = pd.to_datetime(f\"{year_start}-01-01\")\n",
    "    train_end_date = pd.to_datetime(f\"{year_start + num_train_years -1}-12-31\")\n",
    "    \n",
    "    val_start_date = pd.to_datetime(f\"{year_start + num_train_years}-01-01\")\n",
    "    val_end_date = pd.to_datetime(f\"{year_start + num_train_years + num_val_years - 1}-12-31\")\n",
    "    \n",
    "    test_start_date = pd.to_datetime(f\"{year_start + num_train_years + num_val_years}-01-01\")\n",
    "    test_end_date = pd.to_datetime(f\"{year_start + num_train_years + num_val_years + num_test_years - 1}-12-31\")\n",
    "    \n",
    "    print(f\"  Train Period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "    print(f\"  Val Period  : {val_start_date.date()} to {val_end_date.date()}\")\n",
    "    print(f\"  Test Period : {test_start_date.date()} to {test_end_date.date()}\")\n",
    "\n",
    "    # Slicing (ensure index is datetime)\n",
    "    train_prices = prices_df[train_start_date:train_end_date]\n",
    "    train_returns = returns_df[train_start_date:train_end_date]\n",
    "    train_vola = vol_df[train_start_date:train_end_date]\n",
    "    \n",
    "    val_prices = prices_df[val_start_date:val_end_date]\n",
    "    val_returns = returns_df[val_start_date:val_end_date]\n",
    "    val_vola = vol_df[val_start_date:val_end_date]\n",
    "    \n",
    "    test_prices = prices_df[test_start_date:test_end_date]\n",
    "    test_returns = returns_df[test_start_date:test_end_date]\n",
    "    test_vola = vol_df[test_start_date:test_end_date]\n",
    "    \n",
    "    # Basic check for empty slices which can halt env creation\n",
    "    if train_prices.empty or val_prices.empty or test_prices.empty:\n",
    "        print(\"WARNING: One or more data slices are empty. Check date ranges and data availability.\")\n",
    "        # Potentially raise an error or handle as per requirements\n",
    "    \n",
    "    return (train_prices, train_returns, train_vola), \\\n",
    "           (val_prices, val_returns, val_vola), \\\n",
    "           (test_prices, test_returns, test_vola)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Window 1/10 (Train Year Start: 2006) ---\n",
      "  Train Period: 2006-01-01 to 2010-12-31\n",
      "  Val Period  : 2011-01-01 to 2011-12-31\n",
      "  Test Period : 2012-01-01 to 2012-12-31\n",
      "  Training Agent 1/5 with seed 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301a53f7281c427eb9f891646a25ec00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window1_Agent1_Seed0\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -0.7204\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win1_seed0_valrew-0.72.zip\n",
      "    New best agent for this window with validation reward: -0.7204\n",
      "  Training Agent 2/5 with seed 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c821ddd181f04978aac26e16f823f441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window1_Agent2_Seed1\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -0.9013\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win1_seed1_valrew-0.90.zip\n",
      "  Training Agent 3/5 with seed 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6383cb9bf4e4af1a3c9393d73cc4fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window1_Agent3_Seed2\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -0.6334\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "    New best agent for this window with validation reward: -0.6334\n",
      "  Training Agent 4/5 with seed 3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515729a9f0974f748f4ab786ec951406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window1_Agent4_Seed3\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -1.2657\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win1_seed3_valrew-1.27.zip\n",
      "  Training Agent 5/5 with seed 4...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973c9223ea7d40bcbce950b5a026ee9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window1_Agent5_Seed4\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -1.0242\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win1_seed4_valrew-1.02.zip\n",
      "  Backtesting best agent for Window 1 (../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 1:\n",
      "      Annual return: -0.010818746062720264\n",
      "      Cumulative returns: -0.00812508230809228\n",
      "      Annual volatility: 0.11686190880372624\n",
      "      Sharpe ratio: -0.20582213093808885\n",
      "      Calmar ratio: -0.19944280349705118\n",
      "      Stability: 0.8953658389792367\n",
      "      Max drawdown: -0.054244855532630046\n",
      "      Omega ratio: 0.9666915346397733\n",
      "      Sortino ratio: -0.332271223451118\n",
      "      Skew: 0.12954004367889269\n",
      "      Kurtosis: 0.4449310411152507\n",
      "      Tail ratio: 1.21197202611855\n",
      "      Daily value at risk (95%): -0.01098003030491787\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: 2.987013026180446\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 99187.49176919078\n",
      "--- Starting Window 2/10 (Train Year Start: 2007) ---\n",
      "  Train Period: 2007-01-01 to 2011-12-31\n",
      "  Val Period  : 2012-01-01 to 2012-12-31\n",
      "  Test Period : 2013-01-01 to 2013-12-31\n",
      "  Training Agent 1/5 with seed 5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140d6ad20ba44b4a8a1eaa8e586f5091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window2_Agent1_Seed5\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 2.1022\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n",
      "    New best agent for this window with validation reward: 2.1022\n",
      "  Training Agent 2/5 with seed 6...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b177152499b4c79ab3ab0ce87cce4e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window2_Agent2_Seed6\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 2.1022\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win2_seed6_valrew2.10.zip\n",
      "  Training Agent 3/5 with seed 7...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61687e441bcf4fc7bcc748f74c92d0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window2_Agent3_Seed7\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 2.1022\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win2_seed7_valrew2.10.zip\n",
      "  Training Agent 4/5 with seed 8...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86d62cac35447b7b5ef9d5e242eaf3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window2_Agent4_Seed8\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 2.1022\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win2_seed8_valrew2.10.zip\n",
      "  Training Agent 5/5 with seed 9...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb70f6246c6b4e18bd31f5fd2cb3e4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window2_Agent5_Seed9\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 2.1022\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win2_seed9_valrew2.10.zip\n",
      "  Backtesting best agent for Window 2 (../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 2:\n",
      "      Annual return: 0.024056864928895694\n",
      "      Cumulative returns: 0.01818100826193536\n",
      "      Annual volatility: 0.10467793205348996\n",
      "      Sharpe ratio: 0.08843626696866674\n",
      "      Calmar ratio: 0.43839208865562285\n",
      "      Stability: 0.9052412209784042\n",
      "      Max drawdown: -0.054875225971044994\n",
      "      Omega ratio: 1.0142063848283414\n",
      "      Sortino ratio: 0.13699957367171992\n",
      "      Skew: -0.23772409696258504\n",
      "      Kurtosis: 0.6286364685975867\n",
      "      Tail ratio: 0.9778701549685123\n",
      "      Daily value at risk (95%): -0.010884175853358439\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: 0.5632152314924559\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 101818.10082619353\n",
      "--- Starting Window 3/10 (Train Year Start: 2008) ---\n",
      "  Train Period: 2008-01-01 to 2012-12-31\n",
      "  Val Period  : 2013-01-01 to 2013-12-31\n",
      "  Test Period : 2014-01-01 to 2014-12-31\n",
      "  Training Agent 1/5 with seed 10...\n",
      "    Seeding agent from: ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9908ebf9d6174a8d93ab7730810980ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window3_Agent1_Seed10\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 0.5081\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "    New best agent for this window with validation reward: 0.5081\n",
      "  Training Agent 2/5 with seed 11...\n",
      "    Seeding agent from: ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5957fa833b94acda8ccd012df8658e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window3_Agent2_Seed11\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 0.5081\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win3_seed11_valrew0.51.zip\n",
      "  Training Agent 3/5 with seed 12...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7ceb2165204b9eab8fe8beedbe5ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window3_Agent3_Seed12\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 0.5081\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win3_seed12_valrew0.51.zip\n",
      "  Training Agent 4/5 with seed 13...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ebe7eaf147948d688a63b1afbe756f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window3_Agent4_Seed13\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 0.5081\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win3_seed13_valrew0.51.zip\n",
      "  Training Agent 5/5 with seed 14...\n",
      "    Seeding agent from: ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e2914ee65941c998c767986f162f4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window3_Agent5_Seed14\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 0.5081\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win3_seed14_valrew0.51.zip\n",
      "  Backtesting best agent for Window 3 (../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 3:\n",
      "      Annual return: 0.026011735201471042\n",
      "      Cumulative returns: 0.019653834167944195\n",
      "      Annual volatility: 0.09695196519923632\n",
      "      Sharpe ratio: 0.10706713099985747\n",
      "      Calmar ratio: 0.4681761679340203\n",
      "      Stability: 0.9116169456138152\n",
      "      Max drawdown: -0.05555971658330301\n",
      "      Omega ratio: 1.0186854631178337\n",
      "      Sortino ratio: 0.15458837007297252\n",
      "      Skew: -0.012056520558288045\n",
      "      Kurtosis: 1.825217537873554\n",
      "      Tail ratio: 0.9341998251174354\n",
      "      Daily value at risk (95%): -0.009616251917353595\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: 6.209589122199979\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 101965.38341679443\n",
      "--- Starting Window 4/10 (Train Year Start: 2009) ---\n",
      "  Train Period: 2009-01-01 to 2013-12-31\n",
      "  Val Period  : 2014-01-01 to 2014-12-31\n",
      "  Test Period : 2015-01-01 to 2015-12-31\n",
      "  Training Agent 1/5 with seed 15...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "734bd39bb8fa480ca529372f5cdd46da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window4_Agent1_Seed15\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 6.2151\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "    New best agent for this window with validation reward: 6.2151\n",
      "  Training Agent 2/5 with seed 16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e902b3de437449b8c5b55c88e3f3644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window4_Agent2_Seed16\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 6.2151\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win4_seed16_valrew6.22.zip\n",
      "  Training Agent 3/5 with seed 17...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2935fe998658420e9d8077c49e30f676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window4_Agent3_Seed17\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 6.2151\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win4_seed17_valrew6.22.zip\n",
      "  Training Agent 4/5 with seed 18...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc4c89031534f47b013cee2c0b87a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window4_Agent4_Seed18\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 6.2151\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win4_seed18_valrew6.22.zip\n",
      "  Training Agent 5/5 with seed 19...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62591200f21c4d8cb8b6ca17445559fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window4_Agent5_Seed19\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 6.2151\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win4_seed19_valrew6.22.zip\n",
      "  Backtesting best agent for Window 4 (../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 4:\n",
      "      Annual return: 0.011837769436073176\n",
      "      Cumulative returns: 0.008959485222697072\n",
      "      Annual volatility: 0.13671205396811442\n",
      "      Sharpe ratio: 0.00798610794442625\n",
      "      Calmar ratio: 0.18673640697292032\n",
      "      Stability: 0.8797302681089109\n",
      "      Max drawdown: -0.0633929378205817\n",
      "      Omega ratio: 1.0013142301987856\n",
      "      Sortino ratio: 0.0139790525255839\n",
      "      Skew: 0.4196786751401754\n",
      "      Kurtosis: 1.349186985769717\n",
      "      Tail ratio: 1.0884118013066175\n",
      "      Daily value at risk (95%): -0.013298880555271413\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: 3.9488510266834367\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 100895.9485222697\n",
      "--- Starting Window 5/10 (Train Year Start: 2010) ---\n",
      "  Train Period: 2010-01-01 to 2014-12-31\n",
      "  Val Period  : 2015-01-01 to 2015-12-31\n",
      "  Test Period : 2016-01-01 to 2016-12-31\n",
      "  Training Agent 1/5 with seed 20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddf9d1748ad44c2ac11d4ef7972a81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window5_Agent1_Seed20\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 4.1900\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "    New best agent for this window with validation reward: 4.1900\n",
      "  Training Agent 2/5 with seed 21...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df48705cb66647f4bf1ba8ca196d55ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window5_Agent2_Seed21\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 4.1900\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win5_seed21_valrew4.19.zip\n",
      "  Training Agent 3/5 with seed 22...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9d34318049446e9743a7dd25f8d391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window5_Agent3_Seed22\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 4.1900\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win5_seed22_valrew4.19.zip\n",
      "  Training Agent 4/5 with seed 23...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daaf44e9741043fcad7e0c58dc5651e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window5_Agent4_Seed23\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 4.1900\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win5_seed23_valrew4.19.zip\n",
      "  Training Agent 5/5 with seed 24...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268e603d3e784cada4ad5e520ffe564e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window5_Agent5_Seed24\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 4.1900\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win5_seed24_valrew4.19.zip\n",
      "  Backtesting best agent for Window 5 (../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 5:\n",
      "      Annual return: 0.025407934526497478\n",
      "      Cumulative returns: 0.019198995070798475\n",
      "      Annual volatility: 0.09879013830315057\n",
      "      Sharpe ratio: 0.10099172201425692\n",
      "      Calmar ratio: 0.3848461258829601\n",
      "      Stability: 0.9100918957502557\n",
      "      Max drawdown: -0.06602102195573244\n",
      "      Omega ratio: 1.0176042262670864\n",
      "      Sortino ratio: 0.14446616384098712\n",
      "      Skew: -0.27705317851611483\n",
      "      Kurtosis: 2.74516860875174\n",
      "      Tail ratio: 1.232587308851441\n",
      "      Daily value at risk (95%): -0.008877622631927626\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: -1.7881917583768194\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 101919.89950707984\n",
      "--- Starting Window 6/10 (Train Year Start: 2011) ---\n",
      "  Train Period: 2011-01-01 to 2015-12-31\n",
      "  Val Period  : 2016-01-01 to 2016-12-31\n",
      "  Test Period : 2017-01-01 to 2017-12-31\n",
      "  Training Agent 1/5 with seed 25...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f121f700cf407197291bf3b1b1badf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window6_Agent1_Seed25\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -1.2638\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "    New best agent for this window with validation reward: -1.2638\n",
      "  Training Agent 2/5 with seed 26...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7ad4346a1a4980ad3733bb5cfce63e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window6_Agent2_Seed26\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -1.2638\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win6_seed26_valrew-1.26.zip\n",
      "  Training Agent 3/5 with seed 27...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647df3761f6a45a8911d69e104c08601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window6_Agent3_Seed27\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -1.2638\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win6_seed27_valrew-1.26.zip\n",
      "  Training Agent 4/5 with seed 28...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6a9aac34d34066ae8712a05cdb6d20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window6_Agent4_Seed28\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -1.2638\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win6_seed28_valrew-1.26.zip\n",
      "  Training Agent 5/5 with seed 29...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03724754b4048daa9b24b4ff2575aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window6_Agent5_Seed29\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -1.2638\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win6_seed29_valrew-1.26.zip\n",
      "  Backtesting best agent for Window 6 (../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 6:\n",
      "      Annual return: 0.02938656077943258\n",
      "      Cumulative returns: 0.022077399539502718\n",
      "      Annual volatility: 0.059509005916461105\n",
      "      Sharpe ratio: 0.18041354975837828\n",
      "      Calmar ratio: 0.9775385434454213\n",
      "      Stability: 0.943833411906691\n",
      "      Max drawdown: -0.03006179242391511\n",
      "      Omega ratio: 1.032118505000546\n",
      "      Sortino ratio: 0.2596972176573028\n",
      "      Skew: -0.23077173984078495\n",
      "      Kurtosis: 2.0648622507755703\n",
      "      Tail ratio: 1.2384662382682199\n",
      "      Daily value at risk (95%): -0.005790361753315881\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: 2.6989812762563345\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 102207.73995395028\n",
      "--- Starting Window 7/10 (Train Year Start: 2012) ---\n",
      "  Train Period: 2012-01-01 to 2016-12-31\n",
      "  Val Period  : 2017-01-01 to 2017-12-31\n",
      "  Test Period : 2018-01-01 to 2018-12-31\n",
      "  Training Agent 1/5 with seed 30...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68c6544167c54f14b4c3e961b37480be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window7_Agent1_Seed30\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 2.4158\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "    New best agent for this window with validation reward: 2.4158\n",
      "  Training Agent 2/5 with seed 31...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8004d533e429434780625e01fe494d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window7_Agent2_Seed31\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 2.4158\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win7_seed31_valrew2.42.zip\n",
      "  Training Agent 3/5 with seed 32...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aea038225a408e9a12caeb1ee50117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window7_Agent3_Seed32\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 2.4158\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win7_seed32_valrew2.42.zip\n",
      "  Training Agent 4/5 with seed 33...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7719d48e1eeb4bb49c07510b9234b9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window7_Agent4_Seed33\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 2.4158\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win7_seed33_valrew2.42.zip\n",
      "  Training Agent 5/5 with seed 34...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347441765b56420ba0c1ddadb8965051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window7_Agent5_Seed34\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 2.4158\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win7_seed34_valrew2.42.zip\n",
      "  Backtesting best agent for Window 7 (../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 7:\n",
      "      Annual return: -0.014569973569240391\n",
      "      Cumulative returns: -0.01100510703406865\n",
      "      Annual volatility: 0.12928222688428345\n",
      "      Sharpe ratio: -0.20377529860420782\n",
      "      Calmar ratio: -0.14846417242024793\n",
      "      Stability: 0.885518231132552\n",
      "      Max drawdown: -0.09813797720838742\n",
      "      Omega ratio: 0.9645120073648895\n",
      "      Sortino ratio: -0.3068261139284563\n",
      "      Skew: 0.6024237602752441\n",
      "      Kurtosis: 5.6822793139697225\n",
      "      Tail ratio: 0.82348356816565\n",
      "      Daily value at risk (95%): -0.014473443854695074\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: -3.617494125439494\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 98899.48929659314\n",
      "--- Starting Window 8/10 (Train Year Start: 2013) ---\n",
      "  Train Period: 2013-01-01 to 2017-12-31\n",
      "  Val Period  : 2018-01-01 to 2018-12-31\n",
      "  Test Period : 2019-01-01 to 2019-12-31\n",
      "  Training Agent 1/5 with seed 35...\n",
      "    Seeding agent from: ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf77435d68046aca0acd4d6bfd1020d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window8_Agent1_Seed35\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -3.9313\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "    New best agent for this window with validation reward: -3.9313\n",
      "  Training Agent 2/5 with seed 36...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad88a300ca848e5aac8edce08da674d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window8_Agent2_Seed36\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -3.9313\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win8_seed36_valrew-3.93.zip\n",
      "  Training Agent 3/5 with seed 37...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175be4d7e9454da7801758173a6a7abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window8_Agent3_Seed37\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -3.9313\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win8_seed37_valrew-3.93.zip\n",
      "  Training Agent 4/5 with seed 38...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e223e5e497204c17bc63f290ba2eed96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window8_Agent4_Seed38\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -3.9313\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win8_seed38_valrew-3.93.zip\n",
      "  Training Agent 5/5 with seed 39...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf10fb49f834cb2a468f9f2b481ac53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window8_Agent5_Seed39\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -3.9313\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win8_seed39_valrew-3.93.zip\n",
      "  Backtesting best agent for Window 8 (../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 8:\n",
      "      Annual return: 0.1650127303267681\n",
      "      Cumulative returns: 0.1227276985504806\n",
      "      Annual volatility: 0.10774941003807444\n",
      "      Sharpe ratio: 1.2863377743924036\n",
      "      Calmar ratio: 3.1184709617226645\n",
      "      Stability: 0.9027312413243614\n",
      "      Max drawdown: -0.0529146278263287\n",
      "      Omega ratio: 1.2467454078173812\n",
      "      Sortino ratio: 1.6161115068003677\n",
      "      Skew: -0.839421123490313\n",
      "      Kurtosis: 2.4992786728547722\n",
      "      Tail ratio: 1.0680609907829652\n",
      "      Daily value at risk (95%): -0.010061506578691265\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: 5.757768205560518\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 112272.76985504807\n",
      "--- Starting Window 9/10 (Train Year Start: 2014) ---\n",
      "  Train Period: 2014-01-01 to 2018-12-31\n",
      "  Val Period  : 2019-01-01 to 2019-12-31\n",
      "  Test Period : 2020-01-01 to 2020-12-31\n",
      "  Training Agent 1/5 with seed 40...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f9c72b43bd46df87c231e59a7cea3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window9_Agent1_Seed40\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 4.1874\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n",
      "    New best agent for this window with validation reward: 4.1874\n",
      "  Training Agent 2/5 with seed 41...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc2ddf9dbe0e4d9e9a90f3e90a4adda9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window9_Agent2_Seed41\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 4.1874\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win9_seed41_valrew4.19.zip\n",
      "  Training Agent 3/5 with seed 42...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531c037ef1ae46f4bb6e05d98223d049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window9_Agent3_Seed42\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 4.1874\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win9_seed42_valrew4.19.zip\n",
      "  Training Agent 4/5 with seed 43...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfc7edcd7984ab6bd982bf50f4bcd63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window9_Agent4_Seed43\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 4.1874\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win9_seed43_valrew4.19.zip\n",
      "  Training Agent 5/5 with seed 44...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08f0354ad02491a89beae07e98dcaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window9_Agent5_Seed44\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 4.1874\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win9_seed44_valrew4.19.zip\n",
      "  Backtesting best agent for Window 9 (../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 9:\n",
      "      Annual return: 0.45083355031774874\n",
      "      Cumulative returns: 0.3278139023567641\n",
      "      Annual volatility: 0.23091277035377625\n",
      "      Sharpe ratio: 1.641504696574793\n",
      "      Calmar ratio: 4.896718444554854\n",
      "      Stability: 0.8124052524961539\n",
      "      Max drawdown: -0.09206850576003102\n",
      "      Omega ratio: 1.3269460667052981\n",
      "      Sortino ratio: 2.376121019905467\n",
      "      Skew: 0.008333723213018118\n",
      "      Kurtosis: 3.030767689756988\n",
      "      Tail ratio: 0.905769410287267\n",
      "      Daily value at risk (95%): -0.02271063199159751\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: 9.471858239942527\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 132781.3902356764\n",
      "--- Starting Window 10/10 (Train Year Start: 2015) ---\n",
      "  Train Period: 2015-01-01 to 2019-12-31\n",
      "  Val Period  : 2020-01-01 to 2020-12-31\n",
      "  Test Period : 2021-01-01 to 2021-12-31\n",
      "  Training Agent 1/5 with seed 45...\n",
      "    Seeding agent from: ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f5380193314d5f9f01e0255a9f6911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window10_Agent1_Seed45\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 5.1660\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win10_seed45_valrew5.17.zip\n",
      "    New best agent for this window with validation reward: 5.1660\n",
      "  Training Agent 2/5 with seed 46...\n",
      "    Seeding agent from: ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3817ad986f1946f199582d1077d646a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window10_Agent2_Seed46\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 5.1660\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win10_seed46_valrew5.17.zip\n",
      "  Training Agent 3/5 with seed 47...\n",
      "    Seeding agent from: ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ea44181bd245e8a07098eee47c7dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window10_Agent3_Seed47\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 5.1660\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win10_seed47_valrew5.17.zip\n",
      "  Training Agent 4/5 with seed 48...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ed74107cb941729a530f7389fc15c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window10_Agent4_Seed48\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 5.1660\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win10_seed48_valrew5.17.zip\n",
      "  Training Agent 5/5 with seed 49...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa6a30549774bf7859797ea1295a63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n",
      "Model loaded from ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n",
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 7500000 timesteps.\n",
      "TensorBoard logs saved to directory: PPO_Window10_Agent5_Seed49\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 5.1660\n",
      "    Agent saved to: ../models/sliding_window_jules/agent_win10_seed49_valrew5.17.zip\n",
      "  Backtesting best agent for Window 10 (../models/sliding_window_jules/agent_win10_seed45_valrew5.17.zip)\n",
      "    Loading model from: ../models/sliding_window_jules/agent_win10_seed45_valrew5.17.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 10:\n",
      "      Annual return: 0.1761548064259022\n",
      "      Cumulative returns: 0.13012891501868618\n",
      "      Annual volatility: 0.10786736087810725\n",
      "      Sharpe ratio: 1.3732459631788394\n",
      "      Calmar ratio: 3.184562746357209\n",
      "      Stability: 0.9026351306237503\n",
      "      Max drawdown: -0.0553152254975676\n",
      "      Omega ratio: 1.2585403580732544\n",
      "      Sortino ratio: 1.8779464512663917\n",
      "      Skew: -0.43804413982076834\n",
      "      Kurtosis: 0.9055267656195762\n",
      "      Tail ratio: 1.0922283268336077\n",
      "      Daily value at risk (95%): -0.010774814985179165\n",
      "      Portfolio turnover: nan\n",
      "      mean_reward: -7.964083515834813\n",
      "      std_reward: 0.0\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 113012.89150186861\n",
      "\n",
      "--- All Windows Processed ---\n",
      "Summary of Best Agent Paths:\n",
      "Window 1: ../models/sliding_window_jules/agent_win1_seed2_valrew-0.63.zip\n",
      "Window 2: ../models/sliding_window_jules/agent_win2_seed5_valrew2.10.zip\n",
      "Window 3: ../models/sliding_window_jules/agent_win3_seed10_valrew0.51.zip\n",
      "Window 4: ../models/sliding_window_jules/agent_win4_seed15_valrew6.22.zip\n",
      "Window 5: ../models/sliding_window_jules/agent_win5_seed20_valrew4.19.zip\n",
      "Window 6: ../models/sliding_window_jules/agent_win6_seed25_valrew-1.26.zip\n",
      "Window 7: ../models/sliding_window_jules/agent_win7_seed30_valrew2.42.zip\n",
      "Window 8: ../models/sliding_window_jules/agent_win8_seed35_valrew-3.93.zip\n",
      "Window 9: ../models/sliding_window_jules/agent_win9_seed40_valrew4.19.zip\n",
      "Window 10: ../models/sliding_window_jules/agent_win10_seed45_valrew5.17.zip\n",
      "\n",
      "Summary of Backtest Results:\n",
      "Window 1 (completed):\n",
      "    Annual return: -0.0108\n",
      "    Cumulative returns: -0.0081\n",
      "    Annual volatility: 0.1169\n",
      "    Sharpe ratio: -0.2058\n",
      "    Calmar ratio: -0.1994\n",
      "    Stability: 0.8954\n",
      "    Max drawdown: -0.0542\n",
      "    Omega ratio: 0.9667\n",
      "    Sortino ratio: -0.3323\n",
      "    Skew: 0.1295\n",
      "    Kurtosis: 0.4449\n",
      "    Tail ratio: 1.2120\n",
      "    Daily value at risk (95%): -0.0110\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: 2.9870\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 99187.4918\n",
      "Window 2 (completed):\n",
      "    Annual return: 0.0241\n",
      "    Cumulative returns: 0.0182\n",
      "    Annual volatility: 0.1047\n",
      "    Sharpe ratio: 0.0884\n",
      "    Calmar ratio: 0.4384\n",
      "    Stability: 0.9052\n",
      "    Max drawdown: -0.0549\n",
      "    Omega ratio: 1.0142\n",
      "    Sortino ratio: 0.1370\n",
      "    Skew: -0.2377\n",
      "    Kurtosis: 0.6286\n",
      "    Tail ratio: 0.9779\n",
      "    Daily value at risk (95%): -0.0109\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: 0.5632\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 101818.1008\n",
      "Window 3 (completed):\n",
      "    Annual return: 0.0260\n",
      "    Cumulative returns: 0.0197\n",
      "    Annual volatility: 0.0970\n",
      "    Sharpe ratio: 0.1071\n",
      "    Calmar ratio: 0.4682\n",
      "    Stability: 0.9116\n",
      "    Max drawdown: -0.0556\n",
      "    Omega ratio: 1.0187\n",
      "    Sortino ratio: 0.1546\n",
      "    Skew: -0.0121\n",
      "    Kurtosis: 1.8252\n",
      "    Tail ratio: 0.9342\n",
      "    Daily value at risk (95%): -0.0096\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: 6.2096\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 101965.3834\n",
      "Window 4 (completed):\n",
      "    Annual return: 0.0118\n",
      "    Cumulative returns: 0.0090\n",
      "    Annual volatility: 0.1367\n",
      "    Sharpe ratio: 0.0080\n",
      "    Calmar ratio: 0.1867\n",
      "    Stability: 0.8797\n",
      "    Max drawdown: -0.0634\n",
      "    Omega ratio: 1.0013\n",
      "    Sortino ratio: 0.0140\n",
      "    Skew: 0.4197\n",
      "    Kurtosis: 1.3492\n",
      "    Tail ratio: 1.0884\n",
      "    Daily value at risk (95%): -0.0133\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: 3.9489\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 100895.9485\n",
      "Window 5 (completed):\n",
      "    Annual return: 0.0254\n",
      "    Cumulative returns: 0.0192\n",
      "    Annual volatility: 0.0988\n",
      "    Sharpe ratio: 0.1010\n",
      "    Calmar ratio: 0.3848\n",
      "    Stability: 0.9101\n",
      "    Max drawdown: -0.0660\n",
      "    Omega ratio: 1.0176\n",
      "    Sortino ratio: 0.1445\n",
      "    Skew: -0.2771\n",
      "    Kurtosis: 2.7452\n",
      "    Tail ratio: 1.2326\n",
      "    Daily value at risk (95%): -0.0089\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: -1.7882\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 101919.8995\n",
      "Window 6 (completed):\n",
      "    Annual return: 0.0294\n",
      "    Cumulative returns: 0.0221\n",
      "    Annual volatility: 0.0595\n",
      "    Sharpe ratio: 0.1804\n",
      "    Calmar ratio: 0.9775\n",
      "    Stability: 0.9438\n",
      "    Max drawdown: -0.0301\n",
      "    Omega ratio: 1.0321\n",
      "    Sortino ratio: 0.2597\n",
      "    Skew: -0.2308\n",
      "    Kurtosis: 2.0649\n",
      "    Tail ratio: 1.2385\n",
      "    Daily value at risk (95%): -0.0058\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: 2.6990\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 102207.7400\n",
      "Window 7 (completed):\n",
      "    Annual return: -0.0146\n",
      "    Cumulative returns: -0.0110\n",
      "    Annual volatility: 0.1293\n",
      "    Sharpe ratio: -0.2038\n",
      "    Calmar ratio: -0.1485\n",
      "    Stability: 0.8855\n",
      "    Max drawdown: -0.0981\n",
      "    Omega ratio: 0.9645\n",
      "    Sortino ratio: -0.3068\n",
      "    Skew: 0.6024\n",
      "    Kurtosis: 5.6823\n",
      "    Tail ratio: 0.8235\n",
      "    Daily value at risk (95%): -0.0145\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: -3.6175\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 98899.4893\n",
      "Window 8 (completed):\n",
      "    Annual return: 0.1650\n",
      "    Cumulative returns: 0.1227\n",
      "    Annual volatility: 0.1077\n",
      "    Sharpe ratio: 1.2863\n",
      "    Calmar ratio: 3.1185\n",
      "    Stability: 0.9027\n",
      "    Max drawdown: -0.0529\n",
      "    Omega ratio: 1.2467\n",
      "    Sortino ratio: 1.6161\n",
      "    Skew: -0.8394\n",
      "    Kurtosis: 2.4993\n",
      "    Tail ratio: 1.0681\n",
      "    Daily value at risk (95%): -0.0101\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: 5.7578\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 112272.7699\n",
      "Window 9 (completed):\n",
      "    Annual return: 0.4508\n",
      "    Cumulative returns: 0.3278\n",
      "    Annual volatility: 0.2309\n",
      "    Sharpe ratio: 1.6415\n",
      "    Calmar ratio: 4.8967\n",
      "    Stability: 0.8124\n",
      "    Max drawdown: -0.0921\n",
      "    Omega ratio: 1.3269\n",
      "    Sortino ratio: 2.3761\n",
      "    Skew: 0.0083\n",
      "    Kurtosis: 3.0308\n",
      "    Tail ratio: 0.9058\n",
      "    Daily value at risk (95%): -0.0227\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: 9.4719\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 132781.3902\n",
      "Window 10 (completed):\n",
      "    Annual return: 0.1762\n",
      "    Cumulative returns: 0.1301\n",
      "    Annual volatility: 0.1079\n",
      "    Sharpe ratio: 1.3732\n",
      "    Calmar ratio: 3.1846\n",
      "    Stability: 0.9026\n",
      "    Max drawdown: -0.0553\n",
      "    Omega ratio: 1.2585\n",
      "    Sortino ratio: 1.8779\n",
      "    Skew: -0.4380\n",
      "    Kurtosis: 0.9055\n",
      "    Tail ratio: 1.0922\n",
      "    Daily value at risk (95%): -0.0108\n",
      "    Portfolio turnover: nan\n",
      "    mean_reward: -7.9641\n",
      "    std_reward: 0.0000\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 113012.8915\n"
     ]
    }
   ],
   "source": [
    "all_backtest_results = []\n",
    "best_agent_paths_per_window = [] # To store path of the best agent for each window\n",
    "\n",
    "# --- Main Loop for Sliding Windows ---\n",
    "for i_window in range(N_WINDOWS):\n",
    "    current_start_year = BASE_START_YEAR + i_window\n",
    "    print(f\"--- Starting Window {i_window+1}/{N_WINDOWS} (Train Year Start: {current_start_year}) ---\")\n",
    "\n",
    "    # 1. Slice Data for the current window\n",
    "    # 5 years train, 1 year validation, 1 year test\n",
    "    train_data, val_data, test_data = slice_data(\n",
    "        year_start=current_start_year,\n",
    "        num_train_years=5,\n",
    "        num_val_years=1,\n",
    "        num_test_years=1,\n",
    "        prices_df=prices_df_full,\n",
    "        returns_df=returns_df_full,\n",
    "        vol_df=vola_df_full\n",
    "    )\n",
    "    \n",
    "    # Unpack data\n",
    "    (train_prices, train_returns, train_vola) = train_data\n",
    "    (val_prices, val_returns, val_vola) = val_data\n",
    "    (test_prices, test_returns, test_vola) = test_data\n",
    "\n",
    "    # Check if any crucial dataframe is too short (e.g., shorter than ENV_WINDOW_SIZE)\n",
    "    # PortfolioEnv requires at least `window_size` days of data to start.\n",
    "    min_data_len = ENV_WINDOW_SIZE + 1 # Need at least window_size + 1 for one step\n",
    "    if len(train_prices) < min_data_len or len(val_prices) < min_data_len or len(test_prices) < min_data_len:\n",
    "        print(f\"SKIPPING Window {i_window+1} due to insufficient data length for one or more periods.\")\n",
    "        print(f\"  Train length: {len(train_prices)}, Val length: {len(val_prices)}, Test length: {len(test_prices)}\")\n",
    "        print(f\"  Required minimum: {min_data_len}\")\n",
    "        best_agent_paths_per_window.append(None) # Mark as skipped\n",
    "        all_backtest_results.append({\"window\": i_window+1, \"status\": \"skipped_insufficient_data\", \"metrics\": {}})\n",
    "        continue\n",
    "\n",
    "\n",
    "    # 2. Create Training and Validation Environments\n",
    "    # These envs are re-created for each agent to ensure fresh state and correct data.\n",
    "    # However, the data slice itself is per-window.\n",
    "\n",
    "    best_agent_for_window = None\n",
    "    best_val_reward = -np.inf\n",
    "    \n",
    "    # --- Inner Loop for Training AGENTS_PER_WINDOW Agents ---\n",
    "    for i_agent in range(AGENTS_PER_WINDOW):\n",
    "        agent_seed = (i_window * AGENTS_PER_WINDOW) + i_agent # Unique seed for each agent run\n",
    "        print(f\"  Training Agent {i_agent+1}/{AGENTS_PER_WINDOW} with seed {agent_seed}...\")\n",
    "\n",
    "        # Create environments for this specific agent\n",
    "        # Training Env\n",
    "        env_train_config = {\n",
    "            'returns_df': train_returns, 'prices_df': train_prices, 'vol_df': train_vola,\n",
    "            'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "            'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "        }\n",
    "        # The DRLAgent class will use this first env to understand structure for SubprocVecEnv\n",
    "        # This single_env_for_init is just for the DRLAgent constructor to get parameters.\n",
    "        # The actual training will use N_ENVS created by DRLAgent.\n",
    "        single_env_for_init_train = PortfolioEnv(**env_train_config)\n",
    "\n",
    "        # Validation Env (single, not vectorized for evaluation)\n",
    "        env_val_config = {\n",
    "            'returns_df': val_returns, 'prices_df': val_prices, 'vol_df': val_vola,\n",
    "            'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "            'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "        }\n",
    "        env_val = PortfolioEnv(**env_val_config)\n",
    "        \n",
    "        # Instantiate DRL Agent\n",
    "        agent = DRLAgent(\n",
    "            env=single_env_for_init_train, # Pass the sample env for DRLAgent to clone\n",
    "            n_envs=N_ENVS,\n",
    "            policy_kwargs=POLICY_KWARGS,\n",
    "            n_steps=N_STEPS_PER_ENV, # n_steps per environment for PPO\n",
    "            batch_size=BATCH_SIZE,\n",
    "            n_epochs=N_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE_SCHEDULE,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            clip_range=CLIP_RANGE,\n",
    "            seed=agent_seed\n",
    "        )\n",
    "\n",
    "        # Agent Seeding: Load previous window's best agent if not the first window\n",
    "        if i_window > 0 and best_agent_paths_per_window[i_window-1] is not None:\n",
    "            previous_best_agent_path = best_agent_paths_per_window[i_window-1]\n",
    "            print(f\"    Seeding agent from: {previous_best_agent_path}\")\n",
    "            # The env for load_from_file should match the new training env structure\n",
    "            # DRLAgent's load_from_file uses its internal self.env by default if env=None.\n",
    "            # This self.env is already configured with N_ENVS and the new train_data.\n",
    "            agent.load_from_file(path=previous_best_agent_path, env=None) \n",
    "                                   \n",
    "        # Train the agent\n",
    "        print(f\"    Starting training for {TOTAL_TIMESTEPS_PER_ROUND} timesteps...\")\n",
    "        # Note: Training can be very long. For testing, reduce TOTAL_TIMESTEPS_PER_ROUND.\n",
    "        # Example: agent.train(total_timesteps=10000, tb_log_name=f\"ppo_win{i_window}_agent{i_agent}\")\n",
    "        agent.train(\n",
    "            total_timesteps=TOTAL_TIMESTEPS_PER_ROUND, \n",
    "            tb_log_name=f\"PPO_Window{i_window+1}_Agent{i_agent+1}_Seed{agent_seed}\"\n",
    "        )\n",
    "        \n",
    "        # Evaluate the agent on the validation set\n",
    "        print(\"    Evaluating agent on validation set...\")\n",
    "        # The evaluate method in DRLAgentJules is designed for a single eval_env\n",
    "        val_metrics = agent.evaluate(eval_env=env_val, n_eval_episodes=1) # Use 1 episode for validation speed\n",
    "        current_val_reward = val_metrics.get(\"mean_reward\", -np.inf)\n",
    "        print(f\"    Validation Mean Reward: {current_val_reward:.4f}\")\n",
    "        \n",
    "        # Save this agent\n",
    "        current_agent_model_name = f\"agent_win{i_window+1}_seed{agent_seed}_valrew{current_val_reward:.2f}.zip\"\n",
    "        current_agent_save_path = os.path.join(MODEL_SAVE_DIR, current_agent_model_name)\n",
    "        agent.save(current_agent_save_path)\n",
    "        print(f\"    Agent saved to: {current_agent_save_path}\")\n",
    "\n",
    "        if current_val_reward > best_val_reward:\n",
    "            best_val_reward = current_val_reward\n",
    "            best_agent_for_window_path = current_agent_save_path \n",
    "            print(f\"    New best agent for this window with validation reward: {best_val_reward:.4f}\")\n",
    "\n",
    "        # Clean up to free memory if needed, though Python's GC should handle agent and envs\n",
    "        del agent\n",
    "        del single_env_for_init_train\n",
    "        del env_val\n",
    "        torch.cuda.empty_cache() # If using GPU\n",
    "\n",
    "    best_agent_paths_per_window.append(best_agent_for_window_path if 'best_agent_for_window_path' in locals() and best_agent_for_window_path is not None else None)\n",
    "    \n",
    "    if best_agent_paths_per_window[-1] is None:\n",
    "        print(f\"  No best agent found or saved for window {i_window+1}. Skipping backtest.\")\n",
    "        all_backtest_results.append({\"window\": i_window+1, \"status\": \"no_best_agent\", \"metrics\": {}})\n",
    "        continue\n",
    "\n",
    "    # 3. Backtest the best agent of the window\n",
    "    print(f\"  Backtesting best agent for Window {i_window+1} ({best_agent_paths_per_window[-1]})\" )\n",
    "    \n",
    "    # Create Backtesting Environment\n",
    "    env_test_config = {\n",
    "        'returns_df': test_returns, 'prices_df': test_prices, 'vol_df': test_vola,\n",
    "        'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "        'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "    }\n",
    "    env_test = PortfolioEnv(**env_test_config)\n",
    "    \n",
    "    # Load the best agent for this window\n",
    "    # For loading, we need a sample env. We can create a dummy one or use env_test.\n",
    "    # The DRLAgent needs an env instance for its constructor to derive parameters for make_env.\n",
    "    # So, we pass a temporary env instance here.\n",
    "    # The actual self.env for the loaded model will be set by PPO.load(env=env_test)\n",
    "    \n",
    "    # Create a temporary env instance for DRLAgent initialization before loading the model.\n",
    "    # This env should reflect the structure the agent was trained on (e.g. observation/action space from PortfolioEnv)\n",
    "    # but the actual data doesn't matter as much for just loading.\n",
    "    # However, to be safe, use a structure similar to what it was trained on.\n",
    "    # The DRLAgent.load method sets the environment for the loaded PPO model.\n",
    "    \n",
    "    # Simplified: Create a DRLAgent shell, then load into it.\n",
    "    # The DRLAgent constructor needs an 'env' to setup its internal SubprocVecEnv, even if we immediately load.\n",
    "    # We can pass the test_env for this, but DRLAgent will make it a VecEnv.\n",
    "    # For loading for evaluation, the internal self.env of DRLAgent is less critical\n",
    "    # if the PPO.load() correctly associates the model with the new eval_env.\n",
    "    \n",
    "    # Let's use the DRLAgent.load() method which takes an env.\n",
    "    # We need to initialize DRLAgent first with *some* env that has the right structure.\n",
    "    # The `single_env_for_init_train` used earlier has the correct structure.\n",
    "    # It is important that the observation and action spaces match.\n",
    "    \n",
    "    # Re-create a template env for agent initialization before loading\n",
    "    # This is just to satisfy DRLAgent's __init__ requirement for an env instance.\n",
    "    # The actual environment for the loaded model will be `env_test`.\n",
    "    temp_env_for_load_init = PortfolioEnv(\n",
    "        returns_df=train_returns.iloc[:ENV_WINDOW_SIZE+5], # minimal data for init\n",
    "        prices_df=train_prices.iloc[:ENV_WINDOW_SIZE+5],\n",
    "        vol_df=train_vola.iloc[:ENV_WINDOW_SIZE+5],\n",
    "        window_size=ENV_WINDOW_SIZE, \n",
    "        initial_balance=INITIAL_BALANCE\n",
    "    )\n",
    "\n",
    "    best_agent_loaded = DRLAgent(\n",
    "        env=temp_env_for_load_init, # Template env\n",
    "        n_envs=1, # For eval, n_envs=1 is fine for the DRLAgent wrapper\n",
    "        policy_kwargs=POLICY_KWARGS \n",
    "        # Other params don't matter as much as we are loading a pre-trained model\n",
    "    )\n",
    "    \n",
    "    print(f\"    Loading model from: {best_agent_paths_per_window[-1]}\")\n",
    "    # Pass the actual test_env to PPO.load via DRLAgent.load method\n",
    "    best_agent_loaded.load(path=best_agent_paths_per_window[-1], env=env_test) \n",
    "                                   \n",
    "    print(\"    Running backtest evaluation...\")\n",
    "    backtest_metrics = best_agent_loaded.evaluate(eval_env=env_test, n_eval_episodes=1) # n_eval_episodes for backtest\n",
    "    \n",
    "    print(f\"    Backtest Metrics for Window {i_window+1}:\")\n",
    "    for key, value in backtest_metrics.items():\n",
    "        print(f\"      {key}: {value}\")\n",
    "    \n",
    "    all_backtest_results.append({\n",
    "        \"window\": i_window+1, \n",
    "        \"best_agent_path\": best_agent_paths_per_window[-1],\n",
    "        \"status\": \"completed\",\n",
    "        \"metrics\": backtest_metrics\n",
    "    })\n",
    "    \n",
    "    del best_agent_loaded\n",
    "    del temp_env_for_load_init\n",
    "    del env_test\n",
    "    torch.cuda.empty_cache() # If using GPU\n",
    "\n",
    "print(\"\\n--- All Windows Processed ---\")\n",
    "print(\"Summary of Best Agent Paths:\")\n",
    "for i, path in enumerate(best_agent_paths_per_window):\n",
    "    print(f\"Window {i+1}: {path}\")\n",
    "\n",
    "print(\"\\nSummary of Backtest Results:\")\n",
    "for result in all_backtest_results:\n",
    "    print(f\"Window {result['window']} ({result['status']}):\")\n",
    "    if result['status'] == 'completed':\n",
    "        # print(f\"  Agent: {result['best_agent_path']}\")\n",
    "        for k, v in result['metrics'].items():\n",
    "            if isinstance(v, float): print(f\"    {k}: {v:.4f}\")\n",
    "            else: print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backtest results summary saved to: ../models/sliding_window_jules/backtest_results_summary_20250531_125621.csv\n",
      "\n",
      "Final Results DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>best_agent_path</th>\n",
       "      <th>status</th>\n",
       "      <th>Annual return</th>\n",
       "      <th>Cumulative returns</th>\n",
       "      <th>Annual volatility</th>\n",
       "      <th>Sharpe ratio</th>\n",
       "      <th>Calmar ratio</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Max drawdown</th>\n",
       "      <th>...</th>\n",
       "      <th>Sortino ratio</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Tail ratio</th>\n",
       "      <th>Daily value at risk (95%)</th>\n",
       "      <th>Portfolio turnover</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>n_eval_episodes</th>\n",
       "      <th>final_portfolio_value_first_episode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>../models/sliding_window_jules/agent_win1_seed...</td>\n",
       "      <td>completed</td>\n",
       "      <td>-0.010819</td>\n",
       "      <td>-0.008125</td>\n",
       "      <td>0.116862</td>\n",
       "      <td>-0.205822</td>\n",
       "      <td>-0.199443</td>\n",
       "      <td>0.895366</td>\n",
       "      <td>-0.054245</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.332271</td>\n",
       "      <td>0.129540</td>\n",
       "      <td>0.444931</td>\n",
       "      <td>1.211972</td>\n",
       "      <td>-0.010980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.987013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99187.491769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>../models/sliding_window_jules/agent_win2_seed...</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.024057</td>\n",
       "      <td>0.018181</td>\n",
       "      <td>0.104678</td>\n",
       "      <td>0.088436</td>\n",
       "      <td>0.438392</td>\n",
       "      <td>0.905241</td>\n",
       "      <td>-0.054875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.137000</td>\n",
       "      <td>-0.237724</td>\n",
       "      <td>0.628636</td>\n",
       "      <td>0.977870</td>\n",
       "      <td>-0.010884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.563215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101818.100826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>../models/sliding_window_jules/agent_win3_seed...</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.026012</td>\n",
       "      <td>0.019654</td>\n",
       "      <td>0.096952</td>\n",
       "      <td>0.107067</td>\n",
       "      <td>0.468176</td>\n",
       "      <td>0.911617</td>\n",
       "      <td>-0.055560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154588</td>\n",
       "      <td>-0.012057</td>\n",
       "      <td>1.825218</td>\n",
       "      <td>0.934200</td>\n",
       "      <td>-0.009616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.209589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101965.383417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>../models/sliding_window_jules/agent_win4_seed...</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.011838</td>\n",
       "      <td>0.008959</td>\n",
       "      <td>0.136712</td>\n",
       "      <td>0.007986</td>\n",
       "      <td>0.186736</td>\n",
       "      <td>0.879730</td>\n",
       "      <td>-0.063393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013979</td>\n",
       "      <td>0.419679</td>\n",
       "      <td>1.349187</td>\n",
       "      <td>1.088412</td>\n",
       "      <td>-0.013299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.948851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100895.948522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>../models/sliding_window_jules/agent_win5_seed...</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.025408</td>\n",
       "      <td>0.019199</td>\n",
       "      <td>0.098790</td>\n",
       "      <td>0.100992</td>\n",
       "      <td>0.384846</td>\n",
       "      <td>0.910092</td>\n",
       "      <td>-0.066021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.144466</td>\n",
       "      <td>-0.277053</td>\n",
       "      <td>2.745169</td>\n",
       "      <td>1.232587</td>\n",
       "      <td>-0.008878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.788192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101919.899507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   window                                    best_agent_path     status  \\\n",
       "0       1  ../models/sliding_window_jules/agent_win1_seed...  completed   \n",
       "1       2  ../models/sliding_window_jules/agent_win2_seed...  completed   \n",
       "2       3  ../models/sliding_window_jules/agent_win3_seed...  completed   \n",
       "3       4  ../models/sliding_window_jules/agent_win4_seed...  completed   \n",
       "4       5  ../models/sliding_window_jules/agent_win5_seed...  completed   \n",
       "\n",
       "   Annual return  Cumulative returns  Annual volatility  Sharpe ratio  \\\n",
       "0      -0.010819           -0.008125           0.116862     -0.205822   \n",
       "1       0.024057            0.018181           0.104678      0.088436   \n",
       "2       0.026012            0.019654           0.096952      0.107067   \n",
       "3       0.011838            0.008959           0.136712      0.007986   \n",
       "4       0.025408            0.019199           0.098790      0.100992   \n",
       "\n",
       "   Calmar ratio  Stability  Max drawdown  ...  Sortino ratio      Skew  \\\n",
       "0     -0.199443   0.895366     -0.054245  ...      -0.332271  0.129540   \n",
       "1      0.438392   0.905241     -0.054875  ...       0.137000 -0.237724   \n",
       "2      0.468176   0.911617     -0.055560  ...       0.154588 -0.012057   \n",
       "3      0.186736   0.879730     -0.063393  ...       0.013979  0.419679   \n",
       "4      0.384846   0.910092     -0.066021  ...       0.144466 -0.277053   \n",
       "\n",
       "   Kurtosis  Tail ratio  Daily value at risk (95%)  Portfolio turnover  \\\n",
       "0  0.444931    1.211972                  -0.010980                 NaN   \n",
       "1  0.628636    0.977870                  -0.010884                 NaN   \n",
       "2  1.825218    0.934200                  -0.009616                 NaN   \n",
       "3  1.349187    1.088412                  -0.013299                 NaN   \n",
       "4  2.745169    1.232587                  -0.008878                 NaN   \n",
       "\n",
       "   mean_reward  std_reward  n_eval_episodes  \\\n",
       "0     2.987013         0.0              1.0   \n",
       "1     0.563215         0.0              1.0   \n",
       "2     6.209589         0.0              1.0   \n",
       "3     3.948851         0.0              1.0   \n",
       "4    -1.788192         0.0              1.0   \n",
       "\n",
       "   final_portfolio_value_first_episode  \n",
       "0                         99187.491769  \n",
       "1                        101818.100826  \n",
       "2                        101965.383417  \n",
       "3                        100895.948522  \n",
       "4                        101919.899507  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(all_backtest_results)\n",
    "\n",
    "# Expand the 'metrics' dictionary into separate columns\n",
    "metrics_df = results_df['metrics'].apply(pd.Series)\n",
    "results_df = pd.concat([results_df.drop('metrics', axis=1), metrics_df], axis=1)\n",
    "\n",
    "results_filename = f\"backtest_results_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "results_save_path = os.path.join(MODEL_SAVE_DIR, results_filename)\n",
    "results_df.to_csv(results_save_path, index=False)\n",
    "print(f\"\\nBacktest results summary saved to: {results_save_path}\")\n",
    "print(\"\\nFinal Results DataFrame:\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEMS\n",
    "\n",
    "# P1\n",
    "# after the first window, the best agent is chosen as starting point for all agents in the next window\n",
    "# but smth is wrong with the training process, randomization or smth\n",
    "# all of the 5 agents in the following window are the same, with same rewards and performance etc\n",
    "# therefore all of the following 9 windows after the first are superfluous\n",
    "# or at least training 5 agents in the following windows is a waste of time ... smth\n",
    "\n",
    "# P2\n",
    "# the performance is getting better, but the final portfolio value is still just 101919 ie +2k$ which is really not good\n",
    "\n",
    "# P3\n",
    "# training logs are not saved, or at least the print statement says its saved\n",
    "# but there is not directory and I cant find the log files anywhere\n",
    "\n",
    "# P4\n",
    "# training progress is not visible, only a tqdm bar\n",
    "# maybe smth like pytorch lightning with live monitoring would be great"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
