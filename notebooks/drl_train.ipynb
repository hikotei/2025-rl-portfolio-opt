{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dataclasses import asdict\n",
    "\n",
    "from datetime import datetime\n",
    "from utils.config import DRLConfig\n",
    "from utils.drl_train import training_pipeline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SnP500 Sector\n",
    "# DATA_DIR = \"../data/snp_new\"\n",
    "# RETURNS_PATH = os.path.join(DATA_DIR, \"returns_1d.parquet\")\n",
    "# PRICES_PATH = os.path.join(DATA_DIR, \"prices_1d.parquet\")\n",
    "# VOLA_PATH = os.path.join(DATA_DIR, \"vola_1d.parquet\")\n",
    "\n",
    "# MSCI World Index\n",
    "DATA_DIR = \"../data/intl\"\n",
    "RETURNS_PATH = os.path.join(DATA_DIR, \"returns_1d.parquet\")\n",
    "PRICES_PATH = os.path.join(DATA_DIR, \"prices_1d.parquet\")\n",
    "VOLA_PATH = os.path.join(DATA_DIR, \"vola_1d.parquet\")\n",
    "\n",
    "df_ret = pd.read_parquet(RETURNS_PATH)\n",
    "df_prices = pd.read_parquet(PRICES_PATH)\n",
    "df_vol = pd.read_parquet(VOLA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration saved to: ../models/20250702_194422_train_start=2012_best_seed=False/config_20250702_194422.json\n"
     ]
    }
   ],
   "source": [
    "# To view the logs:\n",
    "# 1. Open a terminal or command prompt.\n",
    "# 2. Navigate to the directory *containing* the `logs` directory (i.e., the root of this repository).\n",
    "# 3. Run the command: `tensorboard --logdir logs/`\n",
    "# 4. Open the URL provided by TensorBoard (usually http://localhost:6006/) in your web browser.\n",
    "\n",
    "# Create timestamp for this run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "N_WINDOWS = 3  # 10 in paper\n",
    "N_AGENTS = 5  # 5 in paper\n",
    "START_YR = 2012  # 2006 in paper\n",
    "TOTAL_STEPS = 3_500_000 # 7_500_000 in paper\n",
    "SEED_POLICY = False\n",
    "FOLDER_NAME = f\"{timestamp}_train_start={START_YR}_best_seed={SEED_POLICY}\"\n",
    "\n",
    "# Create configuration\n",
    "config = DRLConfig(\n",
    "    # Window configuration\n",
    "    n_windows=N_WINDOWS,\n",
    "    agents_per_window=N_AGENTS,\n",
    "    base_start_year=START_YR,\n",
    "    seed_policy=SEED_POLICY,\n",
    "    # Environment parameters\n",
    "    env_window_size=60,\n",
    "    transaction_cost=0.0,\n",
    "    initial_balance=100_000,\n",
    "    reward_scaling=1.0,\n",
    "    eta_dsr=1 / 252,\n",
    "    # Training parameters\n",
    "    n_envs=10,\n",
    "    total_timesteps_per_round=TOTAL_STEPS,\n",
    "    n_steps_per_env=252 * 3,\n",
    "    batch_size=1260,\n",
    "    n_epochs=16,\n",
    "    gamma=0.9,\n",
    "    gae_lambda=0.9,\n",
    "    clip_range=0.25,\n",
    "    log_std_init=-1.0,\n",
    "    # Learning rate parameters\n",
    "    initial_lr=3e-4,\n",
    "    final_lr=1e-5,\n",
    "    # Paths\n",
    "    data_dir=DATA_DIR,\n",
    "    model_save_dir=f\"../models/{FOLDER_NAME}\",\n",
    "    tensorboard_log_dir=f\"../logs/{FOLDER_NAME}\",\n",
    "    # prev_best_model_dir=\"../models/full_random_run/agent_6-1_seed=25_test=2017_valrew=43.20.zip\",\n",
    ")\n",
    "\n",
    "config_dict = asdict(config)\n",
    "config_json_path = os.path.join(config.model_save_dir, f\"config_{timestamp}.json\")\n",
    "os.makedirs(config.model_save_dir, exist_ok=True)\n",
    "with open(config_json_path, \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=4)\n",
    "print(f\"\\nConfiguration saved to: {config_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Window 1/3 (Train Year Start: 2012) ---\n",
      "  Train Period: 2012-01-01 to 2016-12-31\n",
      "  Val Period  : 2017-01-01 to 2017-12-31\n",
      "  Test Period : 2018-01-01 to 2018-12-31\n",
      "  Starting with fresh random initialization\n",
      "  Training Agent 1/5 with seed 26234...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa138b892b44d559d3967bc2b2dc3e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 3500000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Seed=26234' saved in directory: ../logs/20250702_194422_train_start=2012_best_seed=False\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Reward: 18.89872415\n",
      "    Agent saved to: ../models/20250702_194422_train_start=2012_best_seed=False/agent_1-1_seed=26234_test=2018_valrew=18.90.zip\n",
      "\n",
      "  Training Agent 2/5 with seed 48083...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f804a2abddf24e6784967ac414cb93ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 3500000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Seed=48083' saved in directory: ../logs/20250702_194422_train_start=2012_best_seed=False\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Reward: 19.50849341\n",
      "    Agent saved to: ../models/20250702_194422_train_start=2012_best_seed=False/agent_1-2_seed=48083_test=2018_valrew=19.51.zip\n",
      "\n",
      "  Training Agent 3/5 with seed 9029...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a92a51420714a86864f58eec400603c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 3500000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Seed=9029' saved in directory: ../logs/20250702_194422_train_start=2012_best_seed=False\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Reward: 18.51427167\n",
      "    Agent saved to: ../models/20250702_194422_train_start=2012_best_seed=False/agent_1-3_seed=9029_test=2018_valrew=18.51.zip\n",
      "\n",
      "  Training Agent 4/5 with seed 2539...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b643c942ed848aebfb6c7febc24d67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run training pipeline\n",
    "results_df, backtest_portfolio = training_pipeline(\n",
    "    drl_config=config, df_prices=df_prices, df_ret=df_ret, df_vol=df_vol\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results DataFrame:\")\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
