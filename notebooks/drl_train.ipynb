{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from utils.config import DRLConfig\n",
    "from utils.drl_train import training_pipeline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ret = pd.read_parquet(\"../data/returns.parquet\")\n",
    "df_prices = pd.read_parquet(\"../data/prices.parquet\")\n",
    "df_vol = pd.read_parquet(\"../data/vola.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view the logs:\n",
    "1. Open a terminal or command prompt.\n",
    "2. Navigate to the directory *containing* the `logs` directory (i.e., the root of this repository).\n",
    "3. Run the command: `tensorboard --logdir logs/`\n",
    "4. Open the URL provided by TensorBoard (usually http://localhost:6006/) in your web browser.\n",
    "\n",
    "You should see experiments named like `PPO_WindowX_AgentY_SeedZ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamp for this run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create configuration\n",
    "config = DRLConfig(\n",
    "    # Window configuration\n",
    "    n_windows=1,  # 10 in paper\n",
    "    agents_per_window=5,  # 5 in paper\n",
    "    base_start_year=2006,\n",
    "    # Environment parameters\n",
    "    env_window_size=60,\n",
    "    transaction_cost=0.0,\n",
    "    initial_balance=100_000,\n",
    "    reward_scaling=1.0,\n",
    "    eta_dsr=1 / 252,\n",
    "    # Training parameters\n",
    "    n_envs=10,\n",
    "    total_timesteps_per_round=7_500_000,  # 7_500_000 in paper\n",
    "    n_steps_per_env=252 * 3,\n",
    "    batch_size=1260,\n",
    "    n_epochs=16,\n",
    "    gamma=0.9,\n",
    "    gae_lambda=0.9,\n",
    "    clip_range=0.25,\n",
    "    log_std_init=-1.0,\n",
    "    # Learning rate parameters\n",
    "    initial_lr=3e-4,\n",
    "    final_lr=1e-5,\n",
    "    # Paths\n",
    "    model_save_dir=f\"../models/{timestamp}\",\n",
    "    tensorboard_log_dir=f\"../logs/{timestamp}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Window 1/1 (Train Year Start: 2006) ---\n",
      "  Train Period: 2006-01-01 to 2010-12-31\n",
      "  Val Period  : 2011-01-01 to 2011-12-31\n",
      "  Test Period : 2012-01-01 to 2012-12-31\n",
      "  Training Agent 1/5 with seed 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94e5b1dfc8c4b7299a704d5c2fda041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for 7500000 timesteps...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run training pipeline\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results, backtest_portfolio = \u001b[43mtraining_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrl_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_prices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_prices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_ret\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_ret\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_vol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_vol\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:23\u001b[39m, in \u001b[36mtraining_pipeline\u001b[39m\u001b[34m(drl_config, df_prices, df_ret, df_vol)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:30\u001b[39m, in \u001b[36mprocess_window\u001b[39m\u001b[34m(window_idx, data_slices, drl_config, previous_best_agent_path)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/2025-rl-portfolio-opt/utils/drl_train.py:215\u001b[39m, in \u001b[36mtrain_single_agent\u001b[39m\u001b[34m(env_train, env_val, drl_config, agent_seed, previous_best_agent_path)\u001b[39m\n\u001b[32m    212\u001b[39m     agent.load(path=previous_best_agent_path, env=None)\n\u001b[32m    213\u001b[39m     agent.model.set_random_seed(agent_seed)\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m # Train agent\n\u001b[32m    216\u001b[39m print(\n\u001b[32m    217\u001b[39m     f\"    Starting training for {drl_config.total_timesteps_per_round} timesteps...\"\n\u001b[32m    218\u001b[39m )\n\u001b[32m    219\u001b[39m agent.train(\n\u001b[32m    220\u001b[39m     total_timesteps=drl_config.total_timesteps_per_round,\n\u001b[32m    221\u001b[39m     tb_experiment_name=f\"PPO_Seed{agent_seed}\",\n\u001b[32m    222\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/2025-rl-portfolio-opt/utils/drl_agent.py:159\u001b[39m, in \u001b[36mDRLAgent.train\u001b[39m\u001b[34m(self, total_timesteps, tb_experiment_name)\u001b[39m\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, total_timesteps: \u001b[38;5;28mint\u001b[39m = \u001b[32m100_000\u001b[39m, tb_experiment_name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mppo\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    144\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m    Trains the PPO model.\u001b[39;00m\n\u001b[32m    146\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    157\u001b[39m \u001b[33;03m    tb_experiment_name : str, optional\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_experiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining complete. Trained for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m timesteps.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    165\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    166\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTensorBoard logs for experiment \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtb_experiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m saved in directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.model.tensorboard_log\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[39m, in \u001b[36mPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    303\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[32m    304\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    309\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    310\u001b[39m ) -> SelfPPO:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/stable_baselines3/common/on_policy_algorithm.py:337\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28mself\u001b[39m.dump_logs(iteration)\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m callback.on_training_end()\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:277\u001b[39m, in \u001b[36mPPO.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    275\u001b[39m     loss.backward()\n\u001b[32m    276\u001b[39m     \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[43mth\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28mself\u001b[39m.policy.optimizer.step()\n\u001b[32m    280\u001b[39m \u001b[38;5;28mself\u001b[39m._n_updates += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:38\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/torch/nn/utils/clip_grad.py:217\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    214\u001b[39m     parameters = [parameters]\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# prevent generators from being exhausted\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     parameters = \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[32m    218\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m    219\u001b[39m total_norm = _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:2642\u001b[39m, in \u001b[36mModule.parameters\u001b[39m\u001b[34m(self, recurse)\u001b[39m\n\u001b[32m   2620\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparameters\u001b[39m(\u001b[38;5;28mself\u001b[39m, recurse: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Iterator[Parameter]:\n\u001b[32m   2621\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters.\u001b[39;00m\n\u001b[32m   2622\u001b[39m \n\u001b[32m   2623\u001b[39m \u001b[33;03m    This is typically passed to an optimizer.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2640\u001b[39m \n\u001b[32m   2641\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2642\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnamed_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2643\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:2675\u001b[39m, in \u001b[36mModule.named_parameters\u001b[39m\u001b[34m(self, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2648\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\u001b[39;00m\n\u001b[32m   2649\u001b[39m \n\u001b[32m   2650\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2667\u001b[39m \n\u001b[32m   2668\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2669\u001b[39m gen = \u001b[38;5;28mself\u001b[39m._named_members(\n\u001b[32m   2670\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m module: module._parameters.items(),\n\u001b[32m   2671\u001b[39m     prefix=prefix,\n\u001b[32m   2672\u001b[39m     recurse=recurse,\n\u001b[32m   2673\u001b[39m     remove_duplicate=remove_duplicate,\n\u001b[32m   2674\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2675\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m gen\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/torch/nn/modules/module.py:2613\u001b[39m, in \u001b[36mModule._named_members\u001b[39m\u001b[34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[39m\n\u001b[32m   2611\u001b[39m members = get_members_fn(module)\n\u001b[32m   2612\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m members:\n\u001b[32m-> \u001b[39m\u001b[32m2613\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m:\n\u001b[32m   2614\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   2615\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m remove_duplicate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/rl/lib/python3.11/site-packages/torch/_tensor.py:1202\u001b[39m, in \u001b[36mTensor.__hash__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1197\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1198\u001b[39m     \u001b[38;5;66;03m# Do NOT handle __torch_function__ here as user's default\u001b[39;00m\n\u001b[32m   1199\u001b[39m     \u001b[38;5;66;03m# implementation that handle most functions will most likely do it wrong.\u001b[39;00m\n\u001b[32m   1200\u001b[39m     \u001b[38;5;66;03m# It can be easily overridden by defining this method on the user\u001b[39;00m\n\u001b[32m   1201\u001b[39m     \u001b[38;5;66;03m# subclass if needed.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run training pipeline\n",
    "results, backtest_portfolio = training_pipeline(\n",
    "    drl_config=config, df_prices=df_prices, df_ret=df_ret, df_vol=df_vol\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>portfolio_value</th>\n",
       "      <th>cash</th>\n",
       "      <th>w_c</th>\n",
       "      <th>w_XLF</th>\n",
       "      <th>w_XLK</th>\n",
       "      <th>w_XLV</th>\n",
       "      <th>w_XLY</th>\n",
       "      <th>w_XLP</th>\n",
       "      <th>w_XLE</th>\n",
       "      <th>w_XLI</th>\n",
       "      <th>...</th>\n",
       "      <th>s_XLK</th>\n",
       "      <th>s_XLV</th>\n",
       "      <th>s_XLY</th>\n",
       "      <th>s_XLP</th>\n",
       "      <th>s_XLE</th>\n",
       "      <th>s_XLI</th>\n",
       "      <th>s_XLU</th>\n",
       "      <th>s_XLB</th>\n",
       "      <th>s_XLRE</th>\n",
       "      <th>s_XLC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>100000.000000</td>\n",
       "      <td>18013.456625</td>\n",
       "      <td>0.180135</td>\n",
       "      <td>0.0923</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>...</td>\n",
       "      <td>445</td>\n",
       "      <td>316</td>\n",
       "      <td>266</td>\n",
       "      <td>391</td>\n",
       "      <td>201</td>\n",
       "      <td>332</td>\n",
       "      <td>396</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>100096.772869</td>\n",
       "      <td>17969.025246</td>\n",
       "      <td>0.179517</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.0961</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>0.0891</td>\n",
       "      <td>0.0890</td>\n",
       "      <td>...</td>\n",
       "      <td>445</td>\n",
       "      <td>317</td>\n",
       "      <td>264</td>\n",
       "      <td>392</td>\n",
       "      <td>200</td>\n",
       "      <td>330</td>\n",
       "      <td>397</td>\n",
       "      <td>368</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>100352.195221</td>\n",
       "      <td>18001.580901</td>\n",
       "      <td>0.179384</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.0965</td>\n",
       "      <td>0.0890</td>\n",
       "      <td>0.0890</td>\n",
       "      <td>0.0891</td>\n",
       "      <td>0.0888</td>\n",
       "      <td>0.0891</td>\n",
       "      <td>...</td>\n",
       "      <td>447</td>\n",
       "      <td>317</td>\n",
       "      <td>262</td>\n",
       "      <td>393</td>\n",
       "      <td>201</td>\n",
       "      <td>331</td>\n",
       "      <td>397</td>\n",
       "      <td>369</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-06</th>\n",
       "      <td>100138.355886</td>\n",
       "      <td>17983.685228</td>\n",
       "      <td>0.179588</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.0890</td>\n",
       "      <td>0.0890</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>0.0891</td>\n",
       "      <td>...</td>\n",
       "      <td>445</td>\n",
       "      <td>316</td>\n",
       "      <td>261</td>\n",
       "      <td>395</td>\n",
       "      <td>202</td>\n",
       "      <td>331</td>\n",
       "      <td>399</td>\n",
       "      <td>367</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-09</th>\n",
       "      <td>100330.063349</td>\n",
       "      <td>17814.694003</td>\n",
       "      <td>0.177561</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>0.0997</td>\n",
       "      <td>0.0882</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>0.0879</td>\n",
       "      <td>0.0880</td>\n",
       "      <td>...</td>\n",
       "      <td>462</td>\n",
       "      <td>313</td>\n",
       "      <td>259</td>\n",
       "      <td>390</td>\n",
       "      <td>199</td>\n",
       "      <td>325</td>\n",
       "      <td>394</td>\n",
       "      <td>382</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-21</th>\n",
       "      <td>111301.597693</td>\n",
       "      <td>19491.746186</td>\n",
       "      <td>0.175125</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.1034</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.0865</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.0867</td>\n",
       "      <td>0.0867</td>\n",
       "      <td>...</td>\n",
       "      <td>468</td>\n",
       "      <td>293</td>\n",
       "      <td>235</td>\n",
       "      <td>379</td>\n",
       "      <td>210</td>\n",
       "      <td>319</td>\n",
       "      <td>411</td>\n",
       "      <td>408</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-24</th>\n",
       "      <td>111107.651207</td>\n",
       "      <td>19427.252551</td>\n",
       "      <td>0.174851</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0864</td>\n",
       "      <td>0.0862</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>...</td>\n",
       "      <td>473</td>\n",
       "      <td>292</td>\n",
       "      <td>234</td>\n",
       "      <td>378</td>\n",
       "      <td>210</td>\n",
       "      <td>318</td>\n",
       "      <td>411</td>\n",
       "      <td>409</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-26</th>\n",
       "      <td>110743.850407</td>\n",
       "      <td>19475.607354</td>\n",
       "      <td>0.175862</td>\n",
       "      <td>0.0951</td>\n",
       "      <td>0.1021</td>\n",
       "      <td>0.0872</td>\n",
       "      <td>0.0872</td>\n",
       "      <td>0.0871</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.0871</td>\n",
       "      <td>...</td>\n",
       "      <td>464</td>\n",
       "      <td>295</td>\n",
       "      <td>238</td>\n",
       "      <td>383</td>\n",
       "      <td>212</td>\n",
       "      <td>321</td>\n",
       "      <td>417</td>\n",
       "      <td>395</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-27</th>\n",
       "      <td>110618.345195</td>\n",
       "      <td>19413.716434</td>\n",
       "      <td>0.175502</td>\n",
       "      <td>0.0955</td>\n",
       "      <td>0.1026</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.0871</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>...</td>\n",
       "      <td>466</td>\n",
       "      <td>294</td>\n",
       "      <td>237</td>\n",
       "      <td>381</td>\n",
       "      <td>212</td>\n",
       "      <td>320</td>\n",
       "      <td>416</td>\n",
       "      <td>399</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-12-28</th>\n",
       "      <td>109618.004268</td>\n",
       "      <td>19541.493121</td>\n",
       "      <td>0.178269</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.0984</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>0.0881</td>\n",
       "      <td>0.0883</td>\n",
       "      <td>...</td>\n",
       "      <td>448</td>\n",
       "      <td>299</td>\n",
       "      <td>240</td>\n",
       "      <td>388</td>\n",
       "      <td>217</td>\n",
       "      <td>326</td>\n",
       "      <td>423</td>\n",
       "      <td>380</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            portfolio_value          cash       w_c   w_XLF   w_XLK   w_XLV  \\\n",
       "date                                                                          \n",
       "2012-01-03    100000.000000  18013.456625  0.180135  0.0923  0.0959  0.0892   \n",
       "2012-01-04    100096.772869  17969.025246  0.179517  0.0928  0.0961  0.0889   \n",
       "2012-01-05    100352.195221  18001.580901  0.179384  0.0925  0.0965  0.0890   \n",
       "2012-01-06    100138.355886  17983.685228  0.179588  0.0926  0.0964  0.0890   \n",
       "2012-01-09    100330.063349  17814.694003  0.177561  0.0937  0.0997  0.0882   \n",
       "...                     ...           ...       ...     ...     ...     ...   \n",
       "2012-12-21    111301.597693  19491.746186  0.175125  0.0956  0.1034  0.0868   \n",
       "2012-12-24    111107.651207  19427.252551  0.174851  0.0959  0.1044  0.0863   \n",
       "2012-12-26    110743.850407  19475.607354  0.175862  0.0951  0.1021  0.0872   \n",
       "2012-12-27    110618.345195  19413.716434  0.175502  0.0955  0.1026  0.0868   \n",
       "2012-12-28    109618.004268  19541.493121  0.178269  0.0941  0.0984  0.0883   \n",
       "\n",
       "             w_XLY   w_XLP   w_XLE   w_XLI  ...  s_XLK  s_XLV  s_XLY  s_XLP  \\\n",
       "date                                        ...                               \n",
       "2012-01-03  0.0892  0.0892  0.0894  0.0892  ...    445    316    266    391   \n",
       "2012-01-04  0.0892  0.0892  0.0891  0.0890  ...    445    317    264    392   \n",
       "2012-01-05  0.0890  0.0891  0.0888  0.0891  ...    447    317    262    393   \n",
       "2012-01-06  0.0890  0.0892  0.0889  0.0891  ...    445    316    261    395   \n",
       "2012-01-09  0.0880  0.0880  0.0879  0.0880  ...    462    313    259    390   \n",
       "...            ...     ...     ...     ...  ...    ...    ...    ...    ...   \n",
       "2012-12-21  0.0865  0.0868  0.0867  0.0867  ...    468    293    235    379   \n",
       "2012-12-24  0.0864  0.0864  0.0862  0.0863  ...    473    292    234    378   \n",
       "2012-12-26  0.0872  0.0871  0.0869  0.0871  ...    464    295    238    383   \n",
       "2012-12-27  0.0871  0.0869  0.0868  0.0868  ...    466    294    237    381   \n",
       "2012-12-28  0.0883  0.0884  0.0881  0.0883  ...    448    299    240    388   \n",
       "\n",
       "            s_XLE  s_XLI  s_XLU  s_XLB  s_XLRE  s_XLC  \n",
       "date                                                   \n",
       "2012-01-03    201    332    396    366       0      0  \n",
       "2012-01-04    200    330    397    368       0      0  \n",
       "2012-01-05    201    331    397    369       0      0  \n",
       "2012-01-06    202    331    399    367       0      0  \n",
       "2012-01-09    199    325    394    382       0      0  \n",
       "...           ...    ...    ...    ...     ...    ...  \n",
       "2012-12-21    210    319    411    408       0      0  \n",
       "2012-12-24    210    318    411    409       0      0  \n",
       "2012-12-26    212    321    417    395       0      0  \n",
       "2012-12-27    212    320    416    399       0      0  \n",
       "2012-12-28    217    326    423    380       0      0  \n",
       "\n",
       "[249 rows x 25 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backtest_portfolio[0].get_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backtest results summary saved to: ../models/20250603_003619/backtest_results_summary_20250603_003619.csv\n",
      "\n",
      "Final Results DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>best_agent_path</th>\n",
       "      <th>n_eval_episodes</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>Annual return</th>\n",
       "      <th>Cumulative returns</th>\n",
       "      <th>Annual volatility</th>\n",
       "      <th>Sharpe ratio</th>\n",
       "      <th>Calmar ratio</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Max drawdown</th>\n",
       "      <th>Omega ratio</th>\n",
       "      <th>Sortino ratio</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Tail ratio</th>\n",
       "      <th>Daily value at risk (95%)</th>\n",
       "      <th>Portfolio turnover (in %)</th>\n",
       "      <th>final_portfolio_value_first_episode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>agent_seed0_valrew-10</td>\n",
       "      <td>1</td>\n",
       "      <td>-19.423373</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097805</td>\n",
       "      <td>0.09618</td>\n",
       "      <td>0.099488</td>\n",
       "      <td>0.987823</td>\n",
       "      <td>1.350245</td>\n",
       "      <td>0.909514</td>\n",
       "      <td>-0.072435</td>\n",
       "      <td>1.181739</td>\n",
       "      <td>1.525845</td>\n",
       "      <td>0.043187</td>\n",
       "      <td>0.782472</td>\n",
       "      <td>1.158062</td>\n",
       "      <td>-0.009649</td>\n",
       "      <td>1.169297</td>\n",
       "      <td>109618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   window        best_agent_path  n_eval_episodes  mean_reward  std_reward  \\\n",
       "0       1  agent_seed0_valrew-10                1   -19.423373         0.0   \n",
       "\n",
       "   Annual return  Cumulative returns  Annual volatility  Sharpe ratio  \\\n",
       "0       0.097805             0.09618           0.099488      0.987823   \n",
       "\n",
       "   Calmar ratio  Stability  Max drawdown  Omega ratio  Sortino ratio  \\\n",
       "0      1.350245   0.909514     -0.072435     1.181739       1.525845   \n",
       "\n",
       "       Skew  Kurtosis  Tail ratio  Daily value at risk (95%)  \\\n",
       "0  0.043187  0.782472    1.158062                  -0.009649   \n",
       "\n",
       "   Portfolio turnover (in %)  final_portfolio_value_first_episode  \n",
       "0                   1.169297                               109618  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_filename = f\"backtest_results_summary_{timestamp}.csv\"\n",
    "results_save_path = os.path.join(config.model_save_dir, results_filename)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(results_save_path, index=False)\n",
    "print(f\"\\nBacktest results summary saved to: {results_save_path}\")\n",
    "print(\"\\nFinal Results DataFrame:\")\n",
    "results_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
