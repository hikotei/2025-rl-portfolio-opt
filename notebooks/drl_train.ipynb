{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Callable\n",
    "from datetime import datetime\n",
    "\n",
    "from utils.portfolio_env import PortfolioEnv\n",
    "from utils.drl_agent import DRLAgent  # Import the modified agent\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Logging\n",
    "\n",
    "This notebook logs training progress using TensorBoard. Logs for each agent and window will be saved in subdirectories within the `../logs/` directory (relative to this notebook's location).\n",
    "\n",
    "To view the logs:\n",
    "1. Open a terminal or command prompt.\n",
    "2. Navigate to the directory *containing* the `logs` directory (i.e., the root of this repository if you are running the notebook from the `notebooks` folder).\n",
    "3. Run the command: `tensorboard --logdir logs/`\n",
    "4. Open the URL provided by TensorBoard (usually http://localhost:6006/) in your web browser.\n",
    "\n",
    "You should see experiments named like `PPO_WindowX_AgentY_SeedZ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Configuration ---\n",
    "N_WINDOWS = 2  # 10 in paper\n",
    "AGENTS_PER_WINDOW = 2  # 5 in paper\n",
    "BASE_START_YEAR = 2006\n",
    "\n",
    "# Data paths\n",
    "PRICE_DATA_PATH = \"../data/prices.parquet\"\n",
    "RETURNS_DATA_PATH = \"../data/returns.parquet\"\n",
    "VOLA_DATA_PATH = \"../data/vola.parquet\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_SAVE_DIR = f\"../models/{timestamp}/\"\n",
    "TENSORBOARD_LOG_DIR = f\"../logs/{timestamp}/\"\n",
    "\n",
    "# Ensure model save directory exists\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(TENSORBOARD_LOG_DIR, exist_ok=True)\n",
    "\n",
    "# --- DRL Agent Hyperparameters (from paper) ---\n",
    "N_ENVS = 10\n",
    "TOTAL_TIMESTEPS_PER_ROUND = 10**5 # 7_500_000  in paper\n",
    "N_STEPS_PER_ENV = 252 * 3  # n_steps = 252 * 3 * n_envs (this is per env for PPO buffer)\n",
    "# total buffer size before update = N_STEPS_PER_ENV * N_ENVS\n",
    "\n",
    "BATCH_SIZE = 1260\n",
    "N_EPOCHS = 16\n",
    "GAMMA = 0.9\n",
    "GAE_LAMBDA = 0.9\n",
    "CLIP_RANGE = 0.25\n",
    "LOG_STD_INIT = -1.0\n",
    "POLICY_KWARGS = dict(\n",
    "    activation_fn=torch.nn.Tanh,\n",
    "    net_arch=[64, 64],  # Shared layers for policy and value networks\n",
    "    log_std_init=LOG_STD_INIT,\n",
    ")\n",
    "\n",
    "# Learning rate schedule: linear decay from 3e-4 to 1e-5\n",
    "INITIAL_LR = 3e-4\n",
    "FINAL_LR = 1e-5\n",
    "\n",
    "\n",
    "def linear_schedule(\n",
    "    initial_value: float, final_value: float\n",
    ") -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :param final_value: Final learning rate.\n",
    "    :return: schedule that computes current learning rate depending on progress remaining (1.0 -> 0.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1.0 to 0.0\n",
    "        \"\"\"\n",
    "        return final_value + progress_remaining * (initial_value - final_value)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "LEARNING_RATE_SCHEDULE = linear_schedule(INITIAL_LR, FINAL_LR)\n",
    "\n",
    "# --- PortfolioEnv Parameters ---\n",
    "ENV_WINDOW_SIZE = 60  # Lookback window for features in PortfolioEnv\n",
    "TRANSACTION_COST = 0.0  # As per paper (or can be adjusted)\n",
    "INITIAL_BALANCE = 100_000\n",
    "REWARD_SCALING = 1.0\n",
    "ETA_DSR = 1 / 252  # For Differential Sharpe Ratio in PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the full datasets once\n",
    "try:\n",
    "    print(\"Loading data...\")\n",
    "    prices_df_full = pd.read_parquet(PRICE_DATA_PATH)\n",
    "    returns_df_full = pd.read_parquet(RETURNS_DATA_PATH)\n",
    "    vola_df_full = pd.read_parquet(VOLA_DATA_PATH)\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "    # Ensure DataFrames have DateTimeIndex\n",
    "    for df in [prices_df_full, returns_df_full, vola_df_full]:\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # print(\"\\nPrice Data Head:\")\n",
    "    # print(prices_df_full.head())\n",
    "    # print(\"\\nReturns Data Head:\")\n",
    "    # print(returns_df_full.head())\n",
    "    # print(\"\\nVolatility Data Head:\")\n",
    "    # print(vola_df_full.head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Data file not found. {e}\")\n",
    "    print(\"Please ensure data is generated and paths are correct in Cell 2.\")\n",
    "    # Stop execution or raise error if data is critical for notebook to run\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(\n",
    "    year_start,\n",
    "    num_train_years,\n",
    "    num_val_years,\n",
    "    num_test_years,\n",
    "    prices_df,\n",
    "    returns_df,\n",
    "    vol_df,\n",
    "):\n",
    "    \"\"\"Slices data for a given window configuration.\"\"\"\n",
    "\n",
    "    train_start_date = pd.to_datetime(f\"{year_start}-01-01\")\n",
    "    train_end_date = pd.to_datetime(f\"{year_start + num_train_years - 1}-12-31\")\n",
    "\n",
    "    val_start_date = pd.to_datetime(f\"{year_start + num_train_years}-01-01\")\n",
    "    val_end_date = pd.to_datetime(\n",
    "        f\"{year_start + num_train_years + num_val_years - 1}-12-31\"\n",
    "    )\n",
    "\n",
    "    test_start_date = pd.to_datetime(\n",
    "        f\"{year_start + num_train_years + num_val_years}-01-01\"\n",
    "    )\n",
    "    test_end_date = pd.to_datetime(\n",
    "        f\"{year_start + num_train_years + num_val_years + num_test_years - 1}-12-31\"\n",
    "    )\n",
    "\n",
    "    print(f\"  Train Period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "    print(f\"  Val Period  : {val_start_date.date()} to {val_end_date.date()}\")\n",
    "    print(f\"  Test Period : {test_start_date.date()} to {test_end_date.date()}\")\n",
    "\n",
    "    # Slicing (ensure index is datetime)\n",
    "    train_prices = prices_df[train_start_date:train_end_date]\n",
    "    train_returns = returns_df[train_start_date:train_end_date]\n",
    "    train_vola = vol_df[train_start_date:train_end_date]\n",
    "\n",
    "    val_prices = prices_df[val_start_date:val_end_date]\n",
    "    val_returns = returns_df[val_start_date:val_end_date]\n",
    "    val_vola = vol_df[val_start_date:val_end_date]\n",
    "\n",
    "    test_prices = prices_df[test_start_date:test_end_date]\n",
    "    test_returns = returns_df[test_start_date:test_end_date]\n",
    "    test_vola = vol_df[test_start_date:test_end_date]\n",
    "\n",
    "    # Basic check for empty slices which can halt env creation\n",
    "    if train_prices.empty or val_prices.empty or test_prices.empty:\n",
    "        print(\n",
    "            \"WARNING: One or more data slices are empty. Check date ranges and data availability.\"\n",
    "        )\n",
    "        # Potentially raise an error or handle as per requirements\n",
    "\n",
    "    return (\n",
    "        (train_prices, train_returns, train_vola),\n",
    "        (val_prices, val_returns, val_vola),\n",
    "        (test_prices, test_returns, test_vola),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Window 1/2 (Train Year Start: 2006) ---\n",
      "  Train Period: 2006-01-01 to 2010-12-31\n",
      "  Val Period  : 2011-01-01 to 2011-12-31\n",
      "  Test Period : 2012-01-01 to 2012-12-31\n",
      "  Training Agent 1/2 with seed 0...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e8412691334cd18e598a12884eea53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for 100000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 100000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Window1_Agent1_Seed0' saved in directory: ../logs/20250601_232410/\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -2.4807\n",
      "    Agent saved to: ../models/20250601_232410/agent_win1_seed0_valrew-2.48.zip\n",
      "    New best agent for this window with validation reward: -2.4807\n",
      "  Training Agent 2/2 with seed 1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c492679ecf042008af85d13fc2404d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Starting training for 100000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 100000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Window1_Agent2_Seed1' saved in directory: ../logs/20250601_232410/\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: -2.5549\n",
      "    Agent saved to: ../models/20250601_232410/agent_win1_seed1_valrew-2.55.zip\n",
      "  Backtesting best agent for Window 1 (../models/20250601_232410/agent_win1_seed0_valrew-2.48.zip)\n",
      "    Loading model from: ../models/20250601_232410/agent_win1_seed0_valrew-2.48.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 1:\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 102169\n",
      "      mean_reward: 12.1483811359796\n",
      "      std_reward: 0.0\n",
      "      Annual return: 0.029026698977375975\n",
      "      Cumulative returns: 0.021691975259780838\n",
      "      Annual volatility: 0.10519099256227284\n",
      "      Sharpe ratio: 0.13447316547230942\n",
      "      Calmar ratio: 0.4010203445368113\n",
      "      Stability: 0.9048209827349404\n",
      "      Max drawdown: -0.07238211071536171\n",
      "      Omega ratio: 1.0229470915146917\n",
      "      Sortino ratio: 0.20755973715370446\n",
      "      Skew: 0.07673864423852526\n",
      "      Kurtosis: 0.642067084086444\n",
      "      Tail ratio: 1.036969892820598\n",
      "      Daily value at risk (95%): -0.011565902794263035\n",
      "      Portfolio turnover: nan\n",
      "--- Starting Window 2/2 (Train Year Start: 2007) ---\n",
      "  Train Period: 2007-01-01 to 2011-12-31\n",
      "  Val Period  : 2012-01-01 to 2012-12-31\n",
      "  Test Period : 2013-01-01 to 2013-12-31\n",
      "  Training Agent 1/2 with seed 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "080fd24a3f254cebbb8dc74700dcfd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/20250601_232410/agent_win1_seed0_valrew-2.48.zip\n",
      "    Starting training for 100000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 100000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Window2_Agent1_Seed2' saved in directory: ../logs/20250601_232410/\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 12.2294\n",
      "    Agent saved to: ../models/20250601_232410/agent_win2_seed2_valrew12.23.zip\n",
      "    New best agent for this window with validation reward: 12.2294\n",
      "  Training Agent 2/2 with seed 3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df41db4dba9f42f3aaeaa2c05452d07f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seeding agent from: ../models/20250601_232410/agent_win1_seed0_valrew-2.48.zip\n",
      "    Starting training for 100000 timesteps...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 100000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Window2_Agent2_Seed3' saved in directory: ../logs/20250601_232410/\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Mean Reward: 12.1947\n",
      "    Agent saved to: ../models/20250601_232410/agent_win2_seed3_valrew12.19.zip\n",
      "  Backtesting best agent for Window 2 (../models/20250601_232410/agent_win2_seed2_valrew12.23.zip)\n",
      "    Loading model from: ../models/20250601_232410/agent_win2_seed2_valrew12.23.zip\n",
      "    Running backtest evaluation...\n",
      "    Backtest Metrics for Window 2:\n",
      "      n_eval_episodes: 1\n",
      "      final_portfolio_value_first_episode: 114545\n",
      "      mean_reward: 19.251787125342172\n",
      "      std_reward: 0.0\n",
      "      Annual return: 0.19622740608730038\n",
      "      Cumulative returns: 0.14545459957122797\n",
      "      Annual volatility: 0.09248403580625901\n",
      "      Sharpe ratio: 1.7680683412332245\n",
      "      Calmar ratio: 4.024603976482638\n",
      "      Stability: 0.9153451832932228\n",
      "      Max drawdown: -0.048756947822428044\n",
      "      Omega ratio: 1.3302090701882234\n",
      "      Sortino ratio: 2.533594293388693\n",
      "      Skew: -0.4746385161529038\n",
      "      Kurtosis: 1.1417155698333357\n",
      "      Tail ratio: 1.022117265364049\n",
      "      Daily value at risk (95%): -0.00924501892429602\n",
      "      Portfolio turnover: nan\n",
      "\n",
      "--- All Windows Processed ---\n",
      "Summary of Best Agent Paths:\n",
      "Window 1: ../models/20250601_232410/agent_win1_seed0_valrew-2.48.zip\n",
      "Window 2: ../models/20250601_232410/agent_win2_seed2_valrew12.23.zip\n",
      "\n",
      "Summary of Backtest Results:\n",
      "Window 1 (completed):\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 102169\n",
      "    mean_reward: 12.1484\n",
      "    std_reward: 0.0000\n",
      "    Annual return: 0.0290\n",
      "    Cumulative returns: 0.0217\n",
      "    Annual volatility: 0.1052\n",
      "    Sharpe ratio: 0.1345\n",
      "    Calmar ratio: 0.4010\n",
      "    Stability: 0.9048\n",
      "    Max drawdown: -0.0724\n",
      "    Omega ratio: 1.0229\n",
      "    Sortino ratio: 0.2076\n",
      "    Skew: 0.0767\n",
      "    Kurtosis: 0.6421\n",
      "    Tail ratio: 1.0370\n",
      "    Daily value at risk (95%): -0.0116\n",
      "    Portfolio turnover: nan\n",
      "Window 2 (completed):\n",
      "    n_eval_episodes: 1\n",
      "    final_portfolio_value_first_episode: 114545\n",
      "    mean_reward: 19.2518\n",
      "    std_reward: 0.0000\n",
      "    Annual return: 0.1962\n",
      "    Cumulative returns: 0.1455\n",
      "    Annual volatility: 0.0925\n",
      "    Sharpe ratio: 1.7681\n",
      "    Calmar ratio: 4.0246\n",
      "    Stability: 0.9153\n",
      "    Max drawdown: -0.0488\n",
      "    Omega ratio: 1.3302\n",
      "    Sortino ratio: 2.5336\n",
      "    Skew: -0.4746\n",
      "    Kurtosis: 1.1417\n",
      "    Tail ratio: 1.0221\n",
      "    Daily value at risk (95%): -0.0092\n",
      "    Portfolio turnover: nan\n"
     ]
    }
   ],
   "source": [
    "all_backtest_results = []\n",
    "best_agent_paths_per_window = []\n",
    "\n",
    "# > PAPER : The data is split into 10 sliding window groups (shifted by 1-year). \n",
    "# Each group contains 7 years worth of data, \n",
    "# the first 5 years are used for training, \n",
    "# the next 1 year is a burn year used for training validation, \n",
    "# and the last year is kept out-of-sample for backtesting.\n",
    "\n",
    "# --- Main Loop for Sliding Windows ---\n",
    "for i_window in range(N_WINDOWS):\n",
    "    current_start_year = BASE_START_YEAR + i_window\n",
    "    print(f\"--- Starting Window {i_window+1}/{N_WINDOWS} (Train Year Start: {current_start_year}) ---\")\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    # 1. Slice Data for the current window\n",
    "    # 5 years train, 1 year validation, 1 year test\n",
    "    train_data, val_data, test_data = slice_data(\n",
    "        year_start=current_start_year,\n",
    "        num_train_years=5,\n",
    "        num_val_years=1,\n",
    "        num_test_years=1,\n",
    "        prices_df=prices_df_full,\n",
    "        returns_df=returns_df_full,\n",
    "        vol_df=vola_df_full\n",
    "    )\n",
    "    \n",
    "    # Unpack data\n",
    "    (train_prices, train_returns, train_vola) = train_data\n",
    "    (val_prices, val_returns, val_vola) = val_data\n",
    "    (test_prices, test_returns, test_vola) = test_data\n",
    "\n",
    "    # Check if any crucial dataframe is too short (e.g., shorter than ENV_WINDOW_SIZE)\n",
    "    # PortfolioEnv requires at least `window_size` days of data to start.\n",
    "    min_data_len = ENV_WINDOW_SIZE + 1 # Need at least window_size + 1 for one step\n",
    "    if len(train_prices) < min_data_len or len(val_prices) < min_data_len or len(test_prices) < min_data_len:\n",
    "        print(f\"SKIPPING Window {i_window+1} due to insufficient data length for one or more periods.\")\n",
    "        print(f\"  Train length: {len(train_prices)}, Val length: {len(val_prices)}, Test length: {len(test_prices)}\")\n",
    "        print(f\"  Required minimum: {min_data_len}\")\n",
    "        best_agent_paths_per_window.append(None) # Mark as skipped\n",
    "        all_backtest_results.append({\"window\": i_window+1, \"status\": \"skipped_insufficient_data\", \"metrics\": {}})\n",
    "        continue\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    # 2. Create Training and Validation Environments\n",
    "    # These envs are re-created for each agent to ensure fresh state and correct data.\n",
    "    # However, the data slice itself is per-window.\n",
    "\n",
    "    best_agent_for_window = None\n",
    "    best_val_reward = -np.inf\n",
    "    \n",
    "    # --- Inner Loop for Training AGENTS_PER_WINDOW Agents ---\n",
    "    for i_agent in range(AGENTS_PER_WINDOW):\n",
    "\n",
    "        # > PAPER : During the first round of training, we initialize 5 agents (different seeds)\n",
    "        agent_seed = (i_window * AGENTS_PER_WINDOW) + i_agent # Unique seed for each agent run\n",
    "        print(f\"  Training Agent {i_agent+1}/{AGENTS_PER_WINDOW} with seed {agent_seed}...\")\n",
    "\n",
    "        # Create environments for this specific agent\n",
    "        # Training Env\n",
    "        env_train_config = {\n",
    "            'returns_df': train_returns, 'prices_df': train_prices, 'vol_df': train_vola,\n",
    "            'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "            'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "        }\n",
    "        # The DRLAgent class will use this first env to understand structure for SubprocVecEnv\n",
    "        # This single_env_for_init is just for the DRLAgent constructor to get parameters.\n",
    "        # The actual training will use N_ENVS created by DRLAgent.\n",
    "        single_env_for_init_train = PortfolioEnv(**env_train_config)\n",
    "\n",
    "        # Validation Env (single, not vectorized for evaluation)\n",
    "        env_val_config = {\n",
    "            'returns_df': val_returns, 'prices_df': val_prices, 'vol_df': val_vola,\n",
    "            'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "            'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "        }\n",
    "        env_val = PortfolioEnv(**env_val_config)\n",
    "        \n",
    "        # Instantiate DRL Agent\n",
    "        agent = DRLAgent(\n",
    "            env=single_env_for_init_train, # Pass the sample env for DRLAgent to clone\n",
    "            n_envs=N_ENVS,\n",
    "            policy_kwargs=POLICY_KWARGS,\n",
    "            n_steps=N_STEPS_PER_ENV, # n_steps per environment for PPO\n",
    "            batch_size=BATCH_SIZE,\n",
    "            n_epochs=N_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE_SCHEDULE,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            clip_range=CLIP_RANGE,\n",
    "            seed=agent_seed,\n",
    "            tensorboard_log=TENSORBOARD_LOG_DIR\n",
    "        )\n",
    "\n",
    "        # Agent Seeding: Load previous window's best agent if not the first window\n",
    "        if i_window > 0 and best_agent_paths_per_window[i_window-1] is not None:\n",
    "            previous_best_agent_path = best_agent_paths_per_window[i_window-1]\n",
    "            print(f\"    Seeding agent from: {previous_best_agent_path}\")\n",
    "            agent.load(path=previous_best_agent_path, env=None) \n",
    "            agent.model.set_random_seed(agent_seed) # Ensure the loaded model uses the new agent_seed\n",
    "                                   \n",
    "        # Train the agent\n",
    "        print(f\"    Starting training for {TOTAL_TIMESTEPS_PER_ROUND} timesteps...\")\n",
    "        # Note: Training can be very long. For testing, reduce TOTAL_TIMESTEPS_PER_ROUND.\n",
    "        # Example: agent.train(total_timesteps=10000, tb_log_name=f\"ppo_win{i_window}_agent{i_agent}\")\n",
    "        agent.train(\n",
    "            total_timesteps=TOTAL_TIMESTEPS_PER_ROUND, \n",
    "            tb_experiment_name=f\"PPO_Window{i_window+1}_Agent{i_agent+1}_Seed{agent_seed}\",\n",
    "        )\n",
    "\n",
    "        # > PAPER : All five agents start training on data from [2006−2011) \n",
    "        # and their performance is periodically evaluated using the validation period 2011\n",
    "\n",
    "        # Evaluate the agent on the validation set\n",
    "        print(\"    Evaluating agent on validation set...\")\n",
    "        # The evaluate method in DRLAgentJules is designed for a single eval_env\n",
    "        val_metrics = agent.evaluate(eval_env=env_val, n_eval_episodes=1)\n",
    "        current_val_reward = val_metrics.get(\"mean_reward\", -np.inf)\n",
    "        print(f\"    Validation Mean Reward: {current_val_reward:.4f}\")\n",
    "        \n",
    "        # Save this agent\n",
    "        current_agent_model_name = f\"agent_win{i_window+1}_seed{agent_seed}_valrew{current_val_reward:.2f}.zip\"\n",
    "        current_agent_save_path = os.path.join(MODEL_SAVE_DIR, current_agent_model_name)\n",
    "        agent.save(current_agent_save_path)\n",
    "        print(f\"    Agent saved to: {current_agent_save_path}\")\n",
    "\n",
    "        # > PAPER : At the end of the first round of training, we save the best performing agent \n",
    "        # (based on highest mean episode validation reward)\n",
    "        if current_val_reward > best_val_reward:\n",
    "            best_val_reward = current_val_reward\n",
    "            best_agent_for_window_path = current_agent_save_path \n",
    "            print(f\"    New best agent for this window with validation reward: {best_val_reward:.4f}\")\n",
    "\n",
    "        # Clean up to free memory\n",
    "        del agent\n",
    "        del single_env_for_init_train\n",
    "        del env_val\n",
    "\n",
    "    best_agent_paths_per_window.append(best_agent_for_window_path if 'best_agent_for_window_path' in locals() and best_agent_for_window_path is not None else None)\n",
    "    \n",
    "    if best_agent_paths_per_window[-1] is None:\n",
    "        print(f\"  No best agent found or saved for window {i_window+1}. Skipping backtest.\")\n",
    "        all_backtest_results.append({\"window\": i_window+1, \"status\": \"no_best_agent\", \"metrics\": {}})\n",
    "        continue\n",
    "    \n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    # 3. Backtest the best agent of the window\n",
    "    print(f\"  Backtesting best agent for Window {i_window+1} ({best_agent_paths_per_window[-1]})\" )\n",
    "    \n",
    "    # Create Backtesting Environment\n",
    "    env_test_config = {\n",
    "        'returns_df': test_returns, 'prices_df': test_prices, 'vol_df': test_vola,\n",
    "        'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "        'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "    }\n",
    "    env_test = PortfolioEnv(**env_test_config)\n",
    "\n",
    "    best_agent_loaded = DRLAgent(\n",
    "        env=env_test, \n",
    "        n_envs=1, # For eval, n_envs=1 is fine for the DRLAgent wrapper\n",
    "        policy_kwargs=POLICY_KWARGS \n",
    "        # Other params don't matter as we are loading a pre-trained model\n",
    "    )\n",
    "    \n",
    "    print(f\"    Loading model from: {best_agent_paths_per_window[-1]}\")\n",
    "    # Pass the actual test_env to PPO.load via DRLAgent.load method\n",
    "    best_agent_loaded.load(path=best_agent_paths_per_window[-1], env=env_test) \n",
    "                                   \n",
    "    print(\"    Running backtest evaluation...\")\n",
    "    backtest_metrics = best_agent_loaded.evaluate(eval_env=env_test, n_eval_episodes=1)\n",
    "    \n",
    "    print(f\"    Backtest Metrics for Window {i_window+1}:\")\n",
    "    for key, value in backtest_metrics.items():\n",
    "        print(f\"      {key}: {value}\")\n",
    "    \n",
    "    all_backtest_results.append({\n",
    "        \"window\": i_window+1, \n",
    "        \"best_agent_path\": best_agent_paths_per_window[-1],\n",
    "        \"status\": \"completed\",\n",
    "        \"metrics\": backtest_metrics\n",
    "    })\n",
    "    \n",
    "    del best_agent_loaded\n",
    "    del env_test\n",
    "    torch.cuda.empty_cache() # If using GPU\n",
    "\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "# 4. Save results\n",
    "print(\"\\n--- All Windows Processed ---\")\n",
    "print(\"Summary of Best Agent Paths:\")\n",
    "for i, path in enumerate(best_agent_paths_per_window):\n",
    "    print(f\"Window {i+1}: {path}\")\n",
    "\n",
    "print(\"\\nSummary of Backtest Results:\")\n",
    "for result in all_backtest_results:\n",
    "    print(f\"Window {result['window']} ({result['status']}):\")\n",
    "    if result['status'] == 'completed':\n",
    "        # print(f\"  Agent: {result['best_agent_path']}\")\n",
    "        for k, v in result['metrics'].items():\n",
    "            if isinstance(v, float): print(f\"    {k}: {v:.4f}\")\n",
    "            else: print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backtest results summary saved to: ../models/20250601_232410/backtest_results_summary_20250601_232508.csv\n",
      "\n",
      "Final Results DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>best_agent_path</th>\n",
       "      <th>status</th>\n",
       "      <th>n_eval_episodes</th>\n",
       "      <th>final_portfolio_value_first_episode</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>Annual return</th>\n",
       "      <th>Cumulative returns</th>\n",
       "      <th>Annual volatility</th>\n",
       "      <th>...</th>\n",
       "      <th>Calmar ratio</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Max drawdown</th>\n",
       "      <th>Omega ratio</th>\n",
       "      <th>Sortino ratio</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Tail ratio</th>\n",
       "      <th>Daily value at risk (95%)</th>\n",
       "      <th>Portfolio turnover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>../models/20250601_232410/agent_win1_seed0_val...</td>\n",
       "      <td>completed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>102169.0</td>\n",
       "      <td>12.148381</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029027</td>\n",
       "      <td>0.021692</td>\n",
       "      <td>0.105191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.401020</td>\n",
       "      <td>0.904821</td>\n",
       "      <td>-0.072382</td>\n",
       "      <td>1.022947</td>\n",
       "      <td>0.207560</td>\n",
       "      <td>0.076739</td>\n",
       "      <td>0.642067</td>\n",
       "      <td>1.036970</td>\n",
       "      <td>-0.011566</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>../models/20250601_232410/agent_win2_seed2_val...</td>\n",
       "      <td>completed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114545.0</td>\n",
       "      <td>19.251787</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.196227</td>\n",
       "      <td>0.145455</td>\n",
       "      <td>0.092484</td>\n",
       "      <td>...</td>\n",
       "      <td>4.024604</td>\n",
       "      <td>0.915345</td>\n",
       "      <td>-0.048757</td>\n",
       "      <td>1.330209</td>\n",
       "      <td>2.533594</td>\n",
       "      <td>-0.474639</td>\n",
       "      <td>1.141716</td>\n",
       "      <td>1.022117</td>\n",
       "      <td>-0.009245</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   window                                    best_agent_path     status  \\\n",
       "0       1  ../models/20250601_232410/agent_win1_seed0_val...  completed   \n",
       "1       2  ../models/20250601_232410/agent_win2_seed2_val...  completed   \n",
       "\n",
       "   n_eval_episodes  final_portfolio_value_first_episode  mean_reward  \\\n",
       "0              1.0                             102169.0    12.148381   \n",
       "1              1.0                             114545.0    19.251787   \n",
       "\n",
       "   std_reward  Annual return  Cumulative returns  Annual volatility  ...  \\\n",
       "0         0.0       0.029027            0.021692           0.105191  ...   \n",
       "1         0.0       0.196227            0.145455           0.092484  ...   \n",
       "\n",
       "   Calmar ratio  Stability  Max drawdown  Omega ratio  Sortino ratio  \\\n",
       "0      0.401020   0.904821     -0.072382     1.022947       0.207560   \n",
       "1      4.024604   0.915345     -0.048757     1.330209       2.533594   \n",
       "\n",
       "       Skew  Kurtosis  Tail ratio  Daily value at risk (95%)  \\\n",
       "0  0.076739  0.642067    1.036970                  -0.011566   \n",
       "1 -0.474639  1.141716    1.022117                  -0.009245   \n",
       "\n",
       "   Portfolio turnover  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(all_backtest_results)\n",
    "\n",
    "# Expand the 'metrics' dictionary into separate columns\n",
    "metrics_df = results_df[\"metrics\"].apply(pd.Series)\n",
    "results_df = pd.concat([results_df.drop(\"metrics\", axis=1), metrics_df], axis=1)\n",
    "\n",
    "results_filename = (\n",
    "    f\"backtest_results_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    ")\n",
    "results_save_path = os.path.join(MODEL_SAVE_DIR, results_filename)\n",
    "results_df.to_csv(results_save_path, index=False)\n",
    "print(f\"\\nBacktest results summary saved to: {results_save_path}\")\n",
    "print(\"\\nFinal Results DataFrame:\")\n",
    "results_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
