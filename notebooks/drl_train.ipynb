{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dataclasses import asdict\n",
    "\n",
    "from datetime import datetime\n",
    "from utils.config import DRLConfig\n",
    "from utils.drl_train import training_pipeline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SnP500 Sector\n",
    "# DATA_DIR = \"../data/snp_new\"\n",
    "# RETURNS_PATH = os.path.join(DATA_DIR, \"returns_1d.parquet\")\n",
    "# PRICES_PATH = os.path.join(DATA_DIR, \"prices_1d.parquet\")\n",
    "# VOLA_PATH = os.path.join(DATA_DIR, \"vola_1d.parquet\")\n",
    "\n",
    "# MSCI World Index\n",
    "DATA_DIR = \"../data/msci\"\n",
    "RETURNS_PATH = os.path.join(DATA_DIR, \"returns_1d.parquet\")\n",
    "PRICES_PATH = os.path.join(DATA_DIR, \"prices_1d.parquet\")\n",
    "VOLA_PATH = os.path.join(DATA_DIR, \"vola_1d.parquet\")\n",
    "\n",
    "df_ret = pd.read_parquet(RETURNS_PATH)\n",
    "df_prices = pd.read_parquet(PRICES_PATH)\n",
    "df_vol = pd.read_parquet(VOLA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configuration saved to: ../models/20250702_205602_train_start=2013_best_seed=False/config_20250702_205602.json\n"
     ]
    }
   ],
   "source": [
    "# To view the logs:\n",
    "# 1. Open a terminal or command prompt.\n",
    "# 2. Navigate to the directory *containing* the `logs` directory (i.e., the root of this repository).\n",
    "# 3. Run the command: `tensorboard --logdir logs/`\n",
    "# 4. Open the URL provided by TensorBoard (usually http://localhost:6006/) in your web browser.\n",
    "\n",
    "# Create timestamp for this run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "N_WINDOWS = 10  # 10 in paper\n",
    "N_AGENTS = 5  # 5 in paper\n",
    "START_YR = 2006  # 2006 in paper\n",
    "TOTAL_STEPS = 7_500_000 # 7_500_000 in paper\n",
    "SEED_POLICY = False\n",
    "FOLDER_NAME = f\"{timestamp}_train_start={START_YR}_best_seed={SEED_POLICY}\"\n",
    "\n",
    "# Create configuration\n",
    "config = DRLConfig(\n",
    "    # Window configuration\n",
    "    n_windows=N_WINDOWS,\n",
    "    agents_per_window=N_AGENTS,\n",
    "    base_start_year=START_YR,\n",
    "    seed_policy=SEED_POLICY,\n",
    "    # Environment parameters\n",
    "    env_window_size=60,\n",
    "    transaction_cost=0.0,\n",
    "    initial_balance=100_000,\n",
    "    reward_scaling=1.0,\n",
    "    eta_dsr=1 / 252,\n",
    "    # Training parameters\n",
    "    n_envs=10,\n",
    "    total_timesteps_per_round=TOTAL_STEPS,\n",
    "    n_steps_per_env=252 * 3,\n",
    "    batch_size=1260,\n",
    "    n_epochs=16,\n",
    "    gamma=0.9,\n",
    "    gae_lambda=0.9,\n",
    "    clip_range=0.25,\n",
    "    log_std_init=-1.0,\n",
    "    # Learning rate parameters\n",
    "    initial_lr=3e-4,\n",
    "    final_lr=1e-5,\n",
    "    # Paths\n",
    "    data_dir=DATA_DIR,\n",
    "    model_save_dir=f\"../models/{FOLDER_NAME}\",\n",
    "    tensorboard_log_dir=f\"../logs/{FOLDER_NAME}\",\n",
    "    # prev_best_model_dir=\"../models/full_random_run/agent_6-1_seed=25_test=2017_valrew=43.20.zip\",\n",
    ")\n",
    "\n",
    "config_dict = asdict(config)\n",
    "config_json_path = os.path.join(config.model_save_dir, f\"config_{timestamp}.json\")\n",
    "os.makedirs(config.model_save_dir, exist_ok=True)\n",
    "with open(config_json_path, \"w\") as f:\n",
    "    json.dump(config_dict, f, indent=4)\n",
    "print(f\"\\nConfiguration saved to: {config_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Window 1/1 (Train Year Start: 2013) ---\n",
      "  Train Period: 2013-01-01 to 2017-12-31\n",
      "  Val Period  : 2018-01-01 to 2018-12-31\n",
      "  Test Period : 2019-01-01 to 2019-12-31\n",
      "  Starting with fresh random initialization\n",
      "  Training Agent 1/2 with seed 38531...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad10612da181491a9a0f35e8bf8ec6d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 3500000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Seed=38531' saved in directory: ../logs/20250702_205602_train_start=2013_best_seed=False\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Reward: -44.13468237\n",
      "    Agent saved to: ../models/20250702_205602_train_start=2013_best_seed=False/agent_1-1_seed=38531_test=2019_valrew=-44.13.zip\n",
      "\n",
      "  Training Agent 2/2 with seed 27845...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00769163906c4c26a71981a133b235c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete. Trained for 3500000 timesteps.\n",
      "TensorBoard logs for experiment 'PPO_Seed=27845' saved in directory: ../logs/20250702_205602_train_start=2013_best_seed=False\n",
      "    Evaluating agent on validation set...\n",
      "    Validation Reward: -46.08051953\n",
      "    Agent saved to: ../models/20250702_205602_train_start=2013_best_seed=False/agent_1-2_seed=27845_test=2019_valrew=-46.08.zip\n",
      "\n",
      "best_agent_path: ../models/20250702_205602_train_start=2013_best_seed=False/agent_1-1_seed=38531_test=2019_valrew=-44.13.zip\n",
      "    Running backtest evaluation...\n",
      "\n",
      "Saving backtest portfolio: Portfolio with 8 assets, initial value: $100,000.00, current value: $127,042.55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run training pipeline\n",
    "results_df, backtest_portfolio = training_pipeline(\n",
    "    drl_config=config, df_prices=df_prices, df_ret=df_ret, df_vol=df_vol\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>best_agent_path</th>\n",
       "      <th>n_eval_episodes</th>\n",
       "      <th>val_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>Annual return</th>\n",
       "      <th>Cumulative returns</th>\n",
       "      <th>Annual volatility</th>\n",
       "      <th>Sharpe ratio</th>\n",
       "      <th>Calmar ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>Omega ratio</th>\n",
       "      <th>Sortino ratio</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Tail ratio</th>\n",
       "      <th>Daily value at risk (95%)</th>\n",
       "      <th>Avg Annual Turnover (in %)</th>\n",
       "      <th>final_portfolio_value_first_episode</th>\n",
       "      <th>val_reward_mean</th>\n",
       "      <th>val_reward_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>agent_1-1_seed=38531_test=2019_valrew=-44</td>\n",
       "      <td>1</td>\n",
       "      <td>11.832245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.27286</td>\n",
       "      <td>0.270425</td>\n",
       "      <td>0.112971</td>\n",
       "      <td>2.193221</td>\n",
       "      <td>3.777494</td>\n",
       "      <td>...</td>\n",
       "      <td>1.453871</td>\n",
       "      <td>2.86022</td>\n",
       "      <td>-0.426346</td>\n",
       "      <td>2.56769</td>\n",
       "      <td>1.006886</td>\n",
       "      <td>-0.011112</td>\n",
       "      <td>50.656707</td>\n",
       "      <td>127043</td>\n",
       "      <td>-45.107601</td>\n",
       "      <td>0.972919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   window                            best_agent_path  n_eval_episodes  \\\n",
       "0       1  agent_1-1_seed=38531_test=2019_valrew=-44                1   \n",
       "\n",
       "   val_reward  std_reward  Annual return  Cumulative returns  \\\n",
       "0   11.832245         0.0        0.27286            0.270425   \n",
       "\n",
       "   Annual volatility  Sharpe ratio  Calmar ratio  ...  Omega ratio  \\\n",
       "0           0.112971      2.193221      3.777494  ...     1.453871   \n",
       "\n",
       "   Sortino ratio      Skew  Kurtosis  Tail ratio  Daily value at risk (95%)  \\\n",
       "0        2.86022 -0.426346   2.56769    1.006886                  -0.011112   \n",
       "\n",
       "   Avg Annual Turnover (in %)  final_portfolio_value_first_episode  \\\n",
       "0                   50.656707                               127043   \n",
       "\n",
       "   val_reward_mean  val_reward_std  \n",
       "0       -45.107601        0.972919  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Final Results DataFrame:\")\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
