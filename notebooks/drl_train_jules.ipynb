{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming utils are in parent directory or PYTHONPATH is set\n",
    "from utils.portfolio_env import PortfolioEnv\n",
    "from utils.drl_agent_jules import DRLAgent  # Import the modified agent\n",
    "\n",
    "# For learning rate schedule\n",
    "from typing import Callable\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Logging\n",
    "\n",
    "This notebook logs training progress using TensorBoard. Logs for each agent and window will be saved in subdirectories within the `../logs/` directory (relative to this notebook's location).\n",
    "\n",
    "To view the logs:\n",
    "1. Open a terminal or command prompt.\n",
    "2. Navigate to the directory *containing* the `logs` directory (i.e., the root of this repository if you are running the notebook from the `notebooks` folder).\n",
    "3. Run the command: `tensorboard --logdir logs/`\n",
    "4. Open the URL provided by TensorBoard (usually http://localhost:6006/) in your web browser.\n",
    "\n",
    "You should see experiments named like `PPO_WindowX_AgentY_SeedZ`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Configuration ---\n",
    "N_WINDOWS = 2  # 10 in paper\n",
    "AGENTS_PER_WINDOW = 2  # 5 in paper\n",
    "BASE_START_YEAR = 2006\n",
    "\n",
    "# Data paths\n",
    "PRICE_DATA_PATH = \"../data/prices.parquet\"\n",
    "RETURNS_DATA_PATH = \"../data/returns.parquet\"\n",
    "VOLA_DATA_PATH = \"../data/vola.parquet\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "MODEL_SAVE_DIR = f\"../models/{timestamp}/\"\n",
    "TENSORBOARD_LOG_DIR = f\"../logs/{timestamp}/\"\n",
    "\n",
    "# Ensure model save directory exists\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(TENSORBOARD_LOG_DIR, exist_ok=True)\n",
    "\n",
    "# --- DRL Agent Hyperparameters (from paper) ---\n",
    "N_ENVS = 10\n",
    "TOTAL_TIMESTEPS_PER_ROUND = 10**6 # 7_500_000  in paper\n",
    "N_STEPS_PER_ENV = 252 * 3  # n_steps = 252 * 3 * n_envs (this is per env for PPO buffer)\n",
    "# total buffer size before update = N_STEPS_PER_ENV * N_ENVS\n",
    "\n",
    "BATCH_SIZE = 1260\n",
    "N_EPOCHS = 16\n",
    "GAMMA = 0.9\n",
    "GAE_LAMBDA = 0.9\n",
    "CLIP_RANGE = 0.25\n",
    "LOG_STD_INIT = -1.0\n",
    "POLICY_KWARGS = dict(\n",
    "    activation_fn=torch.nn.Tanh,\n",
    "    net_arch=[64, 64],  # Shared layers for policy and value networks\n",
    "    log_std_init=LOG_STD_INIT,\n",
    ")\n",
    "\n",
    "# Learning rate schedule: linear decay from 3e-4 to 1e-5\n",
    "INITIAL_LR = 3e-4\n",
    "FINAL_LR = 1e-5\n",
    "\n",
    "\n",
    "def linear_schedule(\n",
    "    initial_value: float, final_value: float\n",
    ") -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :param final_value: Final learning rate.\n",
    "    :return: schedule that computes current learning rate depending on progress remaining (1.0 -> 0.0)\n",
    "    \"\"\"\n",
    "\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1.0 to 0.0\n",
    "        \"\"\"\n",
    "        return final_value + progress_remaining * (initial_value - final_value)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "LEARNING_RATE_SCHEDULE = linear_schedule(INITIAL_LR, FINAL_LR)\n",
    "\n",
    "# --- PortfolioEnv Parameters ---\n",
    "ENV_WINDOW_SIZE = 60  # Lookback window for features in PortfolioEnv\n",
    "TRANSACTION_COST = 0.0  # As per paper (or can be adjusted)\n",
    "INITIAL_BALANCE = 100_000\n",
    "REWARD_SCALING = 1.0\n",
    "ETA_DSR = 1 / 252  # For Differential Sharpe Ratio in PortfolioEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the full datasets once\n",
    "try:\n",
    "    print(\"Loading data...\")\n",
    "    prices_df_full = pd.read_parquet(PRICE_DATA_PATH)\n",
    "    returns_df_full = pd.read_parquet(RETURNS_DATA_PATH)\n",
    "    vola_df_full = pd.read_parquet(VOLA_DATA_PATH)\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "    # Ensure DataFrames have DateTimeIndex\n",
    "    for df in [prices_df_full, returns_df_full, vola_df_full]:\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    # print(\"\\nPrice Data Head:\")\n",
    "    # print(prices_df_full.head())\n",
    "    # print(\"\\nReturns Data Head:\")\n",
    "    # print(returns_df_full.head())\n",
    "    # print(\"\\nVolatility Data Head:\")\n",
    "    # print(vola_df_full.head())\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Data file not found. {e}\")\n",
    "    print(\"Please ensure data is generated and paths are correct in Cell 2.\")\n",
    "    # Stop execution or raise error if data is critical for notebook to run\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(\n",
    "    year_start,\n",
    "    num_train_years,\n",
    "    num_val_years,\n",
    "    num_test_years,\n",
    "    prices_df,\n",
    "    returns_df,\n",
    "    vol_df,\n",
    "):\n",
    "    \"\"\"Slices data for a given window configuration.\"\"\"\n",
    "\n",
    "    train_start_date = pd.to_datetime(f\"{year_start}-01-01\")\n",
    "    train_end_date = pd.to_datetime(f\"{year_start + num_train_years - 1}-12-31\")\n",
    "\n",
    "    val_start_date = pd.to_datetime(f\"{year_start + num_train_years}-01-01\")\n",
    "    val_end_date = pd.to_datetime(\n",
    "        f\"{year_start + num_train_years + num_val_years - 1}-12-31\"\n",
    "    )\n",
    "\n",
    "    test_start_date = pd.to_datetime(\n",
    "        f\"{year_start + num_train_years + num_val_years}-01-01\"\n",
    "    )\n",
    "    test_end_date = pd.to_datetime(\n",
    "        f\"{year_start + num_train_years + num_val_years + num_test_years - 1}-12-31\"\n",
    "    )\n",
    "\n",
    "    print(f\"  Train Period: {train_start_date.date()} to {train_end_date.date()}\")\n",
    "    print(f\"  Val Period  : {val_start_date.date()} to {val_end_date.date()}\")\n",
    "    print(f\"  Test Period : {test_start_date.date()} to {test_end_date.date()}\")\n",
    "\n",
    "    # Slicing (ensure index is datetime)\n",
    "    train_prices = prices_df[train_start_date:train_end_date]\n",
    "    train_returns = returns_df[train_start_date:train_end_date]\n",
    "    train_vola = vol_df[train_start_date:train_end_date]\n",
    "\n",
    "    val_prices = prices_df[val_start_date:val_end_date]\n",
    "    val_returns = returns_df[val_start_date:val_end_date]\n",
    "    val_vola = vol_df[val_start_date:val_end_date]\n",
    "\n",
    "    test_prices = prices_df[test_start_date:test_end_date]\n",
    "    test_returns = returns_df[test_start_date:test_end_date]\n",
    "    test_vola = vol_df[test_start_date:test_end_date]\n",
    "\n",
    "    # Basic check for empty slices which can halt env creation\n",
    "    if train_prices.empty or val_prices.empty or test_prices.empty:\n",
    "        print(\n",
    "            \"WARNING: One or more data slices are empty. Check date ranges and data availability.\"\n",
    "        )\n",
    "        # Potentially raise an error or handle as per requirements\n",
    "\n",
    "    return (\n",
    "        (train_prices, train_returns, train_vola),\n",
    "        (val_prices, val_returns, val_vola),\n",
    "        (test_prices, test_returns, test_vola),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_backtest_results = []\n",
    "best_agent_paths_per_window = []\n",
    "\n",
    "# > PAPER : The data is split into 10 sliding window groups (shifted by 1-year). \n",
    "# Each group contains 7 years worth of data, \n",
    "# the first 5 years are used for training, \n",
    "# the next 1 year is a burn year used for training validation, \n",
    "# and the last year is kept out-of-sample for backtesting.\n",
    "\n",
    "# --- Main Loop for Sliding Windows ---\n",
    "for i_window in range(N_WINDOWS):\n",
    "    current_start_year = BASE_START_YEAR + i_window\n",
    "    print(f\"--- Starting Window {i_window+1}/{N_WINDOWS} (Train Year Start: {current_start_year}) ---\")\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    # 1. Slice Data for the current window\n",
    "    # 5 years train, 1 year validation, 1 year test\n",
    "    train_data, val_data, test_data = slice_data(\n",
    "        year_start=current_start_year,\n",
    "        num_train_years=5,\n",
    "        num_val_years=1,\n",
    "        num_test_years=1,\n",
    "        prices_df=prices_df_full,\n",
    "        returns_df=returns_df_full,\n",
    "        vol_df=vola_df_full\n",
    "    )\n",
    "    \n",
    "    # Unpack data\n",
    "    (train_prices, train_returns, train_vola) = train_data\n",
    "    (val_prices, val_returns, val_vola) = val_data\n",
    "    (test_prices, test_returns, test_vola) = test_data\n",
    "\n",
    "    # Check if any crucial dataframe is too short (e.g., shorter than ENV_WINDOW_SIZE)\n",
    "    # PortfolioEnv requires at least `window_size` days of data to start.\n",
    "    min_data_len = ENV_WINDOW_SIZE + 1 # Need at least window_size + 1 for one step\n",
    "    if len(train_prices) < min_data_len or len(val_prices) < min_data_len or len(test_prices) < min_data_len:\n",
    "        print(f\"SKIPPING Window {i_window+1} due to insufficient data length for one or more periods.\")\n",
    "        print(f\"  Train length: {len(train_prices)}, Val length: {len(val_prices)}, Test length: {len(test_prices)}\")\n",
    "        print(f\"  Required minimum: {min_data_len}\")\n",
    "        best_agent_paths_per_window.append(None) # Mark as skipped\n",
    "        all_backtest_results.append({\"window\": i_window+1, \"status\": \"skipped_insufficient_data\", \"metrics\": {}})\n",
    "        continue\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    # 2. Create Training and Validation Environments\n",
    "    # These envs are re-created for each agent to ensure fresh state and correct data.\n",
    "    # However, the data slice itself is per-window.\n",
    "\n",
    "    best_agent_for_window = None\n",
    "    best_val_reward = -np.inf\n",
    "    \n",
    "    # --- Inner Loop for Training AGENTS_PER_WINDOW Agents ---\n",
    "    for i_agent in range(AGENTS_PER_WINDOW):\n",
    "\n",
    "        # > PAPER : During the first round of training, we initialize 5 agents (different seeds)\n",
    "        agent_seed = (i_window * AGENTS_PER_WINDOW) + i_agent # Unique seed for each agent run\n",
    "        print(f\"  Training Agent {i_agent+1}/{AGENTS_PER_WINDOW} with seed {agent_seed}...\")\n",
    "\n",
    "        # Create environments for this specific agent\n",
    "        # Training Env\n",
    "        env_train_config = {\n",
    "            'returns_df': train_returns, 'prices_df': train_prices, 'vol_df': train_vola,\n",
    "            'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "            'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "        }\n",
    "        # The DRLAgent class will use this first env to understand structure for SubprocVecEnv\n",
    "        # This single_env_for_init is just for the DRLAgent constructor to get parameters.\n",
    "        # The actual training will use N_ENVS created by DRLAgent.\n",
    "        single_env_for_init_train = PortfolioEnv(**env_train_config)\n",
    "\n",
    "        # Validation Env (single, not vectorized for evaluation)\n",
    "        env_val_config = {\n",
    "            'returns_df': val_returns, 'prices_df': val_prices, 'vol_df': val_vola,\n",
    "            'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "            'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "        }\n",
    "        env_val = PortfolioEnv(**env_val_config)\n",
    "        \n",
    "        # Instantiate DRL Agent\n",
    "        agent = DRLAgent(\n",
    "            env=single_env_for_init_train, # Pass the sample env for DRLAgent to clone\n",
    "            n_envs=N_ENVS,\n",
    "            policy_kwargs=POLICY_KWARGS,\n",
    "            n_steps=N_STEPS_PER_ENV, # n_steps per environment for PPO\n",
    "            batch_size=BATCH_SIZE,\n",
    "            n_epochs=N_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE_SCHEDULE,\n",
    "            gamma=GAMMA,\n",
    "            gae_lambda=GAE_LAMBDA,\n",
    "            clip_range=CLIP_RANGE,\n",
    "            seed=agent_seed,\n",
    "            tensorboard_log=TENSORBOARD_LOG_DIR\n",
    "        )\n",
    "\n",
    "        # Agent Seeding: Load previous window's best agent if not the first window\n",
    "        if i_window > 0 and best_agent_paths_per_window[i_window-1] is not None:\n",
    "            previous_best_agent_path = best_agent_paths_per_window[i_window-1]\n",
    "            print(f\"    Seeding agent from: {previous_best_agent_path}\")\n",
    "            # The env for load_from_file should match the new training env structure\n",
    "            # DRLAgent's load_from_file uses its internal self.env by default if env=None.\n",
    "            # This self.env is already configured with N_ENVS and the new train_data.\n",
    "            agent.load_from_file(path=previous_best_agent_path, env=None) \n",
    "            agent.model.set_random_seed(agent_seed) # Ensure the loaded model uses the new agent_seed\n",
    "                                   \n",
    "        # Train the agent\n",
    "        print(f\"    Starting training for {TOTAL_TIMESTEPS_PER_ROUND} timesteps...\")\n",
    "        # Note: Training can be very long. For testing, reduce TOTAL_TIMESTEPS_PER_ROUND.\n",
    "        # Example: agent.train(total_timesteps=10000, tb_log_name=f\"ppo_win{i_window}_agent{i_agent}\")\n",
    "        agent.train(\n",
    "            total_timesteps=TOTAL_TIMESTEPS_PER_ROUND, \n",
    "            tb_experiment_name=f\"PPO_Window{i_window+1}_Agent{i_agent+1}_Seed{agent_seed}\",\n",
    "        )\n",
    "\n",
    "        # > PAPER : All five agents start training on data from [2006−2011) \n",
    "        # and their performance is periodically evaluated using the validation period 2011\n",
    "\n",
    "        # Evaluate the agent on the validation set\n",
    "        print(\"    Evaluating agent on validation set...\")\n",
    "        # The evaluate method in DRLAgentJules is designed for a single eval_env\n",
    "        val_metrics = agent.evaluate(eval_env=env_val, n_eval_episodes=1)\n",
    "        current_val_reward = val_metrics.get(\"mean_reward\", -np.inf)\n",
    "        print(f\"    Validation Mean Reward: {current_val_reward:.4f}\")\n",
    "        \n",
    "        # Save this agent\n",
    "        current_agent_model_name = f\"agent_win{i_window+1}_seed{agent_seed}_valrew{current_val_reward:.2f}.zip\"\n",
    "        current_agent_save_path = os.path.join(MODEL_SAVE_DIR, current_agent_model_name)\n",
    "        agent.save(current_agent_save_path)\n",
    "        print(f\"    Agent saved to: {current_agent_save_path}\")\n",
    "\n",
    "        # > PAPER : At the end of the first round of training, we save the best performing agent \n",
    "        # (based on highest mean episode validation reward)\n",
    "        if current_val_reward > best_val_reward:\n",
    "            best_val_reward = current_val_reward\n",
    "            best_agent_for_window_path = current_agent_save_path \n",
    "            print(f\"    New best agent for this window with validation reward: {best_val_reward:.4f}\")\n",
    "\n",
    "        # Clean up to free memory\n",
    "        del agent\n",
    "        del single_env_for_init_train\n",
    "        del env_val\n",
    "\n",
    "        # If using GPU\n",
    "        torch.cuda.empty_cache() \n",
    "\n",
    "    best_agent_paths_per_window.append(best_agent_for_window_path if 'best_agent_for_window_path' in locals() and best_agent_for_window_path is not None else None)\n",
    "    \n",
    "    if best_agent_paths_per_window[-1] is None:\n",
    "        print(f\"  No best agent found or saved for window {i_window+1}. Skipping backtest.\")\n",
    "        all_backtest_results.append({\"window\": i_window+1, \"status\": \"no_best_agent\", \"metrics\": {}})\n",
    "        continue\n",
    "    \n",
    "    # = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "    # 3. Backtest the best agent of the window\n",
    "    print(f\"  Backtesting best agent for Window {i_window+1} ({best_agent_paths_per_window[-1]})\" )\n",
    "    \n",
    "    # Create Backtesting Environment\n",
    "    env_test_config = {\n",
    "        'returns_df': test_returns, 'prices_df': test_prices, 'vol_df': test_vola,\n",
    "        'window_size': ENV_WINDOW_SIZE, 'transaction_cost': TRANSACTION_COST,\n",
    "        'initial_balance': INITIAL_BALANCE, 'reward_scaling': REWARD_SCALING, 'eta': ETA_DSR\n",
    "    }\n",
    "    env_test = PortfolioEnv(**env_test_config)\n",
    "\n",
    "    best_agent_loaded = DRLAgent(\n",
    "        env=env_test, \n",
    "        n_envs=1, # For eval, n_envs=1 is fine for the DRLAgent wrapper\n",
    "        policy_kwargs=POLICY_KWARGS \n",
    "        # Other params don't matter as we are loading a pre-trained model\n",
    "    )\n",
    "    \n",
    "    print(f\"    Loading model from: {best_agent_paths_per_window[-1]}\")\n",
    "    # Pass the actual test_env to PPO.load via DRLAgent.load method\n",
    "    best_agent_loaded.load(path=best_agent_paths_per_window[-1], env=env_test) \n",
    "                                   \n",
    "    print(\"    Running backtest evaluation...\")\n",
    "    backtest_metrics = best_agent_loaded.evaluate(eval_env=env_test, n_eval_episodes=1)\n",
    "    \n",
    "    print(f\"    Backtest Metrics for Window {i_window+1}:\")\n",
    "    for key, value in backtest_metrics.items():\n",
    "        print(f\"      {key}: {value}\")\n",
    "    \n",
    "    all_backtest_results.append({\n",
    "        \"window\": i_window+1, \n",
    "        \"best_agent_path\": best_agent_paths_per_window[-1],\n",
    "        \"status\": \"completed\",\n",
    "        \"metrics\": backtest_metrics\n",
    "    })\n",
    "    \n",
    "    del best_agent_loaded\n",
    "    del env_test\n",
    "    torch.cuda.empty_cache() # If using GPU\n",
    "\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "# 4. Save results\n",
    "print(\"\\n--- All Windows Processed ---\")\n",
    "print(\"Summary of Best Agent Paths:\")\n",
    "for i, path in enumerate(best_agent_paths_per_window):\n",
    "    print(f\"Window {i+1}: {path}\")\n",
    "\n",
    "print(\"\\nSummary of Backtest Results:\")\n",
    "for result in all_backtest_results:\n",
    "    print(f\"Window {result['window']} ({result['status']}):\")\n",
    "    if result['status'] == 'completed':\n",
    "        # print(f\"  Agent: {result['best_agent_path']}\")\n",
    "        for k, v in result['metrics'].items():\n",
    "            if isinstance(v, float): print(f\"    {k}: {v:.4f}\")\n",
    "            else: print(f\"    {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backtest results summary saved to: ../models/sliding_window_jules/20250531_205413/backtest_results_summary_20250531_210357.csv\n",
      "\n",
      "Final Results DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>window</th>\n",
       "      <th>best_agent_path</th>\n",
       "      <th>status</th>\n",
       "      <th>Annual return</th>\n",
       "      <th>Cumulative returns</th>\n",
       "      <th>Annual volatility</th>\n",
       "      <th>Sharpe ratio</th>\n",
       "      <th>Calmar ratio</th>\n",
       "      <th>Stability</th>\n",
       "      <th>Max drawdown</th>\n",
       "      <th>...</th>\n",
       "      <th>Sortino ratio</th>\n",
       "      <th>Skew</th>\n",
       "      <th>Kurtosis</th>\n",
       "      <th>Tail ratio</th>\n",
       "      <th>Daily value at risk (95%)</th>\n",
       "      <th>Portfolio turnover</th>\n",
       "      <th>mean_reward</th>\n",
       "      <th>std_reward</th>\n",
       "      <th>n_eval_episodes</th>\n",
       "      <th>final_portfolio_value_first_episode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>../models/sliding_window_jules/20250531_205413...</td>\n",
       "      <td>completed</td>\n",
       "      <td>-0.013614</td>\n",
       "      <td>-0.010228</td>\n",
       "      <td>0.112879</td>\n",
       "      <td>-0.242189</td>\n",
       "      <td>-0.295943</td>\n",
       "      <td>0.898570</td>\n",
       "      <td>-0.046002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.387184</td>\n",
       "      <td>0.086205</td>\n",
       "      <td>0.508573</td>\n",
       "      <td>1.222959</td>\n",
       "      <td>-0.010403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.146446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>98977.210871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>../models/sliding_window_jules/20250531_205413...</td>\n",
       "      <td>completed</td>\n",
       "      <td>0.025749</td>\n",
       "      <td>0.019456</td>\n",
       "      <td>0.106050</td>\n",
       "      <td>0.104235</td>\n",
       "      <td>0.461445</td>\n",
       "      <td>0.904118</td>\n",
       "      <td>-0.055801</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160031</td>\n",
       "      <td>-0.263293</td>\n",
       "      <td>0.640463</td>\n",
       "      <td>0.945525</td>\n",
       "      <td>-0.011436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.573420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101945.613424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   window                                    best_agent_path     status  \\\n",
       "0       1  ../models/sliding_window_jules/20250531_205413...  completed   \n",
       "1       2  ../models/sliding_window_jules/20250531_205413...  completed   \n",
       "\n",
       "   Annual return  Cumulative returns  Annual volatility  Sharpe ratio  \\\n",
       "0      -0.013614           -0.010228           0.112879     -0.242189   \n",
       "1       0.025749            0.019456           0.106050      0.104235   \n",
       "\n",
       "   Calmar ratio  Stability  Max drawdown  ...  Sortino ratio      Skew  \\\n",
       "0     -0.295943   0.898570     -0.046002  ...      -0.387184  0.086205   \n",
       "1      0.461445   0.904118     -0.055801  ...       0.160031 -0.263293   \n",
       "\n",
       "   Kurtosis  Tail ratio  Daily value at risk (95%)  Portfolio turnover  \\\n",
       "0  0.508573    1.222959                  -0.010403                 NaN   \n",
       "1  0.640463    0.945525                  -0.011436                 NaN   \n",
       "\n",
       "   mean_reward  std_reward  n_eval_episodes  \\\n",
       "0    -0.146446         0.0              1.0   \n",
       "1     2.573420         0.0              1.0   \n",
       "\n",
       "   final_portfolio_value_first_episode  \n",
       "0                         98977.210871  \n",
       "1                        101945.613424  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(all_backtest_results)\n",
    "\n",
    "# Expand the 'metrics' dictionary into separate columns\n",
    "metrics_df = results_df[\"metrics\"].apply(pd.Series)\n",
    "results_df = pd.concat([results_df.drop(\"metrics\", axis=1), metrics_df], axis=1)\n",
    "\n",
    "results_filename = (\n",
    "    f\"backtest_results_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    ")\n",
    "results_save_path = os.path.join(MODEL_SAVE_DIR, results_filename)\n",
    "results_df.to_csv(results_save_path, index=False)\n",
    "print(f\"\\nBacktest results summary saved to: {results_save_path}\")\n",
    "print(\"\\nFinal Results DataFrame:\")\n",
    "results_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
